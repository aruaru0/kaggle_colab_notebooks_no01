{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3134.482195,
      "end_time": "2021-09-26T00:57:33.514736",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-09-26T00:05:19.032541",
      "version": "2.3.3"
    },
    "colab": {
      "name": "chaii-qa-5-fold-xlmroberta-torch-fit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aruaru0/colab_notebook/blob/main/chaii_qa_5_fold_xlmroberta_torch_fit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shE3S33KxTch"
      },
      "source": [
        "# for colab"
      ],
      "id": "shE3S33KxTch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZJxbaQFyRMb",
        "outputId": "2368d75c-39e3-4d97-971a-2232ac1a2f82"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "BZJxbaQFyRMb",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 27 22:07:29 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnzNG_0jxW_6",
        "outputId": "590ca250-063f-4422-a742-a326bb3c4fd1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "KnzNG_0jxW_6",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTgjav_xxW7p"
      },
      "source": [
        "! pip install --upgrade --force-reinstall --no-deps  kaggle > /dev/null\n",
        "! mkdir /root/.kaggle\n",
        "! cp \"/content/drive/My Drive/Kaggle/kaggle.json\" /root/.kaggle/\n",
        "! chmod 600 /root/.kaggle/kaggle.json"
      ],
      "id": "CTgjav_xxW7p",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_GIjVFxW4G",
        "outputId": "8f58aba5-77ba-429a-c7b0-1b3a4d6ea13e"
      },
      "source": [
        "!kaggle competitions download -c chaii-hindi-and-tamil-question-answering\n",
        "!kaggle datasets download -d rhtsingh/mlqa-hindi-processed\n",
        "!kaggle datasets download -d nbroad/xlm-roberta-squad2"
      ],
      "id": "MO_GIjVFxW4G",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading chaii-hindi-and-tamil-question-answering.zip to /content\n",
            "\r  0% 0.00/6.81M [00:00<?, ?B/s]\r 73% 5.00M/6.81M [00:00<00:00, 38.3MB/s]\n",
            "100% 6.81M/6.81M [00:00<00:00, 50.3MB/s]\n",
            "Downloading mlqa-hindi-processed.zip to /content\n",
            "  0% 0.00/2.49M [00:00<?, ?B/s]\n",
            "100% 2.49M/2.49M [00:00<00:00, 217MB/s]\n",
            "Downloading xlm-roberta-squad2.zip to /content\n",
            "100% 3.33G/3.34G [00:29<00:00, 127MB/s]\n",
            "100% 3.34G/3.34G [00:29<00:00, 122MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PObJQR5XxWrD",
        "outputId": "2fb9531d-6fd3-4862-e809-325030089db0"
      },
      "source": [
        "!apt install unzpip"
      ],
      "id": "PObJQR5XxWrD",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package unzpip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyQVYuBDxxBn"
      },
      "source": [
        "!mkdir -p input output"
      ],
      "id": "wyQVYuBDxxBn",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3uTTHsFxw2D",
        "outputId": "e86e0e8f-f257-4f92-dc77-e28d4e6fe1b4"
      },
      "source": [
        "!mkdir input/chaii-hindi-and-tamil-question-answering\n",
        "!unzip /content/chaii-hindi-and-tamil-question-answering.zip -d input/chaii-hindi-and-tamil-question-answering\n",
        "!mkdir input/mlqa-hindi-processed\n",
        "!unzip /content/mlqa-hindi-processed.zip -d input/mlqa-hindi-processed\n",
        "!mkdir input/xlm-roberta-squad2\n",
        "!unzip /content/xlm-roberta-squad2.zip -d input/xlm-roberta-squad2/"
      ],
      "id": "_3uTTHsFxw2D",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/chaii-hindi-and-tamil-question-answering.zip\n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/sample_submission.csv  \n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/test.csv  \n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/train.csv  \n",
            "Archive:  /content/mlqa-hindi-processed.zip\n",
            "  inflating: input/mlqa-hindi-processed/mlqa_hindi.csv  \n",
            "  inflating: input/mlqa-hindi-processed/xquad.csv  \n",
            "Archive:  /content/xlm-roberta-squad2.zip\n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/config.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/pytorch_model.bin  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/sentencepiece.bpe.model  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/special_tokens_map.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/tokenizer.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/tokenizer_config.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/config.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/pytorch_model.bin  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/sentencepiece.bpe.model  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/special_tokens_map.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/tokenizer.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/tokenizer_config.json  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/config.json  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/pytorch_model.bin  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/sentencepiece.bpe.model  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/special_tokens_map.json  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/tokenizer.json  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/tokenizer_config.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCRudbllx9Be"
      },
      "source": [
        "# restart here.."
      ],
      "id": "SCRudbllx9Be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrhwRDxfx6fa",
        "outputId": "b314c4e1-a352-41d9-8eb0-c1a79a4a51bd"
      },
      "source": [
        "%cd output"
      ],
      "id": "IrhwRDxfx6fa",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPXOqpvbyBky",
        "outputId": "e38e2b6c-cde9-4199-96d2-df871a923483"
      },
      "source": [
        "!pip install transformers"
      ],
      "id": "wPXOqpvbyBky",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022211,
          "end_time": "2021-09-26T00:05:25.568649",
          "exception": false,
          "start_time": "2021-09-26T00:05:25.546438",
          "status": "completed"
        },
        "tags": [],
        "id": "c47c9139"
      },
      "source": [
        "<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n",
        "    \n",
        "<h3><span \"style: color=#444\">Introduction</span></h3>\n",
        "\n",
        "The kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n",
        "\n",
        "This is a three part kernel,\n",
        "\n",
        "- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n",
        "\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n",
        "\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n",
        "\n",
        "<h3><span \"style: color=#444\">Techniques</span></h3>\n",
        "\n",
        "The kernel has implementation for below techniques, click on the links to learn more -\n",
        "\n",
        " - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n",
        " \n",
        " - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n",
        " \n",
        " - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n",
        " \n",
        " - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n",
        " \n",
        " - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n",
        " \n",
        " - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n",
        " \n",
        " - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n",
        " \n",
        " - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n",
        " \n",
        " - etc.\n",
        " \n",
        "<h3><span \"style: color=#444\">References</span></h3>\n",
        "I would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n",
        "\n",
        "- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n",
        "\n",
        "- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n",
        "\n",
        "- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n",
        "\n",
        "- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n",
        "\n",
        "- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read."
      ],
      "id": "c47c9139"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020104,
          "end_time": "2021-09-26T00:05:25.609196",
          "exception": false,
          "start_time": "2021-09-26T00:05:25.589092",
          "status": "completed"
        },
        "tags": [],
        "id": "43955668"
      },
      "source": [
        "<h3><span style=\"color=#444\">Note</span></h3>\n",
        "\n",
        "The below points are worth noting,\n",
        "\n",
        " - I haven't used FP16 because due to some reason this fails and model never starts training.\n",
        " - These are the original hyperparamters and setting that I have used for training my models.\n",
        " - I tried few pooling layers but none of them performed better than simple one.\n",
        " - Gradient clipping reduces model performance.\n",
        " - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n",
        " - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>"
      ],
      "id": "43955668"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.021131,
          "end_time": "2021-09-26T00:05:25.651115",
          "exception": false,
          "start_time": "2021-09-26T00:05:25.629984",
          "status": "completed"
        },
        "tags": [],
        "id": "8f09433b"
      },
      "source": [
        "### Install APEX"
      ],
      "id": "8f09433b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:25.699522Z",
          "iopub.status.busy": "2021-09-26T00:05:25.698767Z",
          "iopub.status.idle": "2021-09-26T00:05:25.700524Z",
          "shell.execute_reply": "2021-09-26T00:05:25.700004Z",
          "shell.execute_reply.started": "2021-08-17T20:22:25.164627Z"
        },
        "id": "f4bb901a",
        "papermill": {
          "duration": 0.028888,
          "end_time": "2021-09-26T00:05:25.700651",
          "exception": false,
          "start_time": "2021-09-26T00:05:25.671763",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# %%writefile setup.sh\n",
        "# export CUDA_HOME=/usr/local/cuda-10.1\n",
        "# git clone https://github.com/NVIDIA/apex\n",
        "# cd apex\n",
        "# pip install -v --disable-pip-version-check --no-cache-dir ./"
      ],
      "id": "f4bb901a",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:25.744044Z",
          "iopub.status.busy": "2021-09-26T00:05:25.743553Z",
          "iopub.status.idle": "2021-09-26T00:05:25.746936Z",
          "shell.execute_reply": "2021-09-26T00:05:25.747416Z",
          "shell.execute_reply.started": "2021-08-17T20:22:26.935881Z"
        },
        "id": "ef5e2a11",
        "papermill": {
          "duration": 0.026756,
          "end_time": "2021-09-26T00:05:25.747543",
          "exception": false,
          "start_time": "2021-09-26T00:05:25.720787",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# %%capture\n",
        "# !sh setup.sh"
      ],
      "id": "ef5e2a11",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.019798,
          "end_time": "2021-09-26T00:05:25.787273",
          "exception": false,
          "start_time": "2021-09-26T00:05:25.767475",
          "status": "completed"
        },
        "tags": [],
        "id": "55059f9a"
      },
      "source": [
        "### Import Dependencies"
      ],
      "id": "55059f9a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:25.912629Z",
          "iopub.status.busy": "2021-09-26T00:05:25.911775Z",
          "iopub.status.idle": "2021-09-26T00:05:32.667757Z",
          "shell.execute_reply": "2021-09-26T00:05:32.666998Z",
          "shell.execute_reply.started": "2021-08-17T20:23:50.232861Z"
        },
        "id": "8a92ad27",
        "papermill": {
          "duration": 6.860601,
          "end_time": "2021-09-26T00:05:32.667886",
          "exception": false,
          "start_time": "2021-09-26T00:05:25.807285",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63867367-445b-40c8-fb02-e65bee526cf9"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "gc.enable()\n",
        "import math\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import multiprocessing\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn import model_selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import (\n",
        "    Dataset, DataLoader,\n",
        "    SequentialSampler, RandomSampler\n",
        ")\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "try:\n",
        "    from apex import amp\n",
        "    APEX_INSTALLED = True\n",
        "except ImportError:\n",
        "    APEX_INSTALLED = False\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    get_cosine_schedule_with_warmup,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    logging,\n",
        "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
        ")\n",
        "logging.set_verbosity_warning()\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "def fix_all_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def optimal_num_of_loader_workers():\n",
        "    num_cpus = multiprocessing.cpu_count()\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
        "    return optimal_value\n",
        "\n",
        "print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
      ],
      "id": "8a92ad27",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apex AMP Installed :: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020279,
          "end_time": "2021-09-26T00:05:32.709998",
          "exception": false,
          "start_time": "2021-09-26T00:05:32.689719",
          "status": "completed"
        },
        "tags": [],
        "id": "ab75148d"
      },
      "source": [
        "### Training Configuration"
      ],
      "id": "ab75148d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:32.756757Z",
          "iopub.status.busy": "2021-09-26T00:05:32.756023Z",
          "iopub.status.idle": "2021-09-26T00:05:32.758204Z",
          "shell.execute_reply": "2021-09-26T00:05:32.758652Z",
          "shell.execute_reply.started": "2021-08-17T20:24:13.391662Z"
        },
        "id": "5934fe6a",
        "papermill": {
          "duration": 0.028413,
          "end_time": "2021-09-26T00:05:32.758778",
          "exception": false,
          "start_time": "2021-09-26T00:05:32.730365",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "class Config:\n",
        "    # model\n",
        "    model_type = 'xlm_roberta'\n",
        "    #model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n",
        "    model_name_or_path = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n",
        "    #config_name = \"deepset/xlm-roberta-large-squad2\"\n",
        "    config_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n",
        "    fp16 = True if APEX_INSTALLED else False\n",
        "    fp16_opt_level = \"O1\"\n",
        "    gradient_accumulation_steps = 2\n",
        "\n",
        "    # tokenizer\n",
        "    tokenizer_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n",
        "    max_seq_length = 384\n",
        "    doc_stride = 128\n",
        "\n",
        "    # train\n",
        "    epochs = 3\n",
        "    train_batch_size = 4\n",
        "    eval_batch_size = 8\n",
        "\n",
        "    # optimizer\n",
        "    optimizer_type = 'AdamW'\n",
        "    learning_rate = 1.5e-5\n",
        "    weight_decay = 1e-2\n",
        "    epsilon = 1e-8\n",
        "    max_grad_norm = 1.0\n",
        "\n",
        "    # scheduler\n",
        "    decay_name = 'linear-warmup'\n",
        "    warmup_ratio = 0.1\n",
        "\n",
        "    # logging\n",
        "    logging_steps = 10\n",
        "\n",
        "    # evaluate\n",
        "    output_dir = 'output'\n",
        "    seed = 2021"
      ],
      "id": "5934fe6a",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020252,
          "end_time": "2021-09-26T00:05:32.799380",
          "exception": false,
          "start_time": "2021-09-26T00:05:32.779128",
          "status": "completed"
        },
        "tags": [],
        "id": "a97366e5"
      },
      "source": [
        "### Data Factory"
      ],
      "id": "a97366e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:32.850430Z",
          "iopub.status.busy": "2021-09-26T00:05:32.849846Z",
          "iopub.status.idle": "2021-09-26T00:05:34.091936Z",
          "shell.execute_reply": "2021-09-26T00:05:34.091252Z",
          "shell.execute_reply.started": "2021-08-17T20:24:26.939409Z"
        },
        "id": "eb636dc6",
        "papermill": {
          "duration": 1.272219,
          "end_time": "2021-09-26T00:05:34.092072",
          "exception": false,
          "start_time": "2021-09-26T00:05:32.819853",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n",
        "test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n",
        "external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n",
        "external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n",
        "external_train = pd.concat([external_mlqa, external_xquad])\n",
        "\n",
        "def create_folds(data, num_splits):\n",
        "    data[\"kfold\"] = -1\n",
        "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    return data\n",
        "\n",
        "train = create_folds(train, num_splits=5)\n",
        "external_train[\"kfold\"] = -1\n",
        "external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
        "train = pd.concat([train, external_train]).reset_index(drop=True)\n",
        "\n",
        "def convert_answers(row):\n",
        "    return {'answer_start': [row[0]], 'text': [row[1]]}\n",
        "\n",
        "train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
      ],
      "id": "eb636dc6",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020496,
          "end_time": "2021-09-26T00:05:34.134296",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.113800",
          "status": "completed"
        },
        "tags": [],
        "id": "60781636"
      },
      "source": [
        "### Covert Examples to Features (Preprocess)"
      ],
      "id": "60781636"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:34.186555Z",
          "iopub.status.busy": "2021-09-26T00:05:34.185751Z",
          "iopub.status.idle": "2021-09-26T00:05:34.188744Z",
          "shell.execute_reply": "2021-09-26T00:05:34.188352Z",
          "shell.execute_reply.started": "2021-08-12T15:50:26.947551Z"
        },
        "id": "63b399b9",
        "papermill": {
          "duration": 0.034178,
          "end_time": "2021-09-26T00:05:34.188849",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.154671",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def prepare_train_features(args, example, tokenizer):\n",
        "    example[\"question\"] = example[\"question\"].lstrip()\n",
        "    tokenized_example = tokenizer(\n",
        "        example[\"question\"],\n",
        "        example[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=args.max_seq_length,\n",
        "        stride=args.doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
        "\n",
        "    features = []\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        feature = {}\n",
        "\n",
        "        input_ids = tokenized_example[\"input_ids\"][i]\n",
        "        attention_mask = tokenized_example[\"attention_mask\"][i]\n",
        "\n",
        "        feature['input_ids'] = input_ids\n",
        "        feature['attention_mask'] = attention_mask\n",
        "        feature['offset_mapping'] = offsets\n",
        "\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        sequence_ids = tokenized_example.sequence_ids(i)\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = example[\"answers\"]\n",
        "\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            feature[\"start_position\"] = cls_index\n",
        "            feature[\"end_position\"] = cls_index\n",
        "        else:\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                feature[\"start_position\"] = cls_index\n",
        "                feature[\"end_position\"] = cls_index\n",
        "            else:\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                feature[\"start_position\"] = token_start_index - 1\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                feature[\"end_position\"] = token_end_index + 1\n",
        "\n",
        "        features.append(feature)\n",
        "    return features"
      ],
      "id": "63b399b9",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02017,
          "end_time": "2021-09-26T00:05:34.229304",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.209134",
          "status": "completed"
        },
        "tags": [],
        "id": "55553f9f"
      },
      "source": [
        "### Dataset Retriever"
      ],
      "id": "55553f9f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:34.279176Z",
          "iopub.status.busy": "2021-09-26T00:05:34.278656Z",
          "iopub.status.idle": "2021-09-26T00:05:34.281502Z",
          "shell.execute_reply": "2021-09-26T00:05:34.281085Z",
          "shell.execute_reply.started": "2021-08-12T15:50:26.963075Z"
        },
        "id": "c78cee8c",
        "papermill": {
          "duration": 0.031645,
          "end_time": "2021-09-26T00:05:34.281607",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.249962",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "class DatasetRetriever(Dataset):\n",
        "    def __init__(self, features, mode='train'):\n",
        "        super(DatasetRetriever, self).__init__()\n",
        "        self.features = features\n",
        "        self.mode = mode\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, item):   \n",
        "        feature = self.features[item]\n",
        "        if self.mode == 'train':\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
        "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
        "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                'offset_mapping':feature['offset_mapping'],\n",
        "                'sequence_ids':feature['sequence_ids'],\n",
        "                'id':feature['example_id'],\n",
        "                'context': feature['context'],\n",
        "                'question': feature['question']\n",
        "            }"
      ],
      "id": "c78cee8c",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020313,
          "end_time": "2021-09-26T00:05:34.322373",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.302060",
          "status": "completed"
        },
        "tags": [],
        "id": "b7e0677b"
      },
      "source": [
        "### Model"
      ],
      "id": "b7e0677b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:34.375959Z",
          "iopub.status.busy": "2021-09-26T00:05:34.374416Z",
          "iopub.status.idle": "2021-09-26T00:05:34.376926Z",
          "shell.execute_reply": "2021-09-26T00:05:34.377433Z",
          "shell.execute_reply.started": "2021-08-12T15:50:26.97747Z"
        },
        "id": "cd46e25e",
        "papermill": {
          "duration": 0.0347,
          "end_time": "2021-09-26T00:05:34.377562",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.342862",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, modelname_or_path, config):\n",
        "        super(Model, self).__init__()\n",
        "        self.config = config\n",
        "        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self._init_weights(self.qa_outputs)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids, \n",
        "        attention_mask=None, \n",
        "        # token_type_ids=None\n",
        "    ):\n",
        "        outputs = self.xlm_roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]\n",
        "        \n",
        "        #sequence_output = self.dropout(sequence_output)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        qa_logits = self.qa_outputs(sequence_output)\n",
        "        #print(qa_logits.size())\n",
        "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "    \n",
        "        return start_logits, end_logits"
      ],
      "id": "cd46e25e",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020673,
          "end_time": "2021-09-26T00:05:34.419564",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.398891",
          "status": "completed"
        },
        "tags": [],
        "id": "b054f2e5"
      },
      "source": [
        "### Loss"
      ],
      "id": "b054f2e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCkwCpg0FtDa"
      },
      "source": [
        "def linear_combination(x, y, epsilon):\n",
        "    return (1 - epsilon) * x + epsilon * y\n",
        "\n",
        "def reduce_loss(loss, reduction='mean'):\n",
        "    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss"
      ],
      "id": "YCkwCpg0FtDa",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU1pWXuBF5TP"
      },
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, epsilon=0.1, ignore_index = -1, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.reduction = reduction\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, preds, target):\n",
        "        n = preds.size()[-1]\n",
        "        log_preds = F.log_softmax(preds, dim=-1)\n",
        "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
        "        nll = F.nll_loss(log_preds, target, reduction=self.reduction, ignore_index=self.ignore_index)\n",
        "        return linear_combination(nll, loss/n, self.epsilon)"
      ],
      "id": "vU1pWXuBF5TP",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:34.467976Z",
          "iopub.status.busy": "2021-09-26T00:05:34.467098Z",
          "iopub.status.idle": "2021-09-26T00:05:34.471369Z",
          "shell.execute_reply": "2021-09-26T00:05:34.470871Z",
          "shell.execute_reply.started": "2021-08-12T15:50:26.992334Z"
        },
        "id": "f490e487",
        "papermill": {
          "duration": 0.030081,
          "end_time": "2021-09-26T00:05:34.471495",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.441414",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def loss_fn(preds, labels):\n",
        "    start_preds, end_preds = preds\n",
        "    start_labels, end_labels = labels\n",
        "    \n",
        "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
        "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
        "    #start_loss = LabelSmoothingCrossEntropy(ignore_index=-1)(start_preds, start_labels)\n",
        "    #end_loss = LabelSmoothingCrossEntropy(ignore_index=-1)(end_preds, end_labels)\n",
        "    total_loss = (start_loss + end_loss) / 2\n",
        "    return total_loss"
      ],
      "id": "f490e487",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022236,
          "end_time": "2021-09-26T00:05:34.517599",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.495363",
          "status": "completed"
        },
        "tags": [],
        "id": "9138cc0e"
      },
      "source": [
        "### Grouped Layerwise Learning Rate Decay"
      ],
      "id": "9138cc0e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:34.572158Z",
          "iopub.status.busy": "2021-09-26T00:05:34.571629Z",
          "iopub.status.idle": "2021-09-26T00:05:34.575079Z",
          "shell.execute_reply": "2021-09-26T00:05:34.575458Z",
          "shell.execute_reply.started": "2021-08-17T20:25:36.361009Z"
        },
        "id": "4e4bcde0",
        "papermill": {
          "duration": 0.037283,
          "end_time": "2021-09-26T00:05:34.575575",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.538292",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def get_optimizer_grouped_parameters(args, model):\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
        "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
        "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n",
        "        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    return optimizer_grouped_parameters"
      ],
      "id": "4e4bcde0",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020321,
          "end_time": "2021-09-26T00:05:34.616082",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.595761",
          "status": "completed"
        },
        "tags": [],
        "id": "2d0ac100"
      },
      "source": [
        "### Metric Logger"
      ],
      "id": "2d0ac100"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:34.664123Z",
          "iopub.status.busy": "2021-09-26T00:05:34.662890Z",
          "iopub.status.idle": "2021-09-26T00:05:34.665247Z",
          "shell.execute_reply": "2021-09-26T00:05:34.665637Z",
          "shell.execute_reply.started": "2021-08-12T15:50:27.021912Z"
        },
        "id": "65b3cd59",
        "papermill": {
          "duration": 0.029349,
          "end_time": "2021-09-26T00:05:34.665787",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.636438",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.max = 0\n",
        "        self.min = 1e5\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        if val > self.max:\n",
        "            self.max = val\n",
        "        if val < self.min:\n",
        "            self.min = val"
      ],
      "id": "65b3cd59",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020341,
          "end_time": "2021-09-26T00:05:34.706534",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.686193",
          "status": "completed"
        },
        "tags": [],
        "id": "03f659a2"
      },
      "source": [
        "### Utilities"
      ],
      "id": "03f659a2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:34.760768Z",
          "iopub.status.busy": "2021-09-26T00:05:34.759547Z",
          "iopub.status.idle": "2021-09-26T00:05:34.762183Z",
          "shell.execute_reply": "2021-09-26T00:05:34.761805Z",
          "shell.execute_reply.started": "2021-08-17T20:26:30.01459Z"
        },
        "id": "0a7d36df",
        "papermill": {
          "duration": 0.035313,
          "end_time": "2021-09-26T00:05:34.762280",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.726967",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def make_model(args):\n",
        "    config = AutoConfig.from_pretrained(args.config_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
        "    model = Model(args.model_name_or_path, config=config)\n",
        "    return config, tokenizer, model\n",
        "\n",
        "def make_optimizer(args, model):\n",
        "    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "    if args.optimizer_type == \"AdamW\":\n",
        "        optimizer = AdamW(\n",
        "            optimizer_grouped_parameters,\n",
        "            lr=args.learning_rate,\n",
        "            eps=args.epsilon,\n",
        "            correct_bias=True\n",
        "        )\n",
        "        return optimizer\n",
        "\n",
        "def make_scheduler(\n",
        "    args, optimizer, \n",
        "    num_warmup_steps, \n",
        "    num_training_steps\n",
        "):\n",
        "    if args.decay_name == \"cosine-warmup\":\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "    else:\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "    return scheduler    \n",
        "\n",
        "def make_loader(\n",
        "    args, data, \n",
        "    tokenizer, fold\n",
        "):\n",
        "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
        "    \n",
        "    train_features, valid_features = [[] for _ in range(2)]\n",
        "    for i, row in train_set.iterrows():\n",
        "        train_features += prepare_train_features(args, row, tokenizer)\n",
        "    for i, row in valid_set.iterrows():\n",
        "        valid_features += prepare_train_features(args, row, tokenizer)\n",
        "\n",
        "    train_dataset = DatasetRetriever(train_features)\n",
        "    valid_dataset = DatasetRetriever(valid_features)\n",
        "    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n",
        "    \n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    valid_sampler = SequentialSampler(valid_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.train_batch_size,\n",
        "        sampler=train_sampler,\n",
        "        num_workers=optimal_num_of_loader_workers(),\n",
        "        pin_memory=True,\n",
        "        drop_last=False \n",
        "    )\n",
        "\n",
        "    valid_dataloader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=args.eval_batch_size, \n",
        "        sampler=valid_sampler,\n",
        "        num_workers=optimal_num_of_loader_workers(),\n",
        "        pin_memory=True, \n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    return train_dataloader, valid_dataloader"
      ],
      "id": "0a7d36df",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020405,
          "end_time": "2021-09-26T00:05:34.802964",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.782559",
          "status": "completed"
        },
        "tags": [],
        "id": "47101630"
      },
      "source": [
        "### Trainer"
      ],
      "id": "47101630"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:34.856288Z",
          "iopub.status.busy": "2021-09-26T00:05:34.855754Z",
          "iopub.status.idle": "2021-09-26T00:05:34.859262Z",
          "shell.execute_reply": "2021-09-26T00:05:34.859726Z",
          "shell.execute_reply.started": "2021-08-17T20:26:36.310796Z"
        },
        "id": "81b92c70",
        "papermill": {
          "duration": 0.036073,
          "end_time": "2021-09-26T00:05:34.859848",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.823775",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self, model, tokenizer, \n",
        "        optimizer, scheduler\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def train(\n",
        "        self, args, \n",
        "        train_dataloader, \n",
        "        epoch, result_dict\n",
        "    ):\n",
        "        count = 0\n",
        "        losses = AverageMeter()\n",
        "        \n",
        "        self.model.zero_grad()\n",
        "        self.model.train()\n",
        "        \n",
        "        fix_all_seeds(args.seed)\n",
        "        \n",
        "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
        "                    batch_data['start_position'], batch_data['end_position']\n",
        "            \n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
        "\n",
        "            outputs_start, outputs_end = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "            \n",
        "            #print(outputs_start.size())\n",
        "\n",
        "            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            count += input_ids.size(0)\n",
        "            losses.update(loss.item(), input_ids.size(0))\n",
        "\n",
        "            # if args.fp16:\n",
        "            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n",
        "            # else:\n",
        "            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n",
        "                _s = str(len(str(len(train_dataloader.sampler))))\n",
        "                ret = [\n",
        "                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n",
        "                    'Train Loss: {: >4.5f}'.format(losses.avg),\n",
        "                ]\n",
        "                print(', '.join(ret))\n",
        "\n",
        "        result_dict['train_loss'].append(losses.avg)\n",
        "        return result_dict"
      ],
      "id": "81b92c70",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020663,
          "end_time": "2021-09-26T00:05:34.901431",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.880768",
          "status": "completed"
        },
        "tags": [],
        "id": "468c7fad"
      },
      "source": [
        "### Evaluator"
      ],
      "id": "468c7fad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:34.952455Z",
          "iopub.status.busy": "2021-09-26T00:05:34.951128Z",
          "iopub.status.idle": "2021-09-26T00:05:34.954011Z",
          "shell.execute_reply": "2021-09-26T00:05:34.953572Z",
          "shell.execute_reply.started": "2021-08-17T20:26:39.517856Z"
        },
        "id": "c77328b3",
        "papermill": {
          "duration": 0.032204,
          "end_time": "2021-09-26T00:05:34.954115",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.921911",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    \n",
        "    def save(self, result, output_dir):\n",
        "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
        "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
        "\n",
        "    def evaluate(self, valid_dataloader, epoch, result_dict):\n",
        "        losses = AverageMeter()\n",
        "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
        "            self.model = self.model.eval()\n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
        "                    batch_data['start_position'], batch_data['end_position']\n",
        "            \n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
        "            \n",
        "            with torch.no_grad():            \n",
        "                outputs_start, outputs_end = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                )\n",
        "                \n",
        "                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
        "                losses.update(loss.item(), input_ids.size(0))\n",
        "                \n",
        "        print('----Validation Results Summary----')\n",
        "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
        "        result_dict['val_loss'].append(losses.avg)        \n",
        "        return result_dict"
      ],
      "id": "c77328b3",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020689,
          "end_time": "2021-09-26T00:05:34.995524",
          "exception": false,
          "start_time": "2021-09-26T00:05:34.974835",
          "status": "completed"
        },
        "tags": [],
        "id": "86c0ad8e"
      },
      "source": [
        "### Initialize Training"
      ],
      "id": "86c0ad8e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:35.045640Z",
          "iopub.status.busy": "2021-09-26T00:05:35.044879Z",
          "iopub.status.idle": "2021-09-26T00:05:35.047788Z",
          "shell.execute_reply": "2021-09-26T00:05:35.047373Z",
          "shell.execute_reply.started": "2021-08-17T20:26:44.418691Z"
        },
        "id": "edd188d2",
        "papermill": {
          "duration": 0.031608,
          "end_time": "2021-09-26T00:05:35.047884",
          "exception": false,
          "start_time": "2021-09-26T00:05:35.016276",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def init_training(args, data, fold):\n",
        "    fix_all_seeds(args.seed)\n",
        "    \n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "    \n",
        "    # model\n",
        "    model_config, tokenizer, model = make_model(args)\n",
        "    if torch.cuda.device_count() >= 1:\n",
        "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
        "            torch.cuda.device_count(), \n",
        "            torch.cuda.get_device_name(0))\n",
        "        )\n",
        "        model = model.cuda() \n",
        "    else:\n",
        "        raise ValueError('CPU training is not supported')\n",
        "    \n",
        "    #model.load_state_dict(\n",
        "    #    torch.load(\"/content/drive/MyDrive/datas/chaii2/output/checkpoint-fold-{}\".format(fold))\n",
        "    #);\n",
        "\n",
        "    # data loaders\n",
        "    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = make_optimizer(args, model)\n",
        "\n",
        "    # scheduler\n",
        "    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n",
        "    if args.warmup_ratio > 0:\n",
        "        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
        "    else:\n",
        "        num_warmup_steps = 0\n",
        "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n",
        "    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n",
        "\n",
        "    # mixed precision training with NVIDIA Apex\n",
        "    if args.fp16:\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "    \n",
        "    result_dict = {\n",
        "        'epoch':[], \n",
        "        'train_loss': [], \n",
        "        'val_loss' : [], \n",
        "        'best_val_loss': np.inf\n",
        "    }\n",
        "\n",
        "    return (\n",
        "        model, model_config, tokenizer, optimizer, scheduler, \n",
        "        train_dataloader, valid_dataloader, result_dict\n",
        "    )"
      ],
      "id": "edd188d2",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02058,
          "end_time": "2021-09-26T00:05:35.089016",
          "exception": false,
          "start_time": "2021-09-26T00:05:35.068436",
          "status": "completed"
        },
        "tags": [],
        "id": "cb42970d"
      },
      "source": [
        "### Run"
      ],
      "id": "cb42970d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:35.140237Z",
          "iopub.status.busy": "2021-09-26T00:05:35.139692Z",
          "iopub.status.idle": "2021-09-26T00:05:35.142085Z",
          "shell.execute_reply": "2021-09-26T00:05:35.142464Z",
          "shell.execute_reply.started": "2021-08-12T15:50:27.097688Z"
        },
        "id": "915ef19d",
        "papermill": {
          "duration": 0.03322,
          "end_time": "2021-09-26T00:05:35.142579",
          "exception": false,
          "start_time": "2021-09-26T00:05:35.109359",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def run(data, fold):\n",
        "    args = Config()\n",
        "    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n",
        "        valid_dataloader, result_dict = init_training(args, data, fold)\n",
        "    \n",
        "    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n",
        "    evaluator = Evaluator(model)\n",
        "\n",
        "    train_time_list = []\n",
        "    valid_time_list = []\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        result_dict['epoch'].append(epoch)\n",
        "\n",
        "        # Train\n",
        "        torch.cuda.synchronize()\n",
        "        tic1 = time.time()\n",
        "        result_dict = trainer.train(\n",
        "            args, train_dataloader, \n",
        "            epoch, result_dict\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        tic2 = time.time() \n",
        "        train_time_list.append(tic2 - tic1)\n",
        "        \n",
        "        # Evaluate\n",
        "        torch.cuda.synchronize()\n",
        "        tic3 = time.time()\n",
        "        result_dict = evaluator.evaluate(\n",
        "            valid_dataloader, epoch, result_dict\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        tic4 = time.time() \n",
        "        valid_time_list.append(tic4 - tic3)\n",
        "            \n",
        "        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n",
        "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
        "            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
        "            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n",
        "            \n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
        "            model_config.save_pretrained(output_dir)\n",
        "            tokenizer.save_pretrained(output_dir)\n",
        "            print(f\"Saving model checkpoint to {output_dir}.\")\n",
        "            \n",
        "        print()\n",
        "\n",
        "    evaluator.save(result_dict, output_dir)\n",
        "    \n",
        "    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n",
        "    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    del trainer, evaluator\n",
        "    del model, model_config, tokenizer\n",
        "    del optimizer, scheduler\n",
        "    del train_dataloader, valid_dataloader, result_dict\n",
        "    gc.collect()"
      ],
      "id": "915ef19d",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:05:35.187688Z",
          "iopub.status.busy": "2021-09-26T00:05:35.187165Z",
          "iopub.status.idle": "2021-09-26T00:57:30.617454Z",
          "shell.execute_reply": "2021-09-26T00:57:30.616925Z"
        },
        "id": "b7a66e0a",
        "papermill": {
          "duration": 3115.454741,
          "end_time": "2021-09-26T00:57:30.617588",
          "exception": false,
          "start_time": "2021-09-26T00:05:35.162847",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc90f21a-9a6e-4b4e-b0ea-2d6ba2f54f18"
      },
      "source": [
        "for fold in range(5):\n",
        "    if fold == 0 :\n",
        "      continue\n",
        "    print();print()\n",
        "    print('-'*50)\n",
        "    print(f'FOLD: {fold}')\n",
        "    print('-'*50)\n",
        "    run(train, fold)\n",
        "    !cp -r ./output /content/drive/MyDrive/datas/chaii2/output"
      ],
      "id": "b7a66e0a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "FOLD: 1\n",
            "--------------------------------------------------\n",
            "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
            "Num examples Train= 20298, Num examples Valid=2771\n",
            "Total Training Steps: 7614, Total Warmup Steps: 761\n",
            "Epoch: 00 [    4/20298 (  0%)], Train Loss: 2.88686\n",
            "Epoch: 00 [   44/20298 (  0%)], Train Loss: 2.96029\n",
            "Epoch: 00 [   84/20298 (  0%)], Train Loss: 2.97485\n",
            "Epoch: 00 [  124/20298 (  1%)], Train Loss: 2.96913\n",
            "Epoch: 00 [  164/20298 (  1%)], Train Loss: 2.96930\n",
            "Epoch: 00 [  204/20298 (  1%)], Train Loss: 2.94456\n",
            "Epoch: 00 [  244/20298 (  1%)], Train Loss: 2.91648\n",
            "Epoch: 00 [  284/20298 (  1%)], Train Loss: 2.89566\n",
            "Epoch: 00 [  324/20298 (  2%)], Train Loss: 2.86009\n",
            "Epoch: 00 [  364/20298 (  2%)], Train Loss: 2.81778\n",
            "Epoch: 00 [  404/20298 (  2%)], Train Loss: 2.77475\n",
            "Epoch: 00 [  444/20298 (  2%)], Train Loss: 2.72434\n",
            "Epoch: 00 [  484/20298 (  2%)], Train Loss: 2.66451\n",
            "Epoch: 00 [  524/20298 (  3%)], Train Loss: 2.61232\n",
            "Epoch: 00 [  564/20298 (  3%)], Train Loss: 2.54561\n",
            "Epoch: 00 [  604/20298 (  3%)], Train Loss: 2.46500\n",
            "Epoch: 00 [  644/20298 (  3%)], Train Loss: 2.36952\n",
            "Epoch: 00 [  684/20298 (  3%)], Train Loss: 2.30789\n",
            "Epoch: 00 [  724/20298 (  4%)], Train Loss: 2.22835\n",
            "Epoch: 00 [  764/20298 (  4%)], Train Loss: 2.16788\n",
            "Epoch: 00 [  804/20298 (  4%)], Train Loss: 2.10304\n",
            "Epoch: 00 [  844/20298 (  4%)], Train Loss: 2.04287\n",
            "Epoch: 00 [  884/20298 (  4%)], Train Loss: 2.00212\n",
            "Epoch: 00 [  924/20298 (  5%)], Train Loss: 1.95018\n",
            "Epoch: 00 [  964/20298 (  5%)], Train Loss: 1.89242\n",
            "Epoch: 00 [ 1004/20298 (  5%)], Train Loss: 1.84828\n",
            "Epoch: 00 [ 1044/20298 (  5%)], Train Loss: 1.79901\n",
            "Epoch: 00 [ 1084/20298 (  5%)], Train Loss: 1.75082\n",
            "Epoch: 00 [ 1124/20298 (  6%)], Train Loss: 1.71640\n",
            "Epoch: 00 [ 1164/20298 (  6%)], Train Loss: 1.67833\n",
            "Epoch: 00 [ 1204/20298 (  6%)], Train Loss: 1.64584\n",
            "Epoch: 00 [ 1244/20298 (  6%)], Train Loss: 1.61091\n",
            "Epoch: 00 [ 1284/20298 (  6%)], Train Loss: 1.57748\n",
            "Epoch: 00 [ 1324/20298 (  7%)], Train Loss: 1.54198\n",
            "Epoch: 00 [ 1364/20298 (  7%)], Train Loss: 1.51163\n",
            "Epoch: 00 [ 1404/20298 (  7%)], Train Loss: 1.48418\n",
            "Epoch: 00 [ 1444/20298 (  7%)], Train Loss: 1.45869\n",
            "Epoch: 00 [ 1484/20298 (  7%)], Train Loss: 1.43749\n",
            "Epoch: 00 [ 1524/20298 (  8%)], Train Loss: 1.41229\n",
            "Epoch: 00 [ 1564/20298 (  8%)], Train Loss: 1.38278\n",
            "Epoch: 00 [ 1604/20298 (  8%)], Train Loss: 1.36268\n",
            "Epoch: 00 [ 1644/20298 (  8%)], Train Loss: 1.34620\n",
            "Epoch: 00 [ 1684/20298 (  8%)], Train Loss: 1.32478\n",
            "Epoch: 00 [ 1724/20298 (  8%)], Train Loss: 1.30542\n",
            "Epoch: 00 [ 1764/20298 (  9%)], Train Loss: 1.28846\n",
            "Epoch: 00 [ 1804/20298 (  9%)], Train Loss: 1.27014\n",
            "Epoch: 00 [ 1844/20298 (  9%)], Train Loss: 1.25046\n",
            "Epoch: 00 [ 1884/20298 (  9%)], Train Loss: 1.23059\n",
            "Epoch: 00 [ 1924/20298 (  9%)], Train Loss: 1.21418\n",
            "Epoch: 00 [ 1964/20298 ( 10%)], Train Loss: 1.19796\n",
            "Epoch: 00 [ 2004/20298 ( 10%)], Train Loss: 1.18525\n",
            "Epoch: 00 [ 2044/20298 ( 10%)], Train Loss: 1.17206\n",
            "Epoch: 00 [ 2084/20298 ( 10%)], Train Loss: 1.15668\n",
            "Epoch: 00 [ 2124/20298 ( 10%)], Train Loss: 1.14229\n",
            "Epoch: 00 [ 2164/20298 ( 11%)], Train Loss: 1.12894\n",
            "Epoch: 00 [ 2204/20298 ( 11%)], Train Loss: 1.11333\n",
            "Epoch: 00 [ 2244/20298 ( 11%)], Train Loss: 1.10096\n",
            "Epoch: 00 [ 2284/20298 ( 11%)], Train Loss: 1.08726\n",
            "Epoch: 00 [ 2324/20298 ( 11%)], Train Loss: 1.07463\n",
            "Epoch: 00 [ 2364/20298 ( 12%)], Train Loss: 1.06138\n",
            "Epoch: 00 [ 2404/20298 ( 12%)], Train Loss: 1.05032\n",
            "Epoch: 00 [ 2444/20298 ( 12%)], Train Loss: 1.03873\n",
            "Epoch: 00 [ 2484/20298 ( 12%)], Train Loss: 1.03111\n",
            "Epoch: 00 [ 2524/20298 ( 12%)], Train Loss: 1.01973\n",
            "Epoch: 00 [ 2564/20298 ( 13%)], Train Loss: 1.00953\n",
            "Epoch: 00 [ 2604/20298 ( 13%)], Train Loss: 1.00005\n",
            "Epoch: 00 [ 2644/20298 ( 13%)], Train Loss: 0.99322\n",
            "Epoch: 00 [ 2684/20298 ( 13%)], Train Loss: 0.98333\n",
            "Epoch: 00 [ 2724/20298 ( 13%)], Train Loss: 0.97882\n",
            "Epoch: 00 [ 2764/20298 ( 14%)], Train Loss: 0.96861\n",
            "Epoch: 00 [ 2804/20298 ( 14%)], Train Loss: 0.96046\n",
            "Epoch: 00 [ 2844/20298 ( 14%)], Train Loss: 0.95514\n",
            "Epoch: 00 [ 2884/20298 ( 14%)], Train Loss: 0.94552\n",
            "Epoch: 00 [ 2924/20298 ( 14%)], Train Loss: 0.93976\n",
            "Epoch: 00 [ 2964/20298 ( 15%)], Train Loss: 0.93447\n",
            "Epoch: 00 [ 3004/20298 ( 15%)], Train Loss: 0.92908\n",
            "Epoch: 00 [ 3044/20298 ( 15%)], Train Loss: 0.92024\n",
            "Epoch: 00 [ 3084/20298 ( 15%)], Train Loss: 0.91317\n",
            "Epoch: 00 [ 3124/20298 ( 15%)], Train Loss: 0.90666\n",
            "Epoch: 00 [ 3164/20298 ( 16%)], Train Loss: 0.90054\n",
            "Epoch: 00 [ 3204/20298 ( 16%)], Train Loss: 0.89415\n",
            "Epoch: 00 [ 3244/20298 ( 16%)], Train Loss: 0.88732\n",
            "Epoch: 00 [ 3284/20298 ( 16%)], Train Loss: 0.88263\n",
            "Epoch: 00 [ 3324/20298 ( 16%)], Train Loss: 0.87728\n",
            "Epoch: 00 [ 3364/20298 ( 17%)], Train Loss: 0.86966\n",
            "Epoch: 00 [ 3404/20298 ( 17%)], Train Loss: 0.86388\n",
            "Epoch: 00 [ 3444/20298 ( 17%)], Train Loss: 0.86274\n",
            "Epoch: 00 [ 3484/20298 ( 17%)], Train Loss: 0.85714\n",
            "Epoch: 00 [ 3524/20298 ( 17%)], Train Loss: 0.85099\n",
            "Epoch: 00 [ 3564/20298 ( 18%)], Train Loss: 0.84579\n",
            "Epoch: 00 [ 3604/20298 ( 18%)], Train Loss: 0.83831\n",
            "Epoch: 00 [ 3644/20298 ( 18%)], Train Loss: 0.83131\n",
            "Epoch: 00 [ 3684/20298 ( 18%)], Train Loss: 0.82662\n",
            "Epoch: 00 [ 3724/20298 ( 18%)], Train Loss: 0.82187\n",
            "Epoch: 00 [ 3764/20298 ( 19%)], Train Loss: 0.81964\n",
            "Epoch: 00 [ 3804/20298 ( 19%)], Train Loss: 0.81569\n",
            "Epoch: 00 [ 3844/20298 ( 19%)], Train Loss: 0.81085\n",
            "Epoch: 00 [ 3884/20298 ( 19%)], Train Loss: 0.80847\n",
            "Epoch: 00 [ 3924/20298 ( 19%)], Train Loss: 0.80432\n",
            "Epoch: 00 [ 3964/20298 ( 20%)], Train Loss: 0.80043\n",
            "Epoch: 00 [ 4004/20298 ( 20%)], Train Loss: 0.79508\n",
            "Epoch: 00 [ 4044/20298 ( 20%)], Train Loss: 0.79110\n",
            "Epoch: 00 [ 4084/20298 ( 20%)], Train Loss: 0.78865\n",
            "Epoch: 00 [ 4124/20298 ( 20%)], Train Loss: 0.78515\n",
            "Epoch: 00 [ 4164/20298 ( 21%)], Train Loss: 0.78050\n",
            "Epoch: 00 [ 4204/20298 ( 21%)], Train Loss: 0.77719\n",
            "Epoch: 00 [ 4244/20298 ( 21%)], Train Loss: 0.77544\n",
            "Epoch: 00 [ 4284/20298 ( 21%)], Train Loss: 0.77365\n",
            "Epoch: 00 [ 4324/20298 ( 21%)], Train Loss: 0.77088\n",
            "Epoch: 00 [ 4364/20298 ( 21%)], Train Loss: 0.76885\n",
            "Epoch: 00 [ 4404/20298 ( 22%)], Train Loss: 0.76581\n",
            "Epoch: 00 [ 4444/20298 ( 22%)], Train Loss: 0.76308\n",
            "Epoch: 00 [ 4484/20298 ( 22%)], Train Loss: 0.75984\n",
            "Epoch: 00 [ 4524/20298 ( 22%)], Train Loss: 0.75543\n",
            "Epoch: 00 [ 4564/20298 ( 22%)], Train Loss: 0.75141\n",
            "Epoch: 00 [ 4604/20298 ( 23%)], Train Loss: 0.74812\n",
            "Epoch: 00 [ 4644/20298 ( 23%)], Train Loss: 0.74632\n",
            "Epoch: 00 [ 4684/20298 ( 23%)], Train Loss: 0.74364\n",
            "Epoch: 00 [ 4724/20298 ( 23%)], Train Loss: 0.73924\n",
            "Epoch: 00 [ 4764/20298 ( 23%)], Train Loss: 0.73890\n",
            "Epoch: 00 [ 4804/20298 ( 24%)], Train Loss: 0.73491\n",
            "Epoch: 00 [ 4844/20298 ( 24%)], Train Loss: 0.73232\n",
            "Epoch: 00 [ 4884/20298 ( 24%)], Train Loss: 0.72779\n",
            "Epoch: 00 [ 4924/20298 ( 24%)], Train Loss: 0.72489\n",
            "Epoch: 00 [ 4964/20298 ( 24%)], Train Loss: 0.72317\n",
            "Epoch: 00 [ 5004/20298 ( 25%)], Train Loss: 0.71923\n",
            "Epoch: 00 [ 5044/20298 ( 25%)], Train Loss: 0.71553\n",
            "Epoch: 00 [ 5084/20298 ( 25%)], Train Loss: 0.71287\n",
            "Epoch: 00 [ 5124/20298 ( 25%)], Train Loss: 0.71049\n",
            "Epoch: 00 [ 5164/20298 ( 25%)], Train Loss: 0.70760\n",
            "Epoch: 00 [ 5204/20298 ( 26%)], Train Loss: 0.70548\n",
            "Epoch: 00 [ 5244/20298 ( 26%)], Train Loss: 0.70227\n",
            "Epoch: 00 [ 5284/20298 ( 26%)], Train Loss: 0.69955\n",
            "Epoch: 00 [ 5324/20298 ( 26%)], Train Loss: 0.69722\n",
            "Epoch: 00 [ 5364/20298 ( 26%)], Train Loss: 0.69462\n",
            "Epoch: 00 [ 5404/20298 ( 27%)], Train Loss: 0.69140\n",
            "Epoch: 00 [ 5444/20298 ( 27%)], Train Loss: 0.68919\n",
            "Epoch: 00 [ 5484/20298 ( 27%)], Train Loss: 0.68675\n",
            "Epoch: 00 [ 5524/20298 ( 27%)], Train Loss: 0.68598\n",
            "Epoch: 00 [ 5564/20298 ( 27%)], Train Loss: 0.68300\n",
            "Epoch: 00 [ 5604/20298 ( 28%)], Train Loss: 0.68137\n",
            "Epoch: 00 [ 5644/20298 ( 28%)], Train Loss: 0.67979\n",
            "Epoch: 00 [ 5684/20298 ( 28%)], Train Loss: 0.67720\n",
            "Epoch: 00 [ 5724/20298 ( 28%)], Train Loss: 0.67562\n",
            "Epoch: 00 [ 5764/20298 ( 28%)], Train Loss: 0.67299\n",
            "Epoch: 00 [ 5804/20298 ( 29%)], Train Loss: 0.67135\n",
            "Epoch: 00 [ 5844/20298 ( 29%)], Train Loss: 0.66878\n",
            "Epoch: 00 [ 5884/20298 ( 29%)], Train Loss: 0.66600\n",
            "Epoch: 00 [ 5924/20298 ( 29%)], Train Loss: 0.66616\n",
            "Epoch: 00 [ 5964/20298 ( 29%)], Train Loss: 0.66562\n",
            "Epoch: 00 [ 6004/20298 ( 30%)], Train Loss: 0.66376\n",
            "Epoch: 00 [ 6044/20298 ( 30%)], Train Loss: 0.66420\n",
            "Epoch: 00 [ 6084/20298 ( 30%)], Train Loss: 0.66337\n",
            "Epoch: 00 [ 6124/20298 ( 30%)], Train Loss: 0.66189\n",
            "Epoch: 00 [ 6164/20298 ( 30%)], Train Loss: 0.66021\n",
            "Epoch: 00 [ 6204/20298 ( 31%)], Train Loss: 0.65780\n",
            "Epoch: 00 [ 6244/20298 ( 31%)], Train Loss: 0.65674\n",
            "Epoch: 00 [ 6284/20298 ( 31%)], Train Loss: 0.65589\n",
            "Epoch: 00 [ 6324/20298 ( 31%)], Train Loss: 0.65444\n",
            "Epoch: 00 [ 6364/20298 ( 31%)], Train Loss: 0.65268\n",
            "Epoch: 00 [ 6404/20298 ( 32%)], Train Loss: 0.65206\n",
            "Epoch: 00 [ 6444/20298 ( 32%)], Train Loss: 0.65095\n",
            "Epoch: 00 [ 6484/20298 ( 32%)], Train Loss: 0.65052\n",
            "Epoch: 00 [ 6524/20298 ( 32%)], Train Loss: 0.65008\n",
            "Epoch: 00 [ 6564/20298 ( 32%)], Train Loss: 0.64793\n",
            "Epoch: 00 [ 6604/20298 ( 33%)], Train Loss: 0.64695\n",
            "Epoch: 00 [ 6644/20298 ( 33%)], Train Loss: 0.64515\n",
            "Epoch: 00 [ 6684/20298 ( 33%)], Train Loss: 0.64347\n",
            "Epoch: 00 [ 6724/20298 ( 33%)], Train Loss: 0.64240\n",
            "Epoch: 00 [ 6764/20298 ( 33%)], Train Loss: 0.64108\n",
            "Epoch: 00 [ 6804/20298 ( 34%)], Train Loss: 0.63914\n",
            "Epoch: 00 [ 6844/20298 ( 34%)], Train Loss: 0.63916\n",
            "Epoch: 00 [ 6884/20298 ( 34%)], Train Loss: 0.63810\n",
            "Epoch: 00 [ 6924/20298 ( 34%)], Train Loss: 0.63707\n",
            "Epoch: 00 [ 6964/20298 ( 34%)], Train Loss: 0.63647\n",
            "Epoch: 00 [ 7004/20298 ( 35%)], Train Loss: 0.63680\n",
            "Epoch: 00 [ 7044/20298 ( 35%)], Train Loss: 0.63539\n",
            "Epoch: 00 [ 7084/20298 ( 35%)], Train Loss: 0.63407\n",
            "Epoch: 00 [ 7124/20298 ( 35%)], Train Loss: 0.63239\n",
            "Epoch: 00 [ 7164/20298 ( 35%)], Train Loss: 0.63114\n",
            "Epoch: 00 [ 7204/20298 ( 35%)], Train Loss: 0.63150\n",
            "Epoch: 00 [ 7244/20298 ( 36%)], Train Loss: 0.63074\n",
            "Epoch: 00 [ 7284/20298 ( 36%)], Train Loss: 0.63084\n",
            "Epoch: 00 [ 7324/20298 ( 36%)], Train Loss: 0.63151\n",
            "Epoch: 00 [ 7364/20298 ( 36%)], Train Loss: 0.63041\n",
            "Epoch: 00 [ 7404/20298 ( 36%)], Train Loss: 0.62815\n",
            "Epoch: 00 [ 7444/20298 ( 37%)], Train Loss: 0.62618\n",
            "Epoch: 00 [ 7484/20298 ( 37%)], Train Loss: 0.62427\n",
            "Epoch: 00 [ 7524/20298 ( 37%)], Train Loss: 0.62516\n",
            "Epoch: 00 [ 7564/20298 ( 37%)], Train Loss: 0.62448\n",
            "Epoch: 00 [ 7604/20298 ( 37%)], Train Loss: 0.62417\n",
            "Epoch: 00 [ 7644/20298 ( 38%)], Train Loss: 0.62344\n",
            "Epoch: 00 [ 7684/20298 ( 38%)], Train Loss: 0.62181\n",
            "Epoch: 00 [ 7724/20298 ( 38%)], Train Loss: 0.62086\n",
            "Epoch: 00 [ 7764/20298 ( 38%)], Train Loss: 0.61981\n",
            "Epoch: 00 [ 7804/20298 ( 38%)], Train Loss: 0.61814\n",
            "Epoch: 00 [ 7844/20298 ( 39%)], Train Loss: 0.61709\n",
            "Epoch: 00 [ 7884/20298 ( 39%)], Train Loss: 0.61585\n",
            "Epoch: 00 [ 7924/20298 ( 39%)], Train Loss: 0.61456\n",
            "Epoch: 00 [ 7964/20298 ( 39%)], Train Loss: 0.61340\n",
            "Epoch: 00 [ 8004/20298 ( 39%)], Train Loss: 0.61259\n",
            "Epoch: 00 [ 8044/20298 ( 40%)], Train Loss: 0.61132\n",
            "Epoch: 00 [ 8084/20298 ( 40%)], Train Loss: 0.60984\n",
            "Epoch: 00 [ 8124/20298 ( 40%)], Train Loss: 0.60793\n",
            "Epoch: 00 [ 8164/20298 ( 40%)], Train Loss: 0.60717\n",
            "Epoch: 00 [ 8204/20298 ( 40%)], Train Loss: 0.60652\n",
            "Epoch: 00 [ 8244/20298 ( 41%)], Train Loss: 0.60594\n",
            "Epoch: 00 [ 8284/20298 ( 41%)], Train Loss: 0.60518\n",
            "Epoch: 00 [ 8324/20298 ( 41%)], Train Loss: 0.60439\n",
            "Epoch: 00 [ 8364/20298 ( 41%)], Train Loss: 0.60291\n",
            "Epoch: 00 [ 8404/20298 ( 41%)], Train Loss: 0.60197\n",
            "Epoch: 00 [ 8444/20298 ( 42%)], Train Loss: 0.60134\n",
            "Epoch: 00 [ 8484/20298 ( 42%)], Train Loss: 0.60123\n",
            "Epoch: 00 [ 8524/20298 ( 42%)], Train Loss: 0.59980\n",
            "Epoch: 00 [ 8564/20298 ( 42%)], Train Loss: 0.59844\n",
            "Epoch: 00 [ 8604/20298 ( 42%)], Train Loss: 0.59734\n",
            "Epoch: 00 [ 8644/20298 ( 43%)], Train Loss: 0.59654\n",
            "Epoch: 00 [ 8684/20298 ( 43%)], Train Loss: 0.59500\n",
            "Epoch: 00 [ 8724/20298 ( 43%)], Train Loss: 0.59569\n",
            "Epoch: 00 [ 8764/20298 ( 43%)], Train Loss: 0.59461\n",
            "Epoch: 00 [ 8804/20298 ( 43%)], Train Loss: 0.59404\n",
            "Epoch: 00 [ 8844/20298 ( 44%)], Train Loss: 0.59323\n",
            "Epoch: 00 [ 8884/20298 ( 44%)], Train Loss: 0.59217\n",
            "Epoch: 00 [ 8924/20298 ( 44%)], Train Loss: 0.59161\n",
            "Epoch: 00 [ 8964/20298 ( 44%)], Train Loss: 0.59135\n",
            "Epoch: 00 [ 9004/20298 ( 44%)], Train Loss: 0.59062\n",
            "Epoch: 00 [ 9044/20298 ( 45%)], Train Loss: 0.58981\n",
            "Epoch: 00 [ 9084/20298 ( 45%)], Train Loss: 0.58891\n",
            "Epoch: 00 [ 9124/20298 ( 45%)], Train Loss: 0.58729\n",
            "Epoch: 00 [ 9164/20298 ( 45%)], Train Loss: 0.58679\n",
            "Epoch: 00 [ 9204/20298 ( 45%)], Train Loss: 0.58546\n",
            "Epoch: 00 [ 9244/20298 ( 46%)], Train Loss: 0.58456\n",
            "Epoch: 00 [ 9284/20298 ( 46%)], Train Loss: 0.58363\n",
            "Epoch: 00 [ 9324/20298 ( 46%)], Train Loss: 0.58311\n",
            "Epoch: 00 [ 9364/20298 ( 46%)], Train Loss: 0.58174\n",
            "Epoch: 00 [ 9404/20298 ( 46%)], Train Loss: 0.58012\n",
            "Epoch: 00 [ 9444/20298 ( 47%)], Train Loss: 0.57837\n",
            "Epoch: 00 [ 9484/20298 ( 47%)], Train Loss: 0.57759\n",
            "Epoch: 00 [ 9524/20298 ( 47%)], Train Loss: 0.57712\n",
            "Epoch: 00 [ 9564/20298 ( 47%)], Train Loss: 0.57580\n",
            "Epoch: 00 [ 9604/20298 ( 47%)], Train Loss: 0.57484\n",
            "Epoch: 00 [ 9644/20298 ( 48%)], Train Loss: 0.57478\n",
            "Epoch: 00 [ 9684/20298 ( 48%)], Train Loss: 0.57448\n",
            "Epoch: 00 [ 9724/20298 ( 48%)], Train Loss: 0.57334\n",
            "Epoch: 00 [ 9764/20298 ( 48%)], Train Loss: 0.57254\n",
            "Epoch: 00 [ 9804/20298 ( 48%)], Train Loss: 0.57232\n",
            "Epoch: 00 [ 9844/20298 ( 48%)], Train Loss: 0.57138\n",
            "Epoch: 00 [ 9884/20298 ( 49%)], Train Loss: 0.57085\n",
            "Epoch: 00 [ 9924/20298 ( 49%)], Train Loss: 0.56999\n",
            "Epoch: 00 [ 9964/20298 ( 49%)], Train Loss: 0.56859\n",
            "Epoch: 00 [10004/20298 ( 49%)], Train Loss: 0.56807\n",
            "Epoch: 00 [10044/20298 ( 49%)], Train Loss: 0.56677\n",
            "Epoch: 00 [10084/20298 ( 50%)], Train Loss: 0.56520\n",
            "Epoch: 00 [10124/20298 ( 50%)], Train Loss: 0.56388\n",
            "Epoch: 00 [10164/20298 ( 50%)], Train Loss: 0.56296\n",
            "Epoch: 00 [10204/20298 ( 50%)], Train Loss: 0.56249\n",
            "Epoch: 00 [10244/20298 ( 50%)], Train Loss: 0.56088\n",
            "Epoch: 00 [10284/20298 ( 51%)], Train Loss: 0.56036\n",
            "Epoch: 00 [10324/20298 ( 51%)], Train Loss: 0.55910\n",
            "Epoch: 00 [10364/20298 ( 51%)], Train Loss: 0.55827\n",
            "Epoch: 00 [10404/20298 ( 51%)], Train Loss: 0.55736\n",
            "Epoch: 00 [10444/20298 ( 51%)], Train Loss: 0.55637\n",
            "Epoch: 00 [10484/20298 ( 52%)], Train Loss: 0.55555\n",
            "Epoch: 00 [10524/20298 ( 52%)], Train Loss: 0.55482\n",
            "Epoch: 00 [10564/20298 ( 52%)], Train Loss: 0.55363\n",
            "Epoch: 00 [10604/20298 ( 52%)], Train Loss: 0.55296\n",
            "Epoch: 00 [10644/20298 ( 52%)], Train Loss: 0.55285\n",
            "Epoch: 00 [10684/20298 ( 53%)], Train Loss: 0.55169\n",
            "Epoch: 00 [10724/20298 ( 53%)], Train Loss: 0.55143\n",
            "Epoch: 00 [10764/20298 ( 53%)], Train Loss: 0.55065\n",
            "Epoch: 00 [10804/20298 ( 53%)], Train Loss: 0.54966\n",
            "Epoch: 00 [10844/20298 ( 53%)], Train Loss: 0.54854\n",
            "Epoch: 00 [10884/20298 ( 54%)], Train Loss: 0.54786\n",
            "Epoch: 00 [10924/20298 ( 54%)], Train Loss: 0.54704\n",
            "Epoch: 00 [10964/20298 ( 54%)], Train Loss: 0.54632\n",
            "Epoch: 00 [11004/20298 ( 54%)], Train Loss: 0.54541\n",
            "Epoch: 00 [11044/20298 ( 54%)], Train Loss: 0.54448\n",
            "Epoch: 00 [11084/20298 ( 55%)], Train Loss: 0.54362\n",
            "Epoch: 00 [11124/20298 ( 55%)], Train Loss: 0.54374\n",
            "Epoch: 00 [11164/20298 ( 55%)], Train Loss: 0.54269\n",
            "Epoch: 00 [11204/20298 ( 55%)], Train Loss: 0.54267\n",
            "Epoch: 00 [11244/20298 ( 55%)], Train Loss: 0.54157\n",
            "Epoch: 00 [11284/20298 ( 56%)], Train Loss: 0.54063\n",
            "Epoch: 00 [11324/20298 ( 56%)], Train Loss: 0.54007\n",
            "Epoch: 00 [11364/20298 ( 56%)], Train Loss: 0.54040\n",
            "Epoch: 00 [11404/20298 ( 56%)], Train Loss: 0.54114\n",
            "Epoch: 00 [11444/20298 ( 56%)], Train Loss: 0.54040\n",
            "Epoch: 00 [11484/20298 ( 57%)], Train Loss: 0.53981\n",
            "Epoch: 00 [11524/20298 ( 57%)], Train Loss: 0.53952\n",
            "Epoch: 00 [11564/20298 ( 57%)], Train Loss: 0.53841\n",
            "Epoch: 00 [11604/20298 ( 57%)], Train Loss: 0.53779\n",
            "Epoch: 00 [11644/20298 ( 57%)], Train Loss: 0.53764\n",
            "Epoch: 00 [11684/20298 ( 58%)], Train Loss: 0.53712\n",
            "Epoch: 00 [11724/20298 ( 58%)], Train Loss: 0.53627\n",
            "Epoch: 00 [11764/20298 ( 58%)], Train Loss: 0.53554\n",
            "Epoch: 00 [11804/20298 ( 58%)], Train Loss: 0.53494\n",
            "Epoch: 00 [11844/20298 ( 58%)], Train Loss: 0.53435\n",
            "Epoch: 00 [11884/20298 ( 59%)], Train Loss: 0.53336\n",
            "Epoch: 00 [11924/20298 ( 59%)], Train Loss: 0.53248\n",
            "Epoch: 00 [11964/20298 ( 59%)], Train Loss: 0.53204\n",
            "Epoch: 00 [12004/20298 ( 59%)], Train Loss: 0.53158\n",
            "Epoch: 00 [12044/20298 ( 59%)], Train Loss: 0.53128\n",
            "Epoch: 00 [12084/20298 ( 60%)], Train Loss: 0.53005\n",
            "Epoch: 00 [12124/20298 ( 60%)], Train Loss: 0.52929\n",
            "Epoch: 00 [12164/20298 ( 60%)], Train Loss: 0.52908\n",
            "Epoch: 00 [12204/20298 ( 60%)], Train Loss: 0.52898\n",
            "Epoch: 00 [12244/20298 ( 60%)], Train Loss: 0.52847\n",
            "Epoch: 00 [12284/20298 ( 61%)], Train Loss: 0.52798\n",
            "Epoch: 00 [12324/20298 ( 61%)], Train Loss: 0.52814\n",
            "Epoch: 00 [12364/20298 ( 61%)], Train Loss: 0.52737\n",
            "Epoch: 00 [12404/20298 ( 61%)], Train Loss: 0.52709\n",
            "Epoch: 00 [12444/20298 ( 61%)], Train Loss: 0.52667\n",
            "Epoch: 00 [12484/20298 ( 62%)], Train Loss: 0.52622\n",
            "Epoch: 00 [12524/20298 ( 62%)], Train Loss: 0.52597\n",
            "Epoch: 00 [12564/20298 ( 62%)], Train Loss: 0.52532\n",
            "Epoch: 00 [12604/20298 ( 62%)], Train Loss: 0.52459\n",
            "Epoch: 00 [12644/20298 ( 62%)], Train Loss: 0.52417\n",
            "Epoch: 00 [12684/20298 ( 62%)], Train Loss: 0.52413\n",
            "Epoch: 00 [12724/20298 ( 63%)], Train Loss: 0.52409\n",
            "Epoch: 00 [12764/20298 ( 63%)], Train Loss: 0.52375\n",
            "Epoch: 00 [12804/20298 ( 63%)], Train Loss: 0.52270\n",
            "Epoch: 00 [12844/20298 ( 63%)], Train Loss: 0.52235\n",
            "Epoch: 00 [12884/20298 ( 63%)], Train Loss: 0.52186\n",
            "Epoch: 00 [12924/20298 ( 64%)], Train Loss: 0.52136\n",
            "Epoch: 00 [12964/20298 ( 64%)], Train Loss: 0.52073\n",
            "Epoch: 00 [13004/20298 ( 64%)], Train Loss: 0.52005\n",
            "Epoch: 00 [13044/20298 ( 64%)], Train Loss: 0.51900\n",
            "Epoch: 00 [13084/20298 ( 64%)], Train Loss: 0.51861\n",
            "Epoch: 00 [13124/20298 ( 65%)], Train Loss: 0.51822\n",
            "Epoch: 00 [13164/20298 ( 65%)], Train Loss: 0.51821\n",
            "Epoch: 00 [13204/20298 ( 65%)], Train Loss: 0.51838\n",
            "Epoch: 00 [13244/20298 ( 65%)], Train Loss: 0.51803\n",
            "Epoch: 00 [13284/20298 ( 65%)], Train Loss: 0.51713\n",
            "Epoch: 00 [13324/20298 ( 66%)], Train Loss: 0.51674\n",
            "Epoch: 00 [13364/20298 ( 66%)], Train Loss: 0.51663\n",
            "Epoch: 00 [13404/20298 ( 66%)], Train Loss: 0.51615\n",
            "Epoch: 00 [13444/20298 ( 66%)], Train Loss: 0.51659\n",
            "Epoch: 00 [13484/20298 ( 66%)], Train Loss: 0.51592\n",
            "Epoch: 00 [13524/20298 ( 67%)], Train Loss: 0.51577\n",
            "Epoch: 00 [13564/20298 ( 67%)], Train Loss: 0.51495\n",
            "Epoch: 00 [13604/20298 ( 67%)], Train Loss: 0.51436\n",
            "Epoch: 00 [13644/20298 ( 67%)], Train Loss: 0.51385\n",
            "Epoch: 00 [13684/20298 ( 67%)], Train Loss: 0.51318\n",
            "Epoch: 00 [13724/20298 ( 68%)], Train Loss: 0.51276\n",
            "Epoch: 00 [13764/20298 ( 68%)], Train Loss: 0.51194\n",
            "Epoch: 00 [13804/20298 ( 68%)], Train Loss: 0.51127\n",
            "Epoch: 00 [13844/20298 ( 68%)], Train Loss: 0.51150\n",
            "Epoch: 00 [13884/20298 ( 68%)], Train Loss: 0.51094\n",
            "Epoch: 00 [13924/20298 ( 69%)], Train Loss: 0.51041\n",
            "Epoch: 00 [13964/20298 ( 69%)], Train Loss: 0.50997\n",
            "Epoch: 00 [14004/20298 ( 69%)], Train Loss: 0.51047\n",
            "Epoch: 00 [14044/20298 ( 69%)], Train Loss: 0.50979\n",
            "Epoch: 00 [14084/20298 ( 69%)], Train Loss: 0.50953\n",
            "Epoch: 00 [14124/20298 ( 70%)], Train Loss: 0.50940\n",
            "Epoch: 00 [14164/20298 ( 70%)], Train Loss: 0.50887\n",
            "Epoch: 00 [14204/20298 ( 70%)], Train Loss: 0.50865\n",
            "Epoch: 00 [14244/20298 ( 70%)], Train Loss: 0.50799\n",
            "Epoch: 00 [14284/20298 ( 70%)], Train Loss: 0.50754\n",
            "Epoch: 00 [14324/20298 ( 71%)], Train Loss: 0.50735\n",
            "Epoch: 00 [14364/20298 ( 71%)], Train Loss: 0.50684\n",
            "Epoch: 00 [14404/20298 ( 71%)], Train Loss: 0.50651\n",
            "Epoch: 00 [14444/20298 ( 71%)], Train Loss: 0.50586\n",
            "Epoch: 00 [14484/20298 ( 71%)], Train Loss: 0.50566\n",
            "Epoch: 00 [14524/20298 ( 72%)], Train Loss: 0.50542\n",
            "Epoch: 00 [14564/20298 ( 72%)], Train Loss: 0.50471\n",
            "Epoch: 00 [14604/20298 ( 72%)], Train Loss: 0.50446\n",
            "Epoch: 00 [14644/20298 ( 72%)], Train Loss: 0.50388\n",
            "Epoch: 00 [14684/20298 ( 72%)], Train Loss: 0.50328\n",
            "Epoch: 00 [14724/20298 ( 73%)], Train Loss: 0.50307\n",
            "Epoch: 00 [14764/20298 ( 73%)], Train Loss: 0.50264\n",
            "Epoch: 00 [14804/20298 ( 73%)], Train Loss: 0.50229\n",
            "Epoch: 00 [14844/20298 ( 73%)], Train Loss: 0.50198\n",
            "Epoch: 00 [14884/20298 ( 73%)], Train Loss: 0.50193\n",
            "Epoch: 00 [14924/20298 ( 74%)], Train Loss: 0.50171\n",
            "Epoch: 00 [14964/20298 ( 74%)], Train Loss: 0.50111\n",
            "Epoch: 00 [15004/20298 ( 74%)], Train Loss: 0.50102\n",
            "Epoch: 00 [15044/20298 ( 74%)], Train Loss: 0.50062\n",
            "Epoch: 00 [15084/20298 ( 74%)], Train Loss: 0.50040\n",
            "Epoch: 00 [15124/20298 ( 75%)], Train Loss: 0.49993\n",
            "Epoch: 00 [15164/20298 ( 75%)], Train Loss: 0.49971\n",
            "Epoch: 00 [15204/20298 ( 75%)], Train Loss: 0.49932\n",
            "Epoch: 00 [15244/20298 ( 75%)], Train Loss: 0.49847\n",
            "Epoch: 00 [15284/20298 ( 75%)], Train Loss: 0.49793\n",
            "Epoch: 00 [15324/20298 ( 75%)], Train Loss: 0.49763\n",
            "Epoch: 00 [15364/20298 ( 76%)], Train Loss: 0.49732\n",
            "Epoch: 00 [15404/20298 ( 76%)], Train Loss: 0.49695\n",
            "Epoch: 00 [15444/20298 ( 76%)], Train Loss: 0.49669\n",
            "Epoch: 00 [15484/20298 ( 76%)], Train Loss: 0.49683\n",
            "Epoch: 00 [15524/20298 ( 76%)], Train Loss: 0.49661\n",
            "Epoch: 00 [15564/20298 ( 77%)], Train Loss: 0.49695\n",
            "Epoch: 00 [15604/20298 ( 77%)], Train Loss: 0.49656\n",
            "Epoch: 00 [15644/20298 ( 77%)], Train Loss: 0.49604\n",
            "Epoch: 00 [15684/20298 ( 77%)], Train Loss: 0.49591\n",
            "Epoch: 00 [15724/20298 ( 77%)], Train Loss: 0.49624\n",
            "Epoch: 00 [15764/20298 ( 78%)], Train Loss: 0.49618\n",
            "Epoch: 00 [15804/20298 ( 78%)], Train Loss: 0.49587\n",
            "Epoch: 00 [15844/20298 ( 78%)], Train Loss: 0.49559\n",
            "Epoch: 00 [15884/20298 ( 78%)], Train Loss: 0.49559\n",
            "Epoch: 00 [15924/20298 ( 78%)], Train Loss: 0.49530\n",
            "Epoch: 00 [15964/20298 ( 79%)], Train Loss: 0.49547\n",
            "Epoch: 00 [16004/20298 ( 79%)], Train Loss: 0.49492\n",
            "Epoch: 00 [16044/20298 ( 79%)], Train Loss: 0.49467\n",
            "Epoch: 00 [16084/20298 ( 79%)], Train Loss: 0.49424\n",
            "Epoch: 00 [16124/20298 ( 79%)], Train Loss: 0.49449\n",
            "Epoch: 00 [16164/20298 ( 80%)], Train Loss: 0.49423\n",
            "Epoch: 00 [16204/20298 ( 80%)], Train Loss: 0.49383\n",
            "Epoch: 00 [16244/20298 ( 80%)], Train Loss: 0.49325\n",
            "Epoch: 00 [16284/20298 ( 80%)], Train Loss: 0.49283\n",
            "Epoch: 00 [16324/20298 ( 80%)], Train Loss: 0.49207\n",
            "Epoch: 00 [16364/20298 ( 81%)], Train Loss: 0.49128\n",
            "Epoch: 00 [16404/20298 ( 81%)], Train Loss: 0.49157\n",
            "Epoch: 00 [16444/20298 ( 81%)], Train Loss: 0.49157\n",
            "Epoch: 00 [16484/20298 ( 81%)], Train Loss: 0.49087\n",
            "Epoch: 00 [16524/20298 ( 81%)], Train Loss: 0.49044\n",
            "Epoch: 00 [16564/20298 ( 82%)], Train Loss: 0.48980\n",
            "Epoch: 00 [16604/20298 ( 82%)], Train Loss: 0.48957\n",
            "Epoch: 00 [16644/20298 ( 82%)], Train Loss: 0.48895\n",
            "Epoch: 00 [16684/20298 ( 82%)], Train Loss: 0.48831\n",
            "Epoch: 00 [16724/20298 ( 82%)], Train Loss: 0.48803\n",
            "Epoch: 00 [16764/20298 ( 83%)], Train Loss: 0.48768\n",
            "Epoch: 00 [16804/20298 ( 83%)], Train Loss: 0.48729\n",
            "Epoch: 00 [16844/20298 ( 83%)], Train Loss: 0.48699\n",
            "Epoch: 00 [16884/20298 ( 83%)], Train Loss: 0.48663\n",
            "Epoch: 00 [16924/20298 ( 83%)], Train Loss: 0.48603\n",
            "Epoch: 00 [16964/20298 ( 84%)], Train Loss: 0.48571\n",
            "Epoch: 00 [17004/20298 ( 84%)], Train Loss: 0.48549\n",
            "Epoch: 00 [17044/20298 ( 84%)], Train Loss: 0.48504\n",
            "Epoch: 00 [17084/20298 ( 84%)], Train Loss: 0.48431\n",
            "Epoch: 00 [17124/20298 ( 84%)], Train Loss: 0.48387\n",
            "Epoch: 00 [17164/20298 ( 85%)], Train Loss: 0.48372\n",
            "Epoch: 00 [17204/20298 ( 85%)], Train Loss: 0.48359\n",
            "Epoch: 00 [17244/20298 ( 85%)], Train Loss: 0.48406\n",
            "Epoch: 00 [17284/20298 ( 85%)], Train Loss: 0.48385\n",
            "Epoch: 00 [17324/20298 ( 85%)], Train Loss: 0.48324\n",
            "Epoch: 00 [17364/20298 ( 86%)], Train Loss: 0.48358\n",
            "Epoch: 00 [17404/20298 ( 86%)], Train Loss: 0.48352\n",
            "Epoch: 00 [17444/20298 ( 86%)], Train Loss: 0.48358\n",
            "Epoch: 00 [17484/20298 ( 86%)], Train Loss: 0.48310\n",
            "Epoch: 00 [17524/20298 ( 86%)], Train Loss: 0.48281\n",
            "Epoch: 00 [17564/20298 ( 87%)], Train Loss: 0.48227\n",
            "Epoch: 00 [17604/20298 ( 87%)], Train Loss: 0.48241\n",
            "Epoch: 00 [17644/20298 ( 87%)], Train Loss: 0.48189\n",
            "Epoch: 00 [17684/20298 ( 87%)], Train Loss: 0.48120\n",
            "Epoch: 00 [17724/20298 ( 87%)], Train Loss: 0.48108\n",
            "Epoch: 00 [17764/20298 ( 88%)], Train Loss: 0.48049\n",
            "Epoch: 00 [17804/20298 ( 88%)], Train Loss: 0.48024\n",
            "Epoch: 00 [17844/20298 ( 88%)], Train Loss: 0.48000\n",
            "Epoch: 00 [17884/20298 ( 88%)], Train Loss: 0.47949\n",
            "Epoch: 00 [17924/20298 ( 88%)], Train Loss: 0.47954\n",
            "Epoch: 00 [17964/20298 ( 89%)], Train Loss: 0.47941\n",
            "Epoch: 00 [18004/20298 ( 89%)], Train Loss: 0.47918\n",
            "Epoch: 00 [18044/20298 ( 89%)], Train Loss: 0.47905\n",
            "Epoch: 00 [18084/20298 ( 89%)], Train Loss: 0.47877\n",
            "Epoch: 00 [18124/20298 ( 89%)], Train Loss: 0.47870\n",
            "Epoch: 00 [18164/20298 ( 89%)], Train Loss: 0.47817\n",
            "Epoch: 00 [18204/20298 ( 90%)], Train Loss: 0.47822\n",
            "Epoch: 00 [18244/20298 ( 90%)], Train Loss: 0.47787\n",
            "Epoch: 00 [18284/20298 ( 90%)], Train Loss: 0.47818\n",
            "Epoch: 00 [18324/20298 ( 90%)], Train Loss: 0.47781\n",
            "Epoch: 00 [18364/20298 ( 90%)], Train Loss: 0.47780\n",
            "Epoch: 00 [18404/20298 ( 91%)], Train Loss: 0.47789\n",
            "Epoch: 00 [18444/20298 ( 91%)], Train Loss: 0.47729\n",
            "Epoch: 00 [18484/20298 ( 91%)], Train Loss: 0.47692\n",
            "Epoch: 00 [18524/20298 ( 91%)], Train Loss: 0.47674\n",
            "Epoch: 00 [18564/20298 ( 91%)], Train Loss: 0.47659\n",
            "Epoch: 00 [18604/20298 ( 92%)], Train Loss: 0.47623\n",
            "Epoch: 00 [18644/20298 ( 92%)], Train Loss: 0.47605\n",
            "Epoch: 00 [18684/20298 ( 92%)], Train Loss: 0.47546\n",
            "Epoch: 00 [18724/20298 ( 92%)], Train Loss: 0.47569\n",
            "Epoch: 00 [18764/20298 ( 92%)], Train Loss: 0.47521\n",
            "Epoch: 00 [18804/20298 ( 93%)], Train Loss: 0.47492\n",
            "Epoch: 00 [18844/20298 ( 93%)], Train Loss: 0.47494\n",
            "Epoch: 00 [18884/20298 ( 93%)], Train Loss: 0.47481\n",
            "Epoch: 00 [18924/20298 ( 93%)], Train Loss: 0.47452\n",
            "Epoch: 00 [18964/20298 ( 93%)], Train Loss: 0.47411\n",
            "Epoch: 00 [19004/20298 ( 94%)], Train Loss: 0.47351\n",
            "Epoch: 00 [19044/20298 ( 94%)], Train Loss: 0.47330\n",
            "Epoch: 00 [19084/20298 ( 94%)], Train Loss: 0.47275\n",
            "Epoch: 00 [19124/20298 ( 94%)], Train Loss: 0.47306\n",
            "Epoch: 00 [19164/20298 ( 94%)], Train Loss: 0.47276\n",
            "Epoch: 00 [19204/20298 ( 95%)], Train Loss: 0.47228\n",
            "Epoch: 00 [19244/20298 ( 95%)], Train Loss: 0.47186\n",
            "Epoch: 00 [19284/20298 ( 95%)], Train Loss: 0.47134\n",
            "Epoch: 00 [19324/20298 ( 95%)], Train Loss: 0.47066\n",
            "Epoch: 00 [19364/20298 ( 95%)], Train Loss: 0.47025\n",
            "Epoch: 00 [19404/20298 ( 96%)], Train Loss: 0.46985\n",
            "Epoch: 00 [19444/20298 ( 96%)], Train Loss: 0.46955\n",
            "Epoch: 00 [19484/20298 ( 96%)], Train Loss: 0.46940\n",
            "Epoch: 00 [19524/20298 ( 96%)], Train Loss: 0.46921\n",
            "Epoch: 00 [19564/20298 ( 96%)], Train Loss: 0.46868\n",
            "Epoch: 00 [19604/20298 ( 97%)], Train Loss: 0.46850\n",
            "Epoch: 00 [19644/20298 ( 97%)], Train Loss: 0.46822\n",
            "Epoch: 00 [19684/20298 ( 97%)], Train Loss: 0.46797\n",
            "Epoch: 00 [19724/20298 ( 97%)], Train Loss: 0.46805\n",
            "Epoch: 00 [19764/20298 ( 97%)], Train Loss: 0.46816\n",
            "Epoch: 00 [19804/20298 ( 98%)], Train Loss: 0.46783\n",
            "Epoch: 00 [19844/20298 ( 98%)], Train Loss: 0.46730\n",
            "Epoch: 00 [19884/20298 ( 98%)], Train Loss: 0.46702\n",
            "Epoch: 00 [19924/20298 ( 98%)], Train Loss: 0.46693\n",
            "Epoch: 00 [19964/20298 ( 98%)], Train Loss: 0.46657\n",
            "Epoch: 00 [20004/20298 ( 99%)], Train Loss: 0.46648\n",
            "Epoch: 00 [20044/20298 ( 99%)], Train Loss: 0.46590\n",
            "Epoch: 00 [20084/20298 ( 99%)], Train Loss: 0.46598\n",
            "Epoch: 00 [20124/20298 ( 99%)], Train Loss: 0.46588\n",
            "Epoch: 00 [20164/20298 ( 99%)], Train Loss: 0.46590\n",
            "Epoch: 00 [20204/20298 (100%)], Train Loss: 0.46570\n",
            "Epoch: 00 [20244/20298 (100%)], Train Loss: 0.46533\n",
            "Epoch: 00 [20284/20298 (100%)], Train Loss: 0.46527\n",
            "Epoch: 00 [20298/20298 (100%)], Train Loss: 0.46505\n",
            "----Validation Results Summary----\n",
            "Epoch: [0] Valid Loss: 0.26547\n",
            "0 Epoch, Best epoch was updated! Valid Loss: 0.26547\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 01 [    4/20298 (  0%)], Train Loss: 0.04409\n",
            "Epoch: 01 [   44/20298 (  0%)], Train Loss: 0.27309\n",
            "Epoch: 01 [   84/20298 (  0%)], Train Loss: 0.32994\n",
            "Epoch: 01 [  124/20298 (  1%)], Train Loss: 0.31799\n",
            "Epoch: 01 [  164/20298 (  1%)], Train Loss: 0.31746\n",
            "Epoch: 01 [  204/20298 (  1%)], Train Loss: 0.32605\n",
            "Epoch: 01 [  244/20298 (  1%)], Train Loss: 0.32743\n",
            "Epoch: 01 [  284/20298 (  1%)], Train Loss: 0.35810\n",
            "Epoch: 01 [  324/20298 (  2%)], Train Loss: 0.36803\n",
            "Epoch: 01 [  364/20298 (  2%)], Train Loss: 0.35372\n",
            "Epoch: 01 [  404/20298 (  2%)], Train Loss: 0.34880\n",
            "Epoch: 01 [  444/20298 (  2%)], Train Loss: 0.35180\n",
            "Epoch: 01 [  484/20298 (  2%)], Train Loss: 0.33754\n",
            "Epoch: 01 [  524/20298 (  3%)], Train Loss: 0.34908\n",
            "Epoch: 01 [  564/20298 (  3%)], Train Loss: 0.35414\n",
            "Epoch: 01 [  604/20298 (  3%)], Train Loss: 0.34485\n",
            "Epoch: 01 [  644/20298 (  3%)], Train Loss: 0.33400\n",
            "Epoch: 01 [  684/20298 (  3%)], Train Loss: 0.35255\n",
            "Epoch: 01 [  724/20298 (  4%)], Train Loss: 0.34800\n",
            "Epoch: 01 [  764/20298 (  4%)], Train Loss: 0.34436\n",
            "Epoch: 01 [  804/20298 (  4%)], Train Loss: 0.34608\n",
            "Epoch: 01 [  844/20298 (  4%)], Train Loss: 0.34716\n",
            "Epoch: 01 [  884/20298 (  4%)], Train Loss: 0.36333\n",
            "Epoch: 01 [  924/20298 (  5%)], Train Loss: 0.36687\n",
            "Epoch: 01 [  964/20298 (  5%)], Train Loss: 0.35846\n",
            "Epoch: 01 [ 1004/20298 (  5%)], Train Loss: 0.35830\n",
            "Epoch: 01 [ 1044/20298 (  5%)], Train Loss: 0.35550\n",
            "Epoch: 01 [ 1084/20298 (  5%)], Train Loss: 0.34858\n",
            "Epoch: 01 [ 1124/20298 (  6%)], Train Loss: 0.34888\n",
            "Epoch: 01 [ 1164/20298 (  6%)], Train Loss: 0.34874\n",
            "Epoch: 01 [ 1204/20298 (  6%)], Train Loss: 0.35017\n",
            "Epoch: 01 [ 1244/20298 (  6%)], Train Loss: 0.35014\n",
            "Epoch: 01 [ 1284/20298 (  6%)], Train Loss: 0.34997\n",
            "Epoch: 01 [ 1324/20298 (  7%)], Train Loss: 0.34538\n",
            "Epoch: 01 [ 1364/20298 (  7%)], Train Loss: 0.34211\n",
            "Epoch: 01 [ 1404/20298 (  7%)], Train Loss: 0.34419\n",
            "Epoch: 01 [ 1444/20298 (  7%)], Train Loss: 0.34389\n",
            "Epoch: 01 [ 1484/20298 (  7%)], Train Loss: 0.34394\n",
            "Epoch: 01 [ 1524/20298 (  8%)], Train Loss: 0.34229\n",
            "Epoch: 01 [ 1564/20298 (  8%)], Train Loss: 0.33622\n",
            "Epoch: 01 [ 1604/20298 (  8%)], Train Loss: 0.33528\n",
            "Epoch: 01 [ 1644/20298 (  8%)], Train Loss: 0.33266\n",
            "Epoch: 01 [ 1684/20298 (  8%)], Train Loss: 0.32954\n",
            "Epoch: 01 [ 1724/20298 (  8%)], Train Loss: 0.32497\n",
            "Epoch: 01 [ 1764/20298 (  9%)], Train Loss: 0.32356\n",
            "Epoch: 01 [ 1804/20298 (  9%)], Train Loss: 0.32382\n",
            "Epoch: 01 [ 1844/20298 (  9%)], Train Loss: 0.32116\n",
            "Epoch: 01 [ 1884/20298 (  9%)], Train Loss: 0.31729\n",
            "Epoch: 01 [ 1924/20298 (  9%)], Train Loss: 0.31524\n",
            "Epoch: 01 [ 1964/20298 ( 10%)], Train Loss: 0.31383\n",
            "Epoch: 01 [ 2004/20298 ( 10%)], Train Loss: 0.31455\n",
            "Epoch: 01 [ 2044/20298 ( 10%)], Train Loss: 0.31370\n",
            "Epoch: 01 [ 2084/20298 ( 10%)], Train Loss: 0.31114\n",
            "Epoch: 01 [ 2124/20298 ( 10%)], Train Loss: 0.30807\n",
            "Epoch: 01 [ 2164/20298 ( 11%)], Train Loss: 0.30710\n",
            "Epoch: 01 [ 2204/20298 ( 11%)], Train Loss: 0.30353\n",
            "Epoch: 01 [ 2244/20298 ( 11%)], Train Loss: 0.30177\n",
            "Epoch: 01 [ 2284/20298 ( 11%)], Train Loss: 0.29911\n",
            "Epoch: 01 [ 2324/20298 ( 11%)], Train Loss: 0.29652\n",
            "Epoch: 01 [ 2364/20298 ( 12%)], Train Loss: 0.29359\n",
            "Epoch: 01 [ 2404/20298 ( 12%)], Train Loss: 0.29162\n",
            "Epoch: 01 [ 2444/20298 ( 12%)], Train Loss: 0.28951\n",
            "Epoch: 01 [ 2484/20298 ( 12%)], Train Loss: 0.29112\n",
            "Epoch: 01 [ 2524/20298 ( 12%)], Train Loss: 0.28866\n",
            "Epoch: 01 [ 2564/20298 ( 13%)], Train Loss: 0.28823\n",
            "Epoch: 01 [ 2604/20298 ( 13%)], Train Loss: 0.28585\n",
            "Epoch: 01 [ 2644/20298 ( 13%)], Train Loss: 0.28607\n",
            "Epoch: 01 [ 2684/20298 ( 13%)], Train Loss: 0.28450\n",
            "Epoch: 01 [ 2724/20298 ( 13%)], Train Loss: 0.28684\n",
            "Epoch: 01 [ 2764/20298 ( 14%)], Train Loss: 0.28517\n",
            "Epoch: 01 [ 2804/20298 ( 14%)], Train Loss: 0.28429\n",
            "Epoch: 01 [ 2844/20298 ( 14%)], Train Loss: 0.28396\n",
            "Epoch: 01 [ 2884/20298 ( 14%)], Train Loss: 0.28145\n",
            "Epoch: 01 [ 2924/20298 ( 14%)], Train Loss: 0.28279\n",
            "Epoch: 01 [ 2964/20298 ( 15%)], Train Loss: 0.28214\n",
            "Epoch: 01 [ 3004/20298 ( 15%)], Train Loss: 0.28099\n",
            "Epoch: 01 [ 3044/20298 ( 15%)], Train Loss: 0.27893\n",
            "Epoch: 01 [ 3084/20298 ( 15%)], Train Loss: 0.27765\n",
            "Epoch: 01 [ 3124/20298 ( 15%)], Train Loss: 0.27726\n",
            "Epoch: 01 [ 3164/20298 ( 16%)], Train Loss: 0.27661\n",
            "Epoch: 01 [ 3204/20298 ( 16%)], Train Loss: 0.27456\n",
            "Epoch: 01 [ 3244/20298 ( 16%)], Train Loss: 0.27284\n",
            "Epoch: 01 [ 3284/20298 ( 16%)], Train Loss: 0.27145\n",
            "Epoch: 01 [ 3324/20298 ( 16%)], Train Loss: 0.27033\n",
            "Epoch: 01 [ 3364/20298 ( 17%)], Train Loss: 0.26867\n",
            "Epoch: 01 [ 3404/20298 ( 17%)], Train Loss: 0.26744\n",
            "Epoch: 01 [ 3444/20298 ( 17%)], Train Loss: 0.26891\n",
            "Epoch: 01 [ 3484/20298 ( 17%)], Train Loss: 0.26855\n",
            "Epoch: 01 [ 3524/20298 ( 17%)], Train Loss: 0.26706\n",
            "Epoch: 01 [ 3564/20298 ( 18%)], Train Loss: 0.26655\n",
            "Epoch: 01 [ 3604/20298 ( 18%)], Train Loss: 0.26450\n",
            "Epoch: 01 [ 3644/20298 ( 18%)], Train Loss: 0.26284\n",
            "Epoch: 01 [ 3684/20298 ( 18%)], Train Loss: 0.26173\n",
            "Epoch: 01 [ 3724/20298 ( 18%)], Train Loss: 0.26109\n",
            "Epoch: 01 [ 3764/20298 ( 19%)], Train Loss: 0.26069\n",
            "Epoch: 01 [ 3804/20298 ( 19%)], Train Loss: 0.26036\n",
            "Epoch: 01 [ 3844/20298 ( 19%)], Train Loss: 0.25915\n",
            "Epoch: 01 [ 3884/20298 ( 19%)], Train Loss: 0.25943\n",
            "Epoch: 01 [ 3924/20298 ( 19%)], Train Loss: 0.25891\n",
            "Epoch: 01 [ 3964/20298 ( 20%)], Train Loss: 0.25781\n",
            "Epoch: 01 [ 4004/20298 ( 20%)], Train Loss: 0.25610\n",
            "Epoch: 01 [ 4044/20298 ( 20%)], Train Loss: 0.25576\n",
            "Epoch: 01 [ 4084/20298 ( 20%)], Train Loss: 0.25502\n",
            "Epoch: 01 [ 4124/20298 ( 20%)], Train Loss: 0.25391\n",
            "Epoch: 01 [ 4164/20298 ( 21%)], Train Loss: 0.25354\n",
            "Epoch: 01 [ 4204/20298 ( 21%)], Train Loss: 0.25220\n",
            "Epoch: 01 [ 4244/20298 ( 21%)], Train Loss: 0.25175\n",
            "Epoch: 01 [ 4284/20298 ( 21%)], Train Loss: 0.25231\n",
            "Epoch: 01 [ 4324/20298 ( 21%)], Train Loss: 0.25205\n",
            "Epoch: 01 [ 4364/20298 ( 21%)], Train Loss: 0.25242\n",
            "Epoch: 01 [ 4404/20298 ( 22%)], Train Loss: 0.25232\n",
            "Epoch: 01 [ 4444/20298 ( 22%)], Train Loss: 0.25222\n",
            "Epoch: 01 [ 4484/20298 ( 22%)], Train Loss: 0.25169\n",
            "Epoch: 01 [ 4524/20298 ( 22%)], Train Loss: 0.25044\n",
            "Epoch: 01 [ 4564/20298 ( 22%)], Train Loss: 0.24918\n",
            "Epoch: 01 [ 4604/20298 ( 23%)], Train Loss: 0.24856\n",
            "Epoch: 01 [ 4644/20298 ( 23%)], Train Loss: 0.24808\n",
            "Epoch: 01 [ 4684/20298 ( 23%)], Train Loss: 0.24721\n",
            "Epoch: 01 [ 4724/20298 ( 23%)], Train Loss: 0.24585\n",
            "Epoch: 01 [ 4764/20298 ( 23%)], Train Loss: 0.24582\n",
            "Epoch: 01 [ 4804/20298 ( 24%)], Train Loss: 0.24509\n",
            "Epoch: 01 [ 4844/20298 ( 24%)], Train Loss: 0.24384\n",
            "Epoch: 01 [ 4884/20298 ( 24%)], Train Loss: 0.24230\n",
            "Epoch: 01 [ 4924/20298 ( 24%)], Train Loss: 0.24095\n",
            "Epoch: 01 [ 4964/20298 ( 24%)], Train Loss: 0.24038\n",
            "Epoch: 01 [ 5004/20298 ( 25%)], Train Loss: 0.23889\n",
            "Epoch: 01 [ 5044/20298 ( 25%)], Train Loss: 0.23789\n",
            "Epoch: 01 [ 5084/20298 ( 25%)], Train Loss: 0.23700\n",
            "Epoch: 01 [ 5124/20298 ( 25%)], Train Loss: 0.23587\n",
            "Epoch: 01 [ 5164/20298 ( 25%)], Train Loss: 0.23450\n",
            "Epoch: 01 [ 5204/20298 ( 26%)], Train Loss: 0.23396\n",
            "Epoch: 01 [ 5244/20298 ( 26%)], Train Loss: 0.23301\n",
            "Epoch: 01 [ 5284/20298 ( 26%)], Train Loss: 0.23218\n",
            "Epoch: 01 [ 5324/20298 ( 26%)], Train Loss: 0.23174\n",
            "Epoch: 01 [ 5364/20298 ( 26%)], Train Loss: 0.23069\n",
            "Epoch: 01 [ 5404/20298 ( 27%)], Train Loss: 0.22929\n",
            "Epoch: 01 [ 5444/20298 ( 27%)], Train Loss: 0.22855\n",
            "Epoch: 01 [ 5484/20298 ( 27%)], Train Loss: 0.22803\n",
            "Epoch: 01 [ 5524/20298 ( 27%)], Train Loss: 0.22801\n",
            "Epoch: 01 [ 5564/20298 ( 27%)], Train Loss: 0.22712\n",
            "Epoch: 01 [ 5604/20298 ( 28%)], Train Loss: 0.22678\n",
            "Epoch: 01 [ 5644/20298 ( 28%)], Train Loss: 0.22599\n",
            "Epoch: 01 [ 5684/20298 ( 28%)], Train Loss: 0.22482\n",
            "Epoch: 01 [ 5724/20298 ( 28%)], Train Loss: 0.22421\n",
            "Epoch: 01 [ 5764/20298 ( 28%)], Train Loss: 0.22408\n",
            "Epoch: 01 [ 5804/20298 ( 29%)], Train Loss: 0.22329\n",
            "Epoch: 01 [ 5844/20298 ( 29%)], Train Loss: 0.22211\n",
            "Epoch: 01 [ 5884/20298 ( 29%)], Train Loss: 0.22114\n",
            "Epoch: 01 [ 5924/20298 ( 29%)], Train Loss: 0.22112\n",
            "Epoch: 01 [ 5964/20298 ( 29%)], Train Loss: 0.22120\n",
            "Epoch: 01 [ 6004/20298 ( 30%)], Train Loss: 0.22087\n",
            "Epoch: 01 [ 6044/20298 ( 30%)], Train Loss: 0.22057\n",
            "Epoch: 01 [ 6084/20298 ( 30%)], Train Loss: 0.22139\n",
            "Epoch: 01 [ 6124/20298 ( 30%)], Train Loss: 0.22110\n",
            "Epoch: 01 [ 6164/20298 ( 30%)], Train Loss: 0.22075\n",
            "Epoch: 01 [ 6204/20298 ( 31%)], Train Loss: 0.22033\n",
            "Epoch: 01 [ 6244/20298 ( 31%)], Train Loss: 0.21972\n",
            "Epoch: 01 [ 6284/20298 ( 31%)], Train Loss: 0.21958\n",
            "Epoch: 01 [ 6324/20298 ( 31%)], Train Loss: 0.22011\n",
            "Epoch: 01 [ 6364/20298 ( 31%)], Train Loss: 0.21960\n",
            "Epoch: 01 [ 6404/20298 ( 32%)], Train Loss: 0.21898\n",
            "Epoch: 01 [ 6444/20298 ( 32%)], Train Loss: 0.21880\n",
            "Epoch: 01 [ 6484/20298 ( 32%)], Train Loss: 0.21901\n",
            "Epoch: 01 [ 6524/20298 ( 32%)], Train Loss: 0.21929\n",
            "Epoch: 01 [ 6564/20298 ( 32%)], Train Loss: 0.21852\n",
            "Epoch: 01 [ 6604/20298 ( 33%)], Train Loss: 0.21828\n",
            "Epoch: 01 [ 6644/20298 ( 33%)], Train Loss: 0.21748\n",
            "Epoch: 01 [ 6684/20298 ( 33%)], Train Loss: 0.21673\n",
            "Epoch: 01 [ 6724/20298 ( 33%)], Train Loss: 0.21667\n",
            "Epoch: 01 [ 6764/20298 ( 33%)], Train Loss: 0.21591\n",
            "Epoch: 01 [ 6804/20298 ( 34%)], Train Loss: 0.21526\n",
            "Epoch: 01 [ 6844/20298 ( 34%)], Train Loss: 0.21488\n",
            "Epoch: 01 [ 6884/20298 ( 34%)], Train Loss: 0.21448\n",
            "Epoch: 01 [ 6924/20298 ( 34%)], Train Loss: 0.21375\n",
            "Epoch: 01 [ 6964/20298 ( 34%)], Train Loss: 0.21372\n",
            "Epoch: 01 [ 7004/20298 ( 35%)], Train Loss: 0.21483\n",
            "Epoch: 01 [ 7044/20298 ( 35%)], Train Loss: 0.21454\n",
            "Epoch: 01 [ 7084/20298 ( 35%)], Train Loss: 0.21366\n",
            "Epoch: 01 [ 7124/20298 ( 35%)], Train Loss: 0.21311\n",
            "Epoch: 01 [ 7164/20298 ( 35%)], Train Loss: 0.21271\n",
            "Epoch: 01 [ 7204/20298 ( 35%)], Train Loss: 0.21209\n",
            "Epoch: 01 [ 7244/20298 ( 36%)], Train Loss: 0.21151\n",
            "Epoch: 01 [ 7284/20298 ( 36%)], Train Loss: 0.21177\n",
            "Epoch: 01 [ 7324/20298 ( 36%)], Train Loss: 0.21212\n",
            "Epoch: 01 [ 7364/20298 ( 36%)], Train Loss: 0.21178\n",
            "Epoch: 01 [ 7404/20298 ( 36%)], Train Loss: 0.21104\n",
            "Epoch: 01 [ 7444/20298 ( 37%)], Train Loss: 0.21026\n",
            "Epoch: 01 [ 7484/20298 ( 37%)], Train Loss: 0.20958\n",
            "Epoch: 01 [ 7524/20298 ( 37%)], Train Loss: 0.21000\n",
            "Epoch: 01 [ 7564/20298 ( 37%)], Train Loss: 0.20942\n",
            "Epoch: 01 [ 7604/20298 ( 37%)], Train Loss: 0.20985\n",
            "Epoch: 01 [ 7644/20298 ( 38%)], Train Loss: 0.21027\n",
            "Epoch: 01 [ 7684/20298 ( 38%)], Train Loss: 0.21011\n",
            "Epoch: 01 [ 7724/20298 ( 38%)], Train Loss: 0.21069\n",
            "Epoch: 01 [ 7764/20298 ( 38%)], Train Loss: 0.21036\n",
            "Epoch: 01 [ 7804/20298 ( 38%)], Train Loss: 0.21006\n",
            "Epoch: 01 [ 7844/20298 ( 39%)], Train Loss: 0.21015\n",
            "Epoch: 01 [ 7884/20298 ( 39%)], Train Loss: 0.20959\n",
            "Epoch: 01 [ 7924/20298 ( 39%)], Train Loss: 0.20929\n",
            "Epoch: 01 [ 7964/20298 ( 39%)], Train Loss: 0.20904\n",
            "Epoch: 01 [ 8004/20298 ( 39%)], Train Loss: 0.20871\n",
            "Epoch: 01 [ 8044/20298 ( 40%)], Train Loss: 0.20814\n",
            "Epoch: 01 [ 8084/20298 ( 40%)], Train Loss: 0.20778\n",
            "Epoch: 01 [ 8124/20298 ( 40%)], Train Loss: 0.20715\n",
            "Epoch: 01 [ 8164/20298 ( 40%)], Train Loss: 0.20683\n",
            "Epoch: 01 [ 8204/20298 ( 40%)], Train Loss: 0.20683\n",
            "Epoch: 01 [ 8244/20298 ( 41%)], Train Loss: 0.20671\n",
            "Epoch: 01 [ 8284/20298 ( 41%)], Train Loss: 0.20636\n",
            "Epoch: 01 [ 8324/20298 ( 41%)], Train Loss: 0.20621\n",
            "Epoch: 01 [ 8364/20298 ( 41%)], Train Loss: 0.20555\n",
            "Epoch: 01 [ 8404/20298 ( 41%)], Train Loss: 0.20535\n",
            "Epoch: 01 [ 8444/20298 ( 42%)], Train Loss: 0.20547\n",
            "Epoch: 01 [ 8484/20298 ( 42%)], Train Loss: 0.20548\n",
            "Epoch: 01 [ 8524/20298 ( 42%)], Train Loss: 0.20508\n",
            "Epoch: 01 [ 8564/20298 ( 42%)], Train Loss: 0.20472\n",
            "Epoch: 01 [ 8604/20298 ( 42%)], Train Loss: 0.20428\n",
            "Epoch: 01 [ 8644/20298 ( 43%)], Train Loss: 0.20423\n",
            "Epoch: 01 [ 8684/20298 ( 43%)], Train Loss: 0.20363\n",
            "Epoch: 01 [ 8724/20298 ( 43%)], Train Loss: 0.20364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1aheVHL0oYV"
      },
      "source": [
        "#!cp -r . /content/drive/MyDrive/datas/chaii2"
      ],
      "id": "C1aheVHL0oYV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-26T00:57:30.928960Z",
          "iopub.status.busy": "2021-09-26T00:57:30.928225Z",
          "iopub.status.idle": "2021-09-26T00:57:30.931715Z",
          "shell.execute_reply": "2021-09-26T00:57:30.931276Z",
          "shell.execute_reply.started": "2021-08-17T20:36:49.722978Z"
        },
        "id": "a34e1895",
        "papermill": {
          "duration": 0.163926,
          "end_time": "2021-09-26T00:57:30.931845",
          "exception": false,
          "start_time": "2021-09-26T00:57:30.767919",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# example for training second fold\n",
        "\n",
        "# for fold in range(1, 2):\n",
        "#     print();print()\n",
        "#     print('-'*50)\n",
        "#     print(f'FOLD: {fold}')\n",
        "#     print('-'*50)\n",
        "#     run(train, fold)"
      ],
      "id": "a34e1895",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.14995,
          "end_time": "2021-09-26T00:57:31.231921",
          "exception": false,
          "start_time": "2021-09-26T00:57:31.081971",
          "status": "completed"
        },
        "tags": [],
        "id": "c600bffd"
      },
      "source": [
        "### Thanks and please do Upvote!"
      ],
      "id": "c600bffd"
    }
  ]
}