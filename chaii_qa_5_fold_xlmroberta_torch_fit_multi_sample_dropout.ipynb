{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "chaii-qa-5-fold-xlmroberta-torch-fit-multi-sample-dropout",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3134.482195,
      "end_time": "2021-09-26T00:57:33.514736",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-09-26T00:05:19.032541",
      "version": "2.3.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aruaru0/colab_notebook/blob/main/chaii_qa_5_fold_xlmroberta_torch_fit_multi_sample_dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf20wKLVQUYg"
      },
      "source": [
        "# Add Multi sample dropout"
      ],
      "id": "Gf20wKLVQUYg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shE3S33KxTch"
      },
      "source": [
        "# for colab"
      ],
      "id": "shE3S33KxTch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZJxbaQFyRMb",
        "outputId": "f3371a42-09a0-46e4-93a7-b3c08e654d43"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "BZJxbaQFyRMb",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct  5 13:12:32 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnzNG_0jxW_6",
        "outputId": "a06b7dfe-7d73-4b23-bf66-c114c5a5985f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "KnzNG_0jxW_6",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTgjav_xxW7p"
      },
      "source": [
        "! pip install --upgrade --force-reinstall --no-deps  kaggle > /dev/null\n",
        "! mkdir /root/.kaggle\n",
        "! cp \"/content/drive/My Drive/Kaggle/kaggle.json\" /root/.kaggle/\n",
        "! chmod 600 /root/.kaggle/kaggle.json"
      ],
      "id": "CTgjav_xxW7p",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_GIjVFxW4G",
        "outputId": "fac54a0c-8dc2-4212-d6b6-24cb454d626a"
      },
      "source": [
        "!kaggle competitions download -c chaii-hindi-and-tamil-question-answering\n",
        "!kaggle datasets download -d rhtsingh/mlqa-hindi-processed\n",
        "!kaggle datasets download -d nbroad/xlm-roberta-squad2\n",
        "!kaggle datasets download -d msafi04/squad-qa-tamil-dataset"
      ],
      "id": "MO_GIjVFxW4G",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading chaii-hindi-and-tamil-question-answering.zip to /content\n",
            "\r  0% 0.00/6.81M [00:00<?, ?B/s]\r 73% 5.00M/6.81M [00:00<00:00, 41.6MB/s]\n",
            "100% 6.81M/6.81M [00:00<00:00, 54.5MB/s]\n",
            "Downloading mlqa-hindi-processed.zip to /content\n",
            "  0% 0.00/2.49M [00:00<?, ?B/s]\n",
            "100% 2.49M/2.49M [00:00<00:00, 174MB/s]\n",
            "Downloading xlm-roberta-squad2.zip to /content\n",
            "100% 3.33G/3.34G [00:27<00:00, 117MB/s]\n",
            "100% 3.34G/3.34G [00:27<00:00, 130MB/s]\n",
            "Downloading squad-qa-tamil-dataset.zip to /content\n",
            "  0% 0.00/564k [00:00<?, ?B/s]\n",
            "100% 564k/564k [00:00<00:00, 94.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PObJQR5XxWrD",
        "outputId": "545d4189-090c-41ec-d9f6-27c7eda2c1bb"
      },
      "source": [
        "!apt install unzip"
      ],
      "id": "PObJQR5XxWrD",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyQVYuBDxxBn"
      },
      "source": [
        "!mkdir -p input output"
      ],
      "id": "wyQVYuBDxxBn",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3uTTHsFxw2D",
        "outputId": "5762d669-699f-4ff0-a826-aed81496e1b4"
      },
      "source": [
        "!mkdir input/chaii-hindi-and-tamil-question-answering\n",
        "!unzip /content/chaii-hindi-and-tamil-question-answering.zip -d input/chaii-hindi-and-tamil-question-answering\n",
        "!mkdir input/mlqa-hindi-processed\n",
        "!unzip /content/mlqa-hindi-processed.zip -d input/mlqa-hindi-processed\n",
        "!mkdir input/xlm-roberta-squad2\n",
        "!unzip /content/xlm-roberta-squad2.zip -d input/xlm-roberta-squad2/\n",
        "!mkdir input/squad-qa-tamil-dataset\n",
        "!unzip /content/squad-qa-tamil-dataset.zip -d input/squad-qa-tamil-dataset"
      ],
      "id": "_3uTTHsFxw2D",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/chaii-hindi-and-tamil-question-answering.zip\n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/sample_submission.csv  \n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/test.csv  \n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/train.csv  \n",
            "Archive:  /content/mlqa-hindi-processed.zip\n",
            "  inflating: input/mlqa-hindi-processed/mlqa_hindi.csv  \n",
            "  inflating: input/mlqa-hindi-processed/xquad.csv  \n",
            "Archive:  /content/xlm-roberta-squad2.zip\n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/config.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/pytorch_model.bin  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/sentencepiece.bpe.model  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/special_tokens_map.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/tokenizer.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/tokenizer_config.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/config.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/pytorch_model.bin  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/sentencepiece.bpe.model  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/special_tokens_map.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/tokenizer.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/tokenizer_config.json  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/config.json  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/pytorch_model.bin  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/sentencepiece.bpe.model  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/special_tokens_map.json  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/tokenizer.json  \n",
            "  inflating: input/xlm-roberta-squad2/seongju/squadv2-xlm-roberta-base/tokenizer_config.json  \n",
            "Archive:  /content/squad-qa-tamil-dataset.zip\n",
            "  inflating: input/squad-qa-tamil-dataset/squad_tamilQA.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCRudbllx9Be"
      },
      "source": [
        "# restart here.."
      ],
      "id": "SCRudbllx9Be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrhwRDxfx6fa",
        "outputId": "d138bc06-b914-412b-84ab-b218ba974089"
      },
      "source": [
        "%cd output"
      ],
      "id": "IrhwRDxfx6fa",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPXOqpvbyBky",
        "outputId": "0211a932-0c96-41f2-ed6e-dfeed0b69de5"
      },
      "source": [
        "!pip install transformers"
      ],
      "id": "wPXOqpvbyBky",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 59.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 59.7 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.18-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Collecting ruamel.yaml==0.17.16\n",
            "  Downloading ruamel.yaml-0.17.16-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 76.9 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.1.2\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 70.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: ruamel.yaml.clib, ruamel.yaml, tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.18 pyyaml-5.4.1 ruamel.yaml-0.17.16 ruamel.yaml.clib-0.2.6 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c47c9139"
      },
      "source": [
        "<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n",
        "    \n",
        "<h3><span \"style: color=#444\">Introduction</span></h3>\n",
        "\n",
        "The kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n",
        "\n",
        "This is a three part kernel,\n",
        "\n",
        "- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n",
        "\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n",
        "\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n",
        "\n",
        "<h3><span \"style: color=#444\">Techniques</span></h3>\n",
        "\n",
        "The kernel has implementation for below techniques, click on the links to learn more -\n",
        "\n",
        " - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n",
        " \n",
        " - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n",
        " \n",
        " - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n",
        " \n",
        " - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n",
        " \n",
        " - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n",
        " \n",
        " - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n",
        " \n",
        " - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n",
        " \n",
        " - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n",
        " \n",
        " - etc.\n",
        " \n",
        "<h3><span \"style: color=#444\">References</span></h3>\n",
        "I would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n",
        "\n",
        "- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n",
        "\n",
        "- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n",
        "\n",
        "- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n",
        "\n",
        "- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n",
        "\n",
        "- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read."
      ],
      "id": "c47c9139"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43955668"
      },
      "source": [
        "<h3><span style=\"color=#444\">Note</span></h3>\n",
        "\n",
        "The below points are worth noting,\n",
        "\n",
        " - I haven't used FP16 because due to some reason this fails and model never starts training.\n",
        " - These are the original hyperparamters and setting that I have used for training my models.\n",
        " - I tried few pooling layers but none of them performed better than simple one.\n",
        " - Gradient clipping reduces model performance.\n",
        " - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n",
        " - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>"
      ],
      "id": "43955668"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f09433b"
      },
      "source": [
        "### Install APEX"
      ],
      "id": "8f09433b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4bb901a"
      },
      "source": [
        "# %%writefile setup.sh\n",
        "# export CUDA_HOME=/usr/local/cuda-10.1\n",
        "# git clone https://github.com/NVIDIA/apex\n",
        "# cd apex\n",
        "# pip install -v --disable-pip-version-check --no-cache-dir ./"
      ],
      "id": "f4bb901a",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef5e2a11"
      },
      "source": [
        "# %%capture\n",
        "# !sh setup.sh"
      ],
      "id": "ef5e2a11",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55059f9a"
      },
      "source": [
        "### Import Dependencies"
      ],
      "id": "55059f9a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a92ad27",
        "outputId": "0de97a37-108f-4768-ba42-b0edfbd328e7"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "gc.enable()\n",
        "import math\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import multiprocessing\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn import model_selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import (\n",
        "    Dataset, DataLoader,\n",
        "    SequentialSampler, RandomSampler\n",
        ")\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "try:\n",
        "    from apex import amp\n",
        "    APEX_INSTALLED = True\n",
        "except ImportError:\n",
        "    APEX_INSTALLED = False\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    get_cosine_schedule_with_warmup,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    logging,\n",
        "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
        ")\n",
        "logging.set_verbosity_warning()\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "def fix_all_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def optimal_num_of_loader_workers():\n",
        "    num_cpus = multiprocessing.cpu_count()\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
        "    return optimal_value\n",
        "\n",
        "print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
      ],
      "id": "8a92ad27",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apex AMP Installed :: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab75148d"
      },
      "source": [
        "### Training Configuration"
      ],
      "id": "ab75148d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMU6cmG6Y7s0"
      },
      "source": [
        "tamil & hindi xsquad"
      ],
      "id": "FMU6cmG6Y7s0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5934fe6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4606d73b-c725-4b76-eac2-5ca4bf6bd00c"
      },
      "source": [
        "class Config:\n",
        "    # model\n",
        "    model_type = 'xlm_roberta'\n",
        "    #model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n",
        "    model_name_or_path = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n",
        "    #config_name = \"deepset/xlm-roberta-large-squad2\"\n",
        "    config_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n",
        "    fp16 = True if APEX_INSTALLED else False\n",
        "    fp16_opt_level = \"O1\"\n",
        "    \n",
        "    gradient_accumulation_steps = 2 #@param {type:\"integer\"}\n",
        "\n",
        "    # tokenizer\n",
        "    tokenizer_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n",
        "#    max_seq_length = 384\n",
        "#    doc_stride = 128\n",
        "    max_seq_length = 400 #@param {type:\"integer\"}\n",
        "    doc_stride = 135 #@param {type:\"integer\"}\n",
        "\n",
        "    # train\n",
        "    epochs = 2 #@param {type:\"integer\"}\n",
        "    train_batch_size = 4\n",
        "    eval_batch_size = 8\n",
        "\n",
        "    # optimizer\n",
        "    optimizer_type = 'AdamW'\n",
        "    learning_rate =  1e-5#@param {type:\"number\"}\n",
        "    weight_decay = 1e-3  #@param {type:\"number\"}\n",
        "    epsilon = 1e-8 #@param {type:\"number\"}\n",
        "    max_grad_norm = 1.0 #@param {type:\"number\"}\n",
        "\n",
        "    # scheduler\n",
        "    decay_name = 'linear-warmup'\n",
        "    warmup_ratio = 0.1\n",
        "\n",
        "    # logging\n",
        "    logging_steps = 10\n",
        "\n",
        "    # evaluate\n",
        "    output_dir = 'output'\n",
        "    seed =  42#@param {type:\"integer\"}\n",
        "\n",
        "print(\"acc_step\", Config.gradient_accumulation_steps,\n",
        "      \"lr\", Config.learning_rate, \"wd\", Config.weight_decay,\n",
        "      \"{}-{}\".format(Config.max_seq_length, Config.doc_stride))"
      ],
      "id": "5934fe6a",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc_step 2 lr 1e-05 wd 0.001 400-135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97366e5"
      },
      "source": [
        "### Data Factory"
      ],
      "id": "a97366e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7neglrAWwgj"
      },
      "source": [
        "#external_tamil_xquad = pd.read_csv('/content/input/squad-qa-tamil-dataset/squad_tamilQA.csv')\n",
        "#external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n"
      ],
      "id": "U7neglrAWwgj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV7PZDwBXGkm"
      },
      "source": [
        "#external_tamil_xquad['language'] = \"tamil\"\n",
        "#external_tamil_xquad.head()"
      ],
      "id": "EV7PZDwBXGkm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb636dc6"
      },
      "source": [
        "train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n",
        "test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n",
        "external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n",
        "external_hindi_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n",
        "external_tamil_xquad = pd.read_csv('/content/input/squad-qa-tamil-dataset/squad_tamilQA.csv')\n",
        "external_tamil_xquad['language'] = \"tamil\"\n",
        "external_xquad = pd.concat([external_hindi_xquad, external_tamil_xquad])\n",
        "external_train = pd.concat([external_mlqa, external_xquad])\n",
        "\n",
        "def create_folds(data, num_splits):\n",
        "    data[\"kfold\"] = -1\n",
        "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    return data\n",
        "\n",
        "train = create_folds(train, num_splits=5)\n",
        "#train = create_folds(train, num_splits=3)\n",
        "external_train[\"kfold\"] = -1\n",
        "external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
        "train = pd.concat([train, external_train]).reset_index(drop=True)\n",
        "\n",
        "def convert_answers(row):\n",
        "    return {'answer_start': [row[0]], 'text': [row[1]]}\n",
        "\n",
        "train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
      ],
      "id": "eb636dc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60781636"
      },
      "source": [
        "### Covert Examples to Features (Preprocess)"
      ],
      "id": "60781636"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63b399b9"
      },
      "source": [
        "def prepare_train_features(args, example, tokenizer):\n",
        "    example[\"question\"] = example[\"question\"].lstrip()\n",
        "    tokenized_example = tokenizer(\n",
        "        example[\"question\"],\n",
        "        example[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=args.max_seq_length,\n",
        "        stride=args.doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
        "\n",
        "    features = []\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        feature = {}\n",
        "\n",
        "        input_ids = tokenized_example[\"input_ids\"][i]\n",
        "        attention_mask = tokenized_example[\"attention_mask\"][i]\n",
        "\n",
        "        feature['input_ids'] = input_ids\n",
        "        feature['attention_mask'] = attention_mask\n",
        "        feature['offset_mapping'] = offsets\n",
        "\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        sequence_ids = tokenized_example.sequence_ids(i)\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = example[\"answers\"]\n",
        "\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            feature[\"start_position\"] = cls_index\n",
        "            feature[\"end_position\"] = cls_index\n",
        "        else:\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                feature[\"start_position\"] = cls_index\n",
        "                feature[\"end_position\"] = cls_index\n",
        "            else:\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                feature[\"start_position\"] = token_start_index - 1\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                feature[\"end_position\"] = token_end_index + 1\n",
        "\n",
        "        features.append(feature)\n",
        "    return features"
      ],
      "id": "63b399b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55553f9f"
      },
      "source": [
        "### Dataset Retriever"
      ],
      "id": "55553f9f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c78cee8c"
      },
      "source": [
        "class DatasetRetriever(Dataset):\n",
        "    def __init__(self, features, mode='train'):\n",
        "        super(DatasetRetriever, self).__init__()\n",
        "        self.features = features\n",
        "        self.mode = mode\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, item):   \n",
        "        feature = self.features[item]\n",
        "        if self.mode == 'train':\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
        "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
        "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                'offset_mapping':feature['offset_mapping'],\n",
        "                'sequence_ids':feature['sequence_ids'],\n",
        "                'id':feature['example_id'],\n",
        "                'context': feature['context'],\n",
        "                'question': feature['question']\n",
        "            }"
      ],
      "id": "c78cee8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7e0677b"
      },
      "source": [
        "### Model"
      ],
      "id": "b7e0677b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd46e25e"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, modelname_or_path, config):\n",
        "        super(Model, self).__init__()\n",
        "        self.n_msd = 8\n",
        "        self.config = config\n",
        "        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(0.2) for _ in range(self.n_msd)])\n",
        "        self._init_weights(self.qa_outputs)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids, \n",
        "        attention_mask=None, \n",
        "        # token_type_ids=None\n",
        "    ):\n",
        "        outputs = self.xlm_roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]\n",
        "        \n",
        "        #sequence_output = self.dropout(sequence_output)\n",
        "        qa_logits = sum([self.qa_outputs(dropout(sequence_output)) for dropout in self.dropouts])\n",
        "        #print(qa_logits.size())\n",
        "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "    \n",
        "        return start_logits, end_logits"
      ],
      "id": "cd46e25e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b054f2e5"
      },
      "source": [
        "### Loss"
      ],
      "id": "b054f2e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCkwCpg0FtDa"
      },
      "source": [
        "def linear_combination(x, y, epsilon):\n",
        "    return (1 - epsilon) * x + epsilon * y\n",
        "\n",
        "def reduce_loss(loss, reduction='mean'):\n",
        "    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss"
      ],
      "id": "YCkwCpg0FtDa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU1pWXuBF5TP"
      },
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, epsilon=0.1, ignore_index = -1, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.reduction = reduction\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, preds, target):\n",
        "        n = preds.size()[-1]\n",
        "        log_preds = F.log_softmax(preds, dim=-1)\n",
        "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
        "        nll = F.nll_loss(log_preds, target, reduction=self.reduction, ignore_index=self.ignore_index)\n",
        "        return linear_combination(nll, loss/n, self.epsilon)"
      ],
      "id": "vU1pWXuBF5TP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f490e487"
      },
      "source": [
        "def loss_fn(preds, labels):\n",
        "    start_preds, end_preds = preds\n",
        "    start_labels, end_labels = labels\n",
        "    \n",
        "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
        "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
        "    #start_loss = LabelSmoothingCrossEntropy(ignore_index=-1)(start_preds, start_labels)\n",
        "    #end_loss = LabelSmoothingCrossEntropy(ignore_index=-1)(end_preds, end_labels)\n",
        "    total_loss = (start_loss + end_loss) / 2\n",
        "    return total_loss"
      ],
      "id": "f490e487",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9138cc0e"
      },
      "source": [
        "### Grouped Layerwise Learning Rate Decay"
      ],
      "id": "9138cc0e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e4bcde0"
      },
      "source": [
        "def get_optimizer_grouped_parameters(args, model):\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
        "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
        "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n",
        "        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    return optimizer_grouped_parameters"
      ],
      "id": "4e4bcde0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d0ac100"
      },
      "source": [
        "### Metric Logger"
      ],
      "id": "2d0ac100"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65b3cd59"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.max = 0\n",
        "        self.min = 1e5\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        if val > self.max:\n",
        "            self.max = val\n",
        "        if val < self.min:\n",
        "            self.min = val"
      ],
      "id": "65b3cd59",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03f659a2"
      },
      "source": [
        "### Utilities"
      ],
      "id": "03f659a2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a7d36df"
      },
      "source": [
        "def make_model(args):\n",
        "    config = AutoConfig.from_pretrained(args.config_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
        "    model = Model(args.model_name_or_path, config=config)\n",
        "    return config, tokenizer, model\n",
        "\n",
        "def make_optimizer(args, model):\n",
        "    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "    if args.optimizer_type == \"AdamW\":\n",
        "        optimizer = AdamW(\n",
        "            optimizer_grouped_parameters,\n",
        "            lr=args.learning_rate,\n",
        "            eps=args.epsilon,\n",
        "            correct_bias=True\n",
        "        )\n",
        "        return optimizer\n",
        "\n",
        "def make_scheduler(\n",
        "    args, optimizer, \n",
        "    num_warmup_steps, \n",
        "    num_training_steps\n",
        "):\n",
        "    if args.decay_name == \"cosine-warmup\":\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "    else:\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "    return scheduler    \n",
        "\n",
        "def make_loader(\n",
        "    args, data, \n",
        "    tokenizer, fold\n",
        "):\n",
        "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
        "    \n",
        "    train_features, valid_features = [[] for _ in range(2)]\n",
        "    for i, row in train_set.iterrows():\n",
        "        train_features += prepare_train_features(args, row, tokenizer)\n",
        "    for i, row in valid_set.iterrows():\n",
        "        valid_features += prepare_train_features(args, row, tokenizer)\n",
        "\n",
        "    train_dataset = DatasetRetriever(train_features)\n",
        "    valid_dataset = DatasetRetriever(valid_features)\n",
        "    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n",
        "    \n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    valid_sampler = SequentialSampler(valid_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.train_batch_size,\n",
        "        sampler=train_sampler,\n",
        "        num_workers=optimal_num_of_loader_workers(),\n",
        "        pin_memory=True,\n",
        "        drop_last=False \n",
        "    )\n",
        "\n",
        "    valid_dataloader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=args.eval_batch_size, \n",
        "        sampler=valid_sampler,\n",
        "        num_workers=optimal_num_of_loader_workers(),\n",
        "        pin_memory=True, \n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    return train_dataloader, valid_dataloader"
      ],
      "id": "0a7d36df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47101630"
      },
      "source": [
        "### Trainer"
      ],
      "id": "47101630"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81b92c70"
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self, model, tokenizer, \n",
        "        optimizer, scheduler\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def train(\n",
        "        self, args, \n",
        "        train_dataloader, \n",
        "        epoch, result_dict\n",
        "    ):\n",
        "        count = 0\n",
        "        losses = AverageMeter()\n",
        "        \n",
        "        self.model.zero_grad()\n",
        "        self.model.train()\n",
        "        \n",
        "        fix_all_seeds(args.seed)\n",
        "        \n",
        "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
        "                    batch_data['start_position'], batch_data['end_position']\n",
        "            \n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
        "\n",
        "            outputs_start, outputs_end = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "            \n",
        "            #print(outputs_start.size())\n",
        "\n",
        "            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            count += input_ids.size(0)\n",
        "            losses.update(loss.item(), input_ids.size(0))\n",
        "\n",
        "            # if args.fp16:\n",
        "            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n",
        "            # else:\n",
        "            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n",
        "                _s = str(len(str(len(train_dataloader.sampler))))\n",
        "                ret = [\n",
        "                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n",
        "                    'Train Loss: {: >4.5f}'.format(losses.avg),\n",
        "                ]\n",
        "                print(', '.join(ret))\n",
        "\n",
        "        result_dict['train_loss'].append(losses.avg)\n",
        "        return result_dict"
      ],
      "id": "81b92c70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "468c7fad"
      },
      "source": [
        "### Evaluator"
      ],
      "id": "468c7fad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c77328b3"
      },
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    \n",
        "    def save(self, result, output_dir):\n",
        "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
        "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
        "\n",
        "    def evaluate(self, valid_dataloader, epoch, result_dict):\n",
        "        losses = AverageMeter()\n",
        "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
        "            self.model = self.model.eval()\n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
        "                    batch_data['start_position'], batch_data['end_position']\n",
        "            \n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
        "            \n",
        "            with torch.no_grad():            \n",
        "                outputs_start, outputs_end = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                )\n",
        "                \n",
        "                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
        "                losses.update(loss.item(), input_ids.size(0))\n",
        "                \n",
        "        print('----Validation Results Summary----')\n",
        "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
        "        result_dict['val_loss'].append(losses.avg)        \n",
        "        return result_dict"
      ],
      "id": "c77328b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86c0ad8e"
      },
      "source": [
        "### Initialize Training"
      ],
      "id": "86c0ad8e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edd188d2"
      },
      "source": [
        "def init_training(args, data, fold):\n",
        "    fix_all_seeds(args.seed)\n",
        "    \n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "    \n",
        "    # model\n",
        "    model_config, tokenizer, model = make_model(args)\n",
        "    if torch.cuda.device_count() >= 1:\n",
        "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
        "            torch.cuda.device_count(), \n",
        "            torch.cuda.get_device_name(0))\n",
        "        )\n",
        "        model = model.cuda() \n",
        "    else:\n",
        "        raise ValueError('CPU training is not supported')\n",
        "    \n",
        "    #model.load_state_dict(\n",
        "    #    torch.load(\"/content/drive/MyDrive/datas/chaii2/output/checkpoint-fold-{}\".format(fold))\n",
        "    #);\n",
        "\n",
        "    # data loaders\n",
        "    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = make_optimizer(args, model)\n",
        "\n",
        "    # scheduler\n",
        "    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n",
        "    if args.warmup_ratio > 0:\n",
        "        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
        "    else:\n",
        "        num_warmup_steps = 0\n",
        "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n",
        "    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n",
        "\n",
        "    # mixed precision training with NVIDIA Apex\n",
        "    if args.fp16:\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "    \n",
        "    result_dict = {\n",
        "        'epoch':[], \n",
        "        'train_loss': [], \n",
        "        'val_loss' : [], \n",
        "        'best_val_loss': np.inf\n",
        "    }\n",
        "\n",
        "    return (\n",
        "        model, model_config, tokenizer, optimizer, scheduler, \n",
        "        train_dataloader, valid_dataloader, result_dict\n",
        "    )"
      ],
      "id": "edd188d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb42970d"
      },
      "source": [
        "### Run"
      ],
      "id": "cb42970d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "915ef19d"
      },
      "source": [
        "def run(data, fold):\n",
        "    args = Config()\n",
        "    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n",
        "        valid_dataloader, result_dict = init_training(args, data, fold)\n",
        "    \n",
        "    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n",
        "    evaluator = Evaluator(model)\n",
        "\n",
        "    train_time_list = []\n",
        "    valid_time_list = []\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        result_dict['epoch'].append(epoch)\n",
        "\n",
        "        # Train\n",
        "        torch.cuda.synchronize()\n",
        "        tic1 = time.time()\n",
        "        result_dict = trainer.train(\n",
        "            args, train_dataloader, \n",
        "            epoch, result_dict\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        tic2 = time.time() \n",
        "        train_time_list.append(tic2 - tic1)\n",
        "        \n",
        "        # Evaluate\n",
        "        torch.cuda.synchronize()\n",
        "        tic3 = time.time()\n",
        "        result_dict = evaluator.evaluate(\n",
        "            valid_dataloader, epoch, result_dict\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        tic4 = time.time() \n",
        "        valid_time_list.append(tic4 - tic3)\n",
        "            \n",
        "        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n",
        "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
        "            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
        "            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n",
        "            \n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
        "            model_config.save_pretrained(output_dir)\n",
        "            tokenizer.save_pretrained(output_dir)\n",
        "            print(f\"Saving model checkpoint to {output_dir}.\")\n",
        "            \n",
        "        print()\n",
        "\n",
        "    evaluator.save(result_dict, output_dir)\n",
        "    \n",
        "    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n",
        "    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n",
        "    \n",
        "    best = result_dict[\"best_val_loss\"]\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    del trainer, evaluator\n",
        "    del model, model_config, tokenizer\n",
        "    del optimizer, scheduler\n",
        "    del train_dataloader, valid_dataloader, result_dict\n",
        "    gc.collect()\n",
        "\n",
        "    return best"
      ],
      "id": "915ef19d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7a66e0a",
        "outputId": "2d473ec5-9cf1-46e6-aae5-9fd71ad9dc0b"
      },
      "source": [
        "best = []\n",
        "for fold in range(5):\n",
        "    # if fold != 0 :\n",
        "    #   continue\n",
        "    print();print()\n",
        "    print('-'*50)\n",
        "    print(f'FOLD: {fold}')\n",
        "    print('-'*50)\n",
        "    ret = run(train, fold)\n",
        "    !cp -r ./output /content/drive/MyDrive/datas/chaii3/output\n",
        "    best.append(ret)\n",
        "\n",
        "print(best)"
      ],
      "id": "b7a66e0a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mストリーミング出力は最後の 5000 行に切り捨てられました。\u001b[0m\n",
            "Epoch: 00 [11404/20624 ( 55%)], Train Loss: 0.65568\n",
            "Epoch: 00 [11444/20624 ( 55%)], Train Loss: 0.65533\n",
            "Epoch: 00 [11484/20624 ( 56%)], Train Loss: 0.65519\n",
            "Epoch: 00 [11524/20624 ( 56%)], Train Loss: 0.65450\n",
            "Epoch: 00 [11564/20624 ( 56%)], Train Loss: 0.65387\n",
            "Epoch: 00 [11604/20624 ( 56%)], Train Loss: 0.65268\n",
            "Epoch: 00 [11644/20624 ( 56%)], Train Loss: 0.65229\n",
            "Epoch: 00 [11684/20624 ( 57%)], Train Loss: 0.65126\n",
            "Epoch: 00 [11724/20624 ( 57%)], Train Loss: 0.65036\n",
            "Epoch: 00 [11764/20624 ( 57%)], Train Loss: 0.65001\n",
            "Epoch: 00 [11804/20624 ( 57%)], Train Loss: 0.64952\n",
            "Epoch: 00 [11844/20624 ( 57%)], Train Loss: 0.64865\n",
            "Epoch: 00 [11884/20624 ( 58%)], Train Loss: 0.64749\n",
            "Epoch: 00 [11924/20624 ( 58%)], Train Loss: 0.64630\n",
            "Epoch: 00 [11964/20624 ( 58%)], Train Loss: 0.64547\n",
            "Epoch: 00 [12004/20624 ( 58%)], Train Loss: 0.64562\n",
            "Epoch: 00 [12044/20624 ( 58%)], Train Loss: 0.64467\n",
            "Epoch: 00 [12084/20624 ( 59%)], Train Loss: 0.64420\n",
            "Epoch: 00 [12124/20624 ( 59%)], Train Loss: 0.64398\n",
            "Epoch: 00 [12164/20624 ( 59%)], Train Loss: 0.64331\n",
            "Epoch: 00 [12204/20624 ( 59%)], Train Loss: 0.64246\n",
            "Epoch: 00 [12244/20624 ( 59%)], Train Loss: 0.64250\n",
            "Epoch: 00 [12284/20624 ( 60%)], Train Loss: 0.64211\n",
            "Epoch: 00 [12324/20624 ( 60%)], Train Loss: 0.64232\n",
            "Epoch: 00 [12364/20624 ( 60%)], Train Loss: 0.64175\n",
            "Epoch: 00 [12404/20624 ( 60%)], Train Loss: 0.64114\n",
            "Epoch: 00 [12444/20624 ( 60%)], Train Loss: 0.64090\n",
            "Epoch: 00 [12484/20624 ( 61%)], Train Loss: 0.64032\n",
            "Epoch: 00 [12524/20624 ( 61%)], Train Loss: 0.63922\n",
            "Epoch: 00 [12564/20624 ( 61%)], Train Loss: 0.63891\n",
            "Epoch: 00 [12604/20624 ( 61%)], Train Loss: 0.63856\n",
            "Epoch: 00 [12644/20624 ( 61%)], Train Loss: 0.63762\n",
            "Epoch: 00 [12684/20624 ( 62%)], Train Loss: 0.63724\n",
            "Epoch: 00 [12724/20624 ( 62%)], Train Loss: 0.63772\n",
            "Epoch: 00 [12764/20624 ( 62%)], Train Loss: 0.63711\n",
            "Epoch: 00 [12804/20624 ( 62%)], Train Loss: 0.63638\n",
            "Epoch: 00 [12844/20624 ( 62%)], Train Loss: 0.63659\n",
            "Epoch: 00 [12884/20624 ( 62%)], Train Loss: 0.63608\n",
            "Epoch: 00 [12924/20624 ( 63%)], Train Loss: 0.63604\n",
            "Epoch: 00 [12964/20624 ( 63%)], Train Loss: 0.63567\n",
            "Epoch: 00 [13004/20624 ( 63%)], Train Loss: 0.63500\n",
            "Epoch: 00 [13044/20624 ( 63%)], Train Loss: 0.63438\n",
            "Epoch: 00 [13084/20624 ( 63%)], Train Loss: 0.63349\n",
            "Epoch: 00 [13124/20624 ( 64%)], Train Loss: 0.63269\n",
            "Epoch: 00 [13164/20624 ( 64%)], Train Loss: 0.63197\n",
            "Epoch: 00 [13204/20624 ( 64%)], Train Loss: 0.63208\n",
            "Epoch: 00 [13244/20624 ( 64%)], Train Loss: 0.63146\n",
            "Epoch: 00 [13284/20624 ( 64%)], Train Loss: 0.63130\n",
            "Epoch: 00 [13324/20624 ( 65%)], Train Loss: 0.63080\n",
            "Epoch: 00 [13364/20624 ( 65%)], Train Loss: 0.63027\n",
            "Epoch: 00 [13404/20624 ( 65%)], Train Loss: 0.62937\n",
            "Epoch: 00 [13444/20624 ( 65%)], Train Loss: 0.62896\n",
            "Epoch: 00 [13484/20624 ( 65%)], Train Loss: 0.62904\n",
            "Epoch: 00 [13524/20624 ( 66%)], Train Loss: 0.62825\n",
            "Epoch: 00 [13564/20624 ( 66%)], Train Loss: 0.62839\n",
            "Epoch: 00 [13604/20624 ( 66%)], Train Loss: 0.62775\n",
            "Epoch: 00 [13644/20624 ( 66%)], Train Loss: 0.62722\n",
            "Epoch: 00 [13684/20624 ( 66%)], Train Loss: 0.62680\n",
            "Epoch: 00 [13724/20624 ( 67%)], Train Loss: 0.62646\n",
            "Epoch: 00 [13764/20624 ( 67%)], Train Loss: 0.62606\n",
            "Epoch: 00 [13804/20624 ( 67%)], Train Loss: 0.62482\n",
            "Epoch: 00 [13844/20624 ( 67%)], Train Loss: 0.62426\n",
            "Epoch: 00 [13884/20624 ( 67%)], Train Loss: 0.62376\n",
            "Epoch: 00 [13924/20624 ( 68%)], Train Loss: 0.62288\n",
            "Epoch: 00 [13964/20624 ( 68%)], Train Loss: 0.62214\n",
            "Epoch: 00 [14004/20624 ( 68%)], Train Loss: 0.62202\n",
            "Epoch: 00 [14044/20624 ( 68%)], Train Loss: 0.62118\n",
            "Epoch: 00 [14084/20624 ( 68%)], Train Loss: 0.62081\n",
            "Epoch: 00 [14124/20624 ( 68%)], Train Loss: 0.62166\n",
            "Epoch: 00 [14164/20624 ( 69%)], Train Loss: 0.62100\n",
            "Epoch: 00 [14204/20624 ( 69%)], Train Loss: 0.62037\n",
            "Epoch: 00 [14244/20624 ( 69%)], Train Loss: 0.61969\n",
            "Epoch: 00 [14284/20624 ( 69%)], Train Loss: 0.61912\n",
            "Epoch: 00 [14324/20624 ( 69%)], Train Loss: 0.61891\n",
            "Epoch: 00 [14364/20624 ( 70%)], Train Loss: 0.61876\n",
            "Epoch: 00 [14404/20624 ( 70%)], Train Loss: 0.61877\n",
            "Epoch: 00 [14444/20624 ( 70%)], Train Loss: 0.61842\n",
            "Epoch: 00 [14484/20624 ( 70%)], Train Loss: 0.61779\n",
            "Epoch: 00 [14524/20624 ( 70%)], Train Loss: 0.61744\n",
            "Epoch: 00 [14564/20624 ( 71%)], Train Loss: 0.61699\n",
            "Epoch: 00 [14604/20624 ( 71%)], Train Loss: 0.61660\n",
            "Epoch: 00 [14644/20624 ( 71%)], Train Loss: 0.61603\n",
            "Epoch: 00 [14684/20624 ( 71%)], Train Loss: 0.61541\n",
            "Epoch: 00 [14724/20624 ( 71%)], Train Loss: 0.61519\n",
            "Epoch: 00 [14764/20624 ( 72%)], Train Loss: 0.61597\n",
            "Epoch: 00 [14804/20624 ( 72%)], Train Loss: 0.61588\n",
            "Epoch: 00 [14844/20624 ( 72%)], Train Loss: 0.61589\n",
            "Epoch: 00 [14884/20624 ( 72%)], Train Loss: 0.61544\n",
            "Epoch: 00 [14924/20624 ( 72%)], Train Loss: 0.61551\n",
            "Epoch: 00 [14964/20624 ( 73%)], Train Loss: 0.61561\n",
            "Epoch: 00 [15004/20624 ( 73%)], Train Loss: 0.61553\n",
            "Epoch: 00 [15044/20624 ( 73%)], Train Loss: 0.61531\n",
            "Epoch: 00 [15084/20624 ( 73%)], Train Loss: 0.61519\n",
            "Epoch: 00 [15124/20624 ( 73%)], Train Loss: 0.61483\n",
            "Epoch: 00 [15164/20624 ( 74%)], Train Loss: 0.61414\n",
            "Epoch: 00 [15204/20624 ( 74%)], Train Loss: 0.61297\n",
            "Epoch: 00 [15244/20624 ( 74%)], Train Loss: 0.61295\n",
            "Epoch: 00 [15284/20624 ( 74%)], Train Loss: 0.61215\n",
            "Epoch: 00 [15324/20624 ( 74%)], Train Loss: 0.61197\n",
            "Epoch: 00 [15364/20624 ( 74%)], Train Loss: 0.61205\n",
            "Epoch: 00 [15404/20624 ( 75%)], Train Loss: 0.61195\n",
            "Epoch: 00 [15444/20624 ( 75%)], Train Loss: 0.61121\n",
            "Epoch: 00 [15484/20624 ( 75%)], Train Loss: 0.61063\n",
            "Epoch: 00 [15524/20624 ( 75%)], Train Loss: 0.61006\n",
            "Epoch: 00 [15564/20624 ( 75%)], Train Loss: 0.60924\n",
            "Epoch: 00 [15604/20624 ( 76%)], Train Loss: 0.60860\n",
            "Epoch: 00 [15644/20624 ( 76%)], Train Loss: 0.60790\n",
            "Epoch: 00 [15684/20624 ( 76%)], Train Loss: 0.60714\n",
            "Epoch: 00 [15724/20624 ( 76%)], Train Loss: 0.60719\n",
            "Epoch: 00 [15764/20624 ( 76%)], Train Loss: 0.60650\n",
            "Epoch: 00 [15804/20624 ( 77%)], Train Loss: 0.60581\n",
            "Epoch: 00 [15844/20624 ( 77%)], Train Loss: 0.60513\n",
            "Epoch: 00 [15884/20624 ( 77%)], Train Loss: 0.60444\n",
            "Epoch: 00 [15924/20624 ( 77%)], Train Loss: 0.60409\n",
            "Epoch: 00 [15964/20624 ( 77%)], Train Loss: 0.60414\n",
            "Epoch: 00 [16004/20624 ( 78%)], Train Loss: 0.60395\n",
            "Epoch: 00 [16044/20624 ( 78%)], Train Loss: 0.60347\n",
            "Epoch: 00 [16084/20624 ( 78%)], Train Loss: 0.60318\n",
            "Epoch: 00 [16124/20624 ( 78%)], Train Loss: 0.60237\n",
            "Epoch: 00 [16164/20624 ( 78%)], Train Loss: 0.60192\n",
            "Epoch: 00 [16204/20624 ( 79%)], Train Loss: 0.60178\n",
            "Epoch: 00 [16244/20624 ( 79%)], Train Loss: 0.60145\n",
            "Epoch: 00 [16284/20624 ( 79%)], Train Loss: 0.60095\n",
            "Epoch: 00 [16324/20624 ( 79%)], Train Loss: 0.59999\n",
            "Epoch: 00 [16364/20624 ( 79%)], Train Loss: 0.59938\n",
            "Epoch: 00 [16404/20624 ( 80%)], Train Loss: 0.59896\n",
            "Epoch: 00 [16444/20624 ( 80%)], Train Loss: 0.59854\n",
            "Epoch: 00 [16484/20624 ( 80%)], Train Loss: 0.59802\n",
            "Epoch: 00 [16524/20624 ( 80%)], Train Loss: 0.59746\n",
            "Epoch: 00 [16564/20624 ( 80%)], Train Loss: 0.59655\n",
            "Epoch: 00 [16604/20624 ( 81%)], Train Loss: 0.59634\n",
            "Epoch: 00 [16644/20624 ( 81%)], Train Loss: 0.59575\n",
            "Epoch: 00 [16684/20624 ( 81%)], Train Loss: 0.59598\n",
            "Epoch: 00 [16724/20624 ( 81%)], Train Loss: 0.59533\n",
            "Epoch: 00 [16764/20624 ( 81%)], Train Loss: 0.59465\n",
            "Epoch: 00 [16804/20624 ( 81%)], Train Loss: 0.59452\n",
            "Epoch: 00 [16844/20624 ( 82%)], Train Loss: 0.59387\n",
            "Epoch: 00 [16884/20624 ( 82%)], Train Loss: 0.59351\n",
            "Epoch: 00 [16924/20624 ( 82%)], Train Loss: 0.59315\n",
            "Epoch: 00 [16964/20624 ( 82%)], Train Loss: 0.59300\n",
            "Epoch: 00 [17004/20624 ( 82%)], Train Loss: 0.59253\n",
            "Epoch: 00 [17044/20624 ( 83%)], Train Loss: 0.59197\n",
            "Epoch: 00 [17084/20624 ( 83%)], Train Loss: 0.59198\n",
            "Epoch: 00 [17124/20624 ( 83%)], Train Loss: 0.59119\n",
            "Epoch: 00 [17164/20624 ( 83%)], Train Loss: 0.59088\n",
            "Epoch: 00 [17204/20624 ( 83%)], Train Loss: 0.59068\n",
            "Epoch: 00 [17244/20624 ( 84%)], Train Loss: 0.59050\n",
            "Epoch: 00 [17284/20624 ( 84%)], Train Loss: 0.59047\n",
            "Epoch: 00 [17324/20624 ( 84%)], Train Loss: 0.58973\n",
            "Epoch: 00 [17364/20624 ( 84%)], Train Loss: 0.58946\n",
            "Epoch: 00 [17404/20624 ( 84%)], Train Loss: 0.58894\n",
            "Epoch: 00 [17444/20624 ( 85%)], Train Loss: 0.58848\n",
            "Epoch: 00 [17484/20624 ( 85%)], Train Loss: 0.58881\n",
            "Epoch: 00 [17524/20624 ( 85%)], Train Loss: 0.58875\n",
            "Epoch: 00 [17564/20624 ( 85%)], Train Loss: 0.58866\n",
            "Epoch: 00 [17604/20624 ( 85%)], Train Loss: 0.58847\n",
            "Epoch: 00 [17644/20624 ( 86%)], Train Loss: 0.58806\n",
            "Epoch: 00 [17684/20624 ( 86%)], Train Loss: 0.58753\n",
            "Epoch: 00 [17724/20624 ( 86%)], Train Loss: 0.58745\n",
            "Epoch: 00 [17764/20624 ( 86%)], Train Loss: 0.58709\n",
            "Epoch: 00 [17804/20624 ( 86%)], Train Loss: 0.58637\n",
            "Epoch: 00 [17844/20624 ( 87%)], Train Loss: 0.58596\n",
            "Epoch: 00 [17884/20624 ( 87%)], Train Loss: 0.58569\n",
            "Epoch: 00 [17924/20624 ( 87%)], Train Loss: 0.58496\n",
            "Epoch: 00 [17964/20624 ( 87%)], Train Loss: 0.58483\n",
            "Epoch: 00 [18004/20624 ( 87%)], Train Loss: 0.58525\n",
            "Epoch: 00 [18044/20624 ( 87%)], Train Loss: 0.58480\n",
            "Epoch: 00 [18084/20624 ( 88%)], Train Loss: 0.58457\n",
            "Epoch: 00 [18124/20624 ( 88%)], Train Loss: 0.58439\n",
            "Epoch: 00 [18164/20624 ( 88%)], Train Loss: 0.58416\n",
            "Epoch: 00 [18204/20624 ( 88%)], Train Loss: 0.58381\n",
            "Epoch: 00 [18244/20624 ( 88%)], Train Loss: 0.58364\n",
            "Epoch: 00 [18284/20624 ( 89%)], Train Loss: 0.58325\n",
            "Epoch: 00 [18324/20624 ( 89%)], Train Loss: 0.58279\n",
            "Epoch: 00 [18364/20624 ( 89%)], Train Loss: 0.58246\n",
            "Epoch: 00 [18404/20624 ( 89%)], Train Loss: 0.58212\n",
            "Epoch: 00 [18444/20624 ( 89%)], Train Loss: 0.58171\n",
            "Epoch: 00 [18484/20624 ( 90%)], Train Loss: 0.58156\n",
            "Epoch: 00 [18524/20624 ( 90%)], Train Loss: 0.58195\n",
            "Epoch: 00 [18564/20624 ( 90%)], Train Loss: 0.58143\n",
            "Epoch: 00 [18604/20624 ( 90%)], Train Loss: 0.58205\n",
            "Epoch: 00 [18644/20624 ( 90%)], Train Loss: 0.58142\n",
            "Epoch: 00 [18684/20624 ( 91%)], Train Loss: 0.58111\n",
            "Epoch: 00 [18724/20624 ( 91%)], Train Loss: 0.58063\n",
            "Epoch: 00 [18764/20624 ( 91%)], Train Loss: 0.58013\n",
            "Epoch: 00 [18804/20624 ( 91%)], Train Loss: 0.57945\n",
            "Epoch: 00 [18844/20624 ( 91%)], Train Loss: 0.57935\n",
            "Epoch: 00 [18884/20624 ( 92%)], Train Loss: 0.57944\n",
            "Epoch: 00 [18924/20624 ( 92%)], Train Loss: 0.57956\n",
            "Epoch: 00 [18964/20624 ( 92%)], Train Loss: 0.57913\n",
            "Epoch: 00 [19004/20624 ( 92%)], Train Loss: 0.57905\n",
            "Epoch: 00 [19044/20624 ( 92%)], Train Loss: 0.57898\n",
            "Epoch: 00 [19084/20624 ( 93%)], Train Loss: 0.57884\n",
            "Epoch: 00 [19124/20624 ( 93%)], Train Loss: 0.57857\n",
            "Epoch: 00 [19164/20624 ( 93%)], Train Loss: 0.57825\n",
            "Epoch: 00 [19204/20624 ( 93%)], Train Loss: 0.57792\n",
            "Epoch: 00 [19244/20624 ( 93%)], Train Loss: 0.57748\n",
            "Epoch: 00 [19284/20624 ( 94%)], Train Loss: 0.57753\n",
            "Epoch: 00 [19324/20624 ( 94%)], Train Loss: 0.57687\n",
            "Epoch: 00 [19364/20624 ( 94%)], Train Loss: 0.57656\n",
            "Epoch: 00 [19404/20624 ( 94%)], Train Loss: 0.57644\n",
            "Epoch: 00 [19444/20624 ( 94%)], Train Loss: 0.57632\n",
            "Epoch: 00 [19484/20624 ( 94%)], Train Loss: 0.57632\n",
            "Epoch: 00 [19524/20624 ( 95%)], Train Loss: 0.57606\n",
            "Epoch: 00 [19564/20624 ( 95%)], Train Loss: 0.57573\n",
            "Epoch: 00 [19604/20624 ( 95%)], Train Loss: 0.57535\n",
            "Epoch: 00 [19644/20624 ( 95%)], Train Loss: 0.57480\n",
            "Epoch: 00 [19684/20624 ( 95%)], Train Loss: 0.57486\n",
            "Epoch: 00 [19724/20624 ( 96%)], Train Loss: 0.57423\n",
            "Epoch: 00 [19764/20624 ( 96%)], Train Loss: 0.57371\n",
            "Epoch: 00 [19804/20624 ( 96%)], Train Loss: 0.57324\n",
            "Epoch: 00 [19844/20624 ( 96%)], Train Loss: 0.57293\n",
            "Epoch: 00 [19884/20624 ( 96%)], Train Loss: 0.57270\n",
            "Epoch: 00 [19924/20624 ( 97%)], Train Loss: 0.57250\n",
            "Epoch: 00 [19964/20624 ( 97%)], Train Loss: 0.57194\n",
            "Epoch: 00 [20004/20624 ( 97%)], Train Loss: 0.57181\n",
            "Epoch: 00 [20044/20624 ( 97%)], Train Loss: 0.57185\n",
            "Epoch: 00 [20084/20624 ( 97%)], Train Loss: 0.57135\n",
            "Epoch: 00 [20124/20624 ( 98%)], Train Loss: 0.57099\n",
            "Epoch: 00 [20164/20624 ( 98%)], Train Loss: 0.57070\n",
            "Epoch: 00 [20204/20624 ( 98%)], Train Loss: 0.57021\n",
            "Epoch: 00 [20244/20624 ( 98%)], Train Loss: 0.56989\n",
            "Epoch: 00 [20284/20624 ( 98%)], Train Loss: 0.56933\n",
            "Epoch: 00 [20324/20624 ( 99%)], Train Loss: 0.56880\n",
            "Epoch: 00 [20364/20624 ( 99%)], Train Loss: 0.56869\n",
            "Epoch: 00 [20404/20624 ( 99%)], Train Loss: 0.56845\n",
            "Epoch: 00 [20444/20624 ( 99%)], Train Loss: 0.56828\n",
            "Epoch: 00 [20484/20624 ( 99%)], Train Loss: 0.56782\n",
            "Epoch: 00 [20524/20624 (100%)], Train Loss: 0.56730\n",
            "Epoch: 00 [20564/20624 (100%)], Train Loss: 0.56740\n",
            "Epoch: 00 [20604/20624 (100%)], Train Loss: 0.56696\n",
            "Epoch: 00 [20624/20624 (100%)], Train Loss: 0.56658\n",
            "----Validation Results Summary----\n",
            "Epoch: [0] Valid Loss: 0.27200\n",
            "0 Epoch, Best epoch was updated! Valid Loss: 0.27200\n",
            "Saving model checkpoint to output/checkpoint-fold-0.\n",
            "\n",
            "Epoch: 01 [    4/20624 (  0%)], Train Loss: 0.27419\n",
            "Epoch: 01 [   44/20624 (  0%)], Train Loss: 0.42990\n",
            "Epoch: 01 [   84/20624 (  0%)], Train Loss: 0.47394\n",
            "Epoch: 01 [  124/20624 (  1%)], Train Loss: 0.50227\n",
            "Epoch: 01 [  164/20624 (  1%)], Train Loss: 0.49321\n",
            "Epoch: 01 [  204/20624 (  1%)], Train Loss: 0.50813\n",
            "Epoch: 01 [  244/20624 (  1%)], Train Loss: 0.50482\n",
            "Epoch: 01 [  284/20624 (  1%)], Train Loss: 0.52016\n",
            "Epoch: 01 [  324/20624 (  2%)], Train Loss: 0.49631\n",
            "Epoch: 01 [  364/20624 (  2%)], Train Loss: 0.49187\n",
            "Epoch: 01 [  404/20624 (  2%)], Train Loss: 0.49445\n",
            "Epoch: 01 [  444/20624 (  2%)], Train Loss: 0.48414\n",
            "Epoch: 01 [  484/20624 (  2%)], Train Loss: 0.48264\n",
            "Epoch: 01 [  524/20624 (  3%)], Train Loss: 0.46301\n",
            "Epoch: 01 [  564/20624 (  3%)], Train Loss: 0.45428\n",
            "Epoch: 01 [  604/20624 (  3%)], Train Loss: 0.45472\n",
            "Epoch: 01 [  644/20624 (  3%)], Train Loss: 0.45242\n",
            "Epoch: 01 [  684/20624 (  3%)], Train Loss: 0.45615\n",
            "Epoch: 01 [  724/20624 (  4%)], Train Loss: 0.46559\n",
            "Epoch: 01 [  764/20624 (  4%)], Train Loss: 0.46509\n",
            "Epoch: 01 [  804/20624 (  4%)], Train Loss: 0.46403\n",
            "Epoch: 01 [  844/20624 (  4%)], Train Loss: 0.46137\n",
            "Epoch: 01 [  884/20624 (  4%)], Train Loss: 0.45878\n",
            "Epoch: 01 [  924/20624 (  4%)], Train Loss: 0.45020\n",
            "Epoch: 01 [  964/20624 (  5%)], Train Loss: 0.43713\n",
            "Epoch: 01 [ 1004/20624 (  5%)], Train Loss: 0.43765\n",
            "Epoch: 01 [ 1044/20624 (  5%)], Train Loss: 0.42896\n",
            "Epoch: 01 [ 1084/20624 (  5%)], Train Loss: 0.43044\n",
            "Epoch: 01 [ 1124/20624 (  5%)], Train Loss: 0.43156\n",
            "Epoch: 01 [ 1164/20624 (  6%)], Train Loss: 0.42147\n",
            "Epoch: 01 [ 1204/20624 (  6%)], Train Loss: 0.41579\n",
            "Epoch: 01 [ 1244/20624 (  6%)], Train Loss: 0.41193\n",
            "Epoch: 01 [ 1284/20624 (  6%)], Train Loss: 0.40555\n",
            "Epoch: 01 [ 1324/20624 (  6%)], Train Loss: 0.40861\n",
            "Epoch: 01 [ 1364/20624 (  7%)], Train Loss: 0.40913\n",
            "Epoch: 01 [ 1404/20624 (  7%)], Train Loss: 0.40645\n",
            "Epoch: 01 [ 1444/20624 (  7%)], Train Loss: 0.40401\n",
            "Epoch: 01 [ 1484/20624 (  7%)], Train Loss: 0.40020\n",
            "Epoch: 01 [ 1524/20624 (  7%)], Train Loss: 0.39895\n",
            "Epoch: 01 [ 1564/20624 (  8%)], Train Loss: 0.39568\n",
            "Epoch: 01 [ 1604/20624 (  8%)], Train Loss: 0.39615\n",
            "Epoch: 01 [ 1644/20624 (  8%)], Train Loss: 0.39546\n",
            "Epoch: 01 [ 1684/20624 (  8%)], Train Loss: 0.39657\n",
            "Epoch: 01 [ 1724/20624 (  8%)], Train Loss: 0.39801\n",
            "Epoch: 01 [ 1764/20624 (  9%)], Train Loss: 0.40078\n",
            "Epoch: 01 [ 1804/20624 (  9%)], Train Loss: 0.40045\n",
            "Epoch: 01 [ 1844/20624 (  9%)], Train Loss: 0.39996\n",
            "Epoch: 01 [ 1884/20624 (  9%)], Train Loss: 0.39512\n",
            "Epoch: 01 [ 1924/20624 (  9%)], Train Loss: 0.39187\n",
            "Epoch: 01 [ 1964/20624 ( 10%)], Train Loss: 0.39076\n",
            "Epoch: 01 [ 2004/20624 ( 10%)], Train Loss: 0.38718\n",
            "Epoch: 01 [ 2044/20624 ( 10%)], Train Loss: 0.38306\n",
            "Epoch: 01 [ 2084/20624 ( 10%)], Train Loss: 0.37929\n",
            "Epoch: 01 [ 2124/20624 ( 10%)], Train Loss: 0.37792\n",
            "Epoch: 01 [ 2164/20624 ( 10%)], Train Loss: 0.37651\n",
            "Epoch: 01 [ 2204/20624 ( 11%)], Train Loss: 0.37114\n",
            "Epoch: 01 [ 2244/20624 ( 11%)], Train Loss: 0.36992\n",
            "Epoch: 01 [ 2284/20624 ( 11%)], Train Loss: 0.36739\n",
            "Epoch: 01 [ 2324/20624 ( 11%)], Train Loss: 0.36657\n",
            "Epoch: 01 [ 2364/20624 ( 11%)], Train Loss: 0.36523\n",
            "Epoch: 01 [ 2404/20624 ( 12%)], Train Loss: 0.36652\n",
            "Epoch: 01 [ 2444/20624 ( 12%)], Train Loss: 0.36650\n",
            "Epoch: 01 [ 2484/20624 ( 12%)], Train Loss: 0.36562\n",
            "Epoch: 01 [ 2524/20624 ( 12%)], Train Loss: 0.36442\n",
            "Epoch: 01 [ 2564/20624 ( 12%)], Train Loss: 0.36115\n",
            "Epoch: 01 [ 2604/20624 ( 13%)], Train Loss: 0.36260\n",
            "Epoch: 01 [ 2644/20624 ( 13%)], Train Loss: 0.36083\n",
            "Epoch: 01 [ 2684/20624 ( 13%)], Train Loss: 0.35688\n",
            "Epoch: 01 [ 2724/20624 ( 13%)], Train Loss: 0.35596\n",
            "Epoch: 01 [ 2764/20624 ( 13%)], Train Loss: 0.35334\n",
            "Epoch: 01 [ 2804/20624 ( 14%)], Train Loss: 0.35314\n",
            "Epoch: 01 [ 2844/20624 ( 14%)], Train Loss: 0.35233\n",
            "Epoch: 01 [ 2884/20624 ( 14%)], Train Loss: 0.35136\n",
            "Epoch: 01 [ 2924/20624 ( 14%)], Train Loss: 0.34855\n",
            "Epoch: 01 [ 2964/20624 ( 14%)], Train Loss: 0.34647\n",
            "Epoch: 01 [ 3004/20624 ( 15%)], Train Loss: 0.34409\n",
            "Epoch: 01 [ 3044/20624 ( 15%)], Train Loss: 0.34145\n",
            "Epoch: 01 [ 3084/20624 ( 15%)], Train Loss: 0.34064\n",
            "Epoch: 01 [ 3124/20624 ( 15%)], Train Loss: 0.33894\n",
            "Epoch: 01 [ 3164/20624 ( 15%)], Train Loss: 0.33942\n",
            "Epoch: 01 [ 3204/20624 ( 16%)], Train Loss: 0.33872\n",
            "Epoch: 01 [ 3244/20624 ( 16%)], Train Loss: 0.33630\n",
            "Epoch: 01 [ 3284/20624 ( 16%)], Train Loss: 0.33623\n",
            "Epoch: 01 [ 3324/20624 ( 16%)], Train Loss: 0.33443\n",
            "Epoch: 01 [ 3364/20624 ( 16%)], Train Loss: 0.33545\n",
            "Epoch: 01 [ 3404/20624 ( 17%)], Train Loss: 0.33602\n",
            "Epoch: 01 [ 3444/20624 ( 17%)], Train Loss: 0.33659\n",
            "Epoch: 01 [ 3484/20624 ( 17%)], Train Loss: 0.33609\n",
            "Epoch: 01 [ 3524/20624 ( 17%)], Train Loss: 0.33488\n",
            "Epoch: 01 [ 3564/20624 ( 17%)], Train Loss: 0.33371\n",
            "Epoch: 01 [ 3604/20624 ( 17%)], Train Loss: 0.33164\n",
            "Epoch: 01 [ 3644/20624 ( 18%)], Train Loss: 0.32882\n",
            "Epoch: 01 [ 3684/20624 ( 18%)], Train Loss: 0.32805\n",
            "Epoch: 01 [ 3724/20624 ( 18%)], Train Loss: 0.32706\n",
            "Epoch: 01 [ 3764/20624 ( 18%)], Train Loss: 0.32543\n",
            "Epoch: 01 [ 3804/20624 ( 18%)], Train Loss: 0.32475\n",
            "Epoch: 01 [ 3844/20624 ( 19%)], Train Loss: 0.32355\n",
            "Epoch: 01 [ 3884/20624 ( 19%)], Train Loss: 0.32150\n",
            "Epoch: 01 [ 3924/20624 ( 19%)], Train Loss: 0.32049\n",
            "Epoch: 01 [ 3964/20624 ( 19%)], Train Loss: 0.31929\n",
            "Epoch: 01 [ 4004/20624 ( 19%)], Train Loss: 0.31837\n",
            "Epoch: 01 [ 4044/20624 ( 20%)], Train Loss: 0.31730\n",
            "Epoch: 01 [ 4084/20624 ( 20%)], Train Loss: 0.31623\n",
            "Epoch: 01 [ 4124/20624 ( 20%)], Train Loss: 0.31467\n",
            "Epoch: 01 [ 4164/20624 ( 20%)], Train Loss: 0.31322\n",
            "Epoch: 01 [ 4204/20624 ( 20%)], Train Loss: 0.31147\n",
            "Epoch: 01 [ 4244/20624 ( 21%)], Train Loss: 0.30961\n",
            "Epoch: 01 [ 4284/20624 ( 21%)], Train Loss: 0.30747\n",
            "Epoch: 01 [ 4324/20624 ( 21%)], Train Loss: 0.30702\n",
            "Epoch: 01 [ 4364/20624 ( 21%)], Train Loss: 0.30597\n",
            "Epoch: 01 [ 4404/20624 ( 21%)], Train Loss: 0.30532\n",
            "Epoch: 01 [ 4444/20624 ( 22%)], Train Loss: 0.30379\n",
            "Epoch: 01 [ 4484/20624 ( 22%)], Train Loss: 0.30476\n",
            "Epoch: 01 [ 4524/20624 ( 22%)], Train Loss: 0.30517\n",
            "Epoch: 01 [ 4564/20624 ( 22%)], Train Loss: 0.30397\n",
            "Epoch: 01 [ 4604/20624 ( 22%)], Train Loss: 0.30343\n",
            "Epoch: 01 [ 4644/20624 ( 23%)], Train Loss: 0.30372\n",
            "Epoch: 01 [ 4684/20624 ( 23%)], Train Loss: 0.30243\n",
            "Epoch: 01 [ 4724/20624 ( 23%)], Train Loss: 0.30134\n",
            "Epoch: 01 [ 4764/20624 ( 23%)], Train Loss: 0.29956\n",
            "Epoch: 01 [ 4804/20624 ( 23%)], Train Loss: 0.29794\n",
            "Epoch: 01 [ 4844/20624 ( 23%)], Train Loss: 0.29650\n",
            "Epoch: 01 [ 4884/20624 ( 24%)], Train Loss: 0.29489\n",
            "Epoch: 01 [ 4924/20624 ( 24%)], Train Loss: 0.29393\n",
            "Epoch: 01 [ 4964/20624 ( 24%)], Train Loss: 0.29246\n",
            "Epoch: 01 [ 5004/20624 ( 24%)], Train Loss: 0.29155\n",
            "Epoch: 01 [ 5044/20624 ( 24%)], Train Loss: 0.28984\n",
            "Epoch: 01 [ 5084/20624 ( 25%)], Train Loss: 0.28836\n",
            "Epoch: 01 [ 5124/20624 ( 25%)], Train Loss: 0.28774\n",
            "Epoch: 01 [ 5164/20624 ( 25%)], Train Loss: 0.28638\n",
            "Epoch: 01 [ 5204/20624 ( 25%)], Train Loss: 0.28525\n",
            "Epoch: 01 [ 5244/20624 ( 25%)], Train Loss: 0.28489\n",
            "Epoch: 01 [ 5284/20624 ( 26%)], Train Loss: 0.28369\n",
            "Epoch: 01 [ 5324/20624 ( 26%)], Train Loss: 0.28241\n",
            "Epoch: 01 [ 5364/20624 ( 26%)], Train Loss: 0.28171\n",
            "Epoch: 01 [ 5404/20624 ( 26%)], Train Loss: 0.28103\n",
            "Epoch: 01 [ 5444/20624 ( 26%)], Train Loss: 0.28027\n",
            "Epoch: 01 [ 5484/20624 ( 27%)], Train Loss: 0.27894\n",
            "Epoch: 01 [ 5524/20624 ( 27%)], Train Loss: 0.27841\n",
            "Epoch: 01 [ 5564/20624 ( 27%)], Train Loss: 0.27765\n",
            "Epoch: 01 [ 5604/20624 ( 27%)], Train Loss: 0.27732\n",
            "Epoch: 01 [ 5644/20624 ( 27%)], Train Loss: 0.27606\n",
            "Epoch: 01 [ 5684/20624 ( 28%)], Train Loss: 0.27521\n",
            "Epoch: 01 [ 5724/20624 ( 28%)], Train Loss: 0.27497\n",
            "Epoch: 01 [ 5764/20624 ( 28%)], Train Loss: 0.27486\n",
            "Epoch: 01 [ 5804/20624 ( 28%)], Train Loss: 0.27450\n",
            "Epoch: 01 [ 5844/20624 ( 28%)], Train Loss: 0.27386\n",
            "Epoch: 01 [ 5884/20624 ( 29%)], Train Loss: 0.27326\n",
            "Epoch: 01 [ 5924/20624 ( 29%)], Train Loss: 0.27216\n",
            "Epoch: 01 [ 5964/20624 ( 29%)], Train Loss: 0.27182\n",
            "Epoch: 01 [ 6004/20624 ( 29%)], Train Loss: 0.27098\n",
            "Epoch: 01 [ 6044/20624 ( 29%)], Train Loss: 0.27020\n",
            "Epoch: 01 [ 6084/20624 ( 29%)], Train Loss: 0.26895\n",
            "Epoch: 01 [ 6124/20624 ( 30%)], Train Loss: 0.26802\n",
            "Epoch: 01 [ 6164/20624 ( 30%)], Train Loss: 0.26748\n",
            "Epoch: 01 [ 6204/20624 ( 30%)], Train Loss: 0.26622\n",
            "Epoch: 01 [ 6244/20624 ( 30%)], Train Loss: 0.26617\n",
            "Epoch: 01 [ 6284/20624 ( 30%)], Train Loss: 0.26643\n",
            "Epoch: 01 [ 6324/20624 ( 31%)], Train Loss: 0.26578\n",
            "Epoch: 01 [ 6364/20624 ( 31%)], Train Loss: 0.26523\n",
            "Epoch: 01 [ 6404/20624 ( 31%)], Train Loss: 0.26511\n",
            "Epoch: 01 [ 6444/20624 ( 31%)], Train Loss: 0.26455\n",
            "Epoch: 01 [ 6484/20624 ( 31%)], Train Loss: 0.26381\n",
            "Epoch: 01 [ 6524/20624 ( 32%)], Train Loss: 0.26279\n",
            "Epoch: 01 [ 6564/20624 ( 32%)], Train Loss: 0.26239\n",
            "Epoch: 01 [ 6604/20624 ( 32%)], Train Loss: 0.26165\n",
            "Epoch: 01 [ 6644/20624 ( 32%)], Train Loss: 0.26105\n",
            "Epoch: 01 [ 6684/20624 ( 32%)], Train Loss: 0.26042\n",
            "Epoch: 01 [ 6724/20624 ( 33%)], Train Loss: 0.26055\n",
            "Epoch: 01 [ 6764/20624 ( 33%)], Train Loss: 0.26011\n",
            "Epoch: 01 [ 6804/20624 ( 33%)], Train Loss: 0.25956\n",
            "Epoch: 01 [ 6844/20624 ( 33%)], Train Loss: 0.25961\n",
            "Epoch: 01 [ 6884/20624 ( 33%)], Train Loss: 0.25856\n",
            "Epoch: 01 [ 6924/20624 ( 34%)], Train Loss: 0.25824\n",
            "Epoch: 01 [ 6964/20624 ( 34%)], Train Loss: 0.25767\n",
            "Epoch: 01 [ 7004/20624 ( 34%)], Train Loss: 0.25806\n",
            "Epoch: 01 [ 7044/20624 ( 34%)], Train Loss: 0.25778\n",
            "Epoch: 01 [ 7084/20624 ( 34%)], Train Loss: 0.25758\n",
            "Epoch: 01 [ 7124/20624 ( 35%)], Train Loss: 0.25666\n",
            "Epoch: 01 [ 7164/20624 ( 35%)], Train Loss: 0.25604\n",
            "Epoch: 01 [ 7204/20624 ( 35%)], Train Loss: 0.25538\n",
            "Epoch: 01 [ 7244/20624 ( 35%)], Train Loss: 0.25448\n",
            "Epoch: 01 [ 7284/20624 ( 35%)], Train Loss: 0.25405\n",
            "Epoch: 01 [ 7324/20624 ( 36%)], Train Loss: 0.25314\n",
            "Epoch: 01 [ 7364/20624 ( 36%)], Train Loss: 0.25283\n",
            "Epoch: 01 [ 7404/20624 ( 36%)], Train Loss: 0.25213\n",
            "Epoch: 01 [ 7444/20624 ( 36%)], Train Loss: 0.25191\n",
            "Epoch: 01 [ 7484/20624 ( 36%)], Train Loss: 0.25266\n",
            "Epoch: 01 [ 7524/20624 ( 36%)], Train Loss: 0.25192\n",
            "Epoch: 01 [ 7564/20624 ( 37%)], Train Loss: 0.25134\n",
            "Epoch: 01 [ 7604/20624 ( 37%)], Train Loss: 0.25057\n",
            "Epoch: 01 [ 7644/20624 ( 37%)], Train Loss: 0.24962\n",
            "Epoch: 01 [ 7684/20624 ( 37%)], Train Loss: 0.24878\n",
            "Epoch: 01 [ 7724/20624 ( 37%)], Train Loss: 0.24843\n",
            "Epoch: 01 [ 7764/20624 ( 38%)], Train Loss: 0.24790\n",
            "Epoch: 01 [ 7804/20624 ( 38%)], Train Loss: 0.24828\n",
            "Epoch: 01 [ 7844/20624 ( 38%)], Train Loss: 0.24768\n",
            "Epoch: 01 [ 7884/20624 ( 38%)], Train Loss: 0.24723\n",
            "Epoch: 01 [ 7924/20624 ( 38%)], Train Loss: 0.24722\n",
            "Epoch: 01 [ 7964/20624 ( 39%)], Train Loss: 0.24730\n",
            "Epoch: 01 [ 8004/20624 ( 39%)], Train Loss: 0.24700\n",
            "Epoch: 01 [ 8044/20624 ( 39%)], Train Loss: 0.24793\n",
            "Epoch: 01 [ 8084/20624 ( 39%)], Train Loss: 0.24753\n",
            "Epoch: 01 [ 8124/20624 ( 39%)], Train Loss: 0.24817\n",
            "Epoch: 01 [ 8164/20624 ( 40%)], Train Loss: 0.24737\n",
            "Epoch: 01 [ 8204/20624 ( 40%)], Train Loss: 0.24670\n",
            "Epoch: 01 [ 8244/20624 ( 40%)], Train Loss: 0.24621\n",
            "Epoch: 01 [ 8284/20624 ( 40%)], Train Loss: 0.24585\n",
            "Epoch: 01 [ 8324/20624 ( 40%)], Train Loss: 0.24559\n",
            "Epoch: 01 [ 8364/20624 ( 41%)], Train Loss: 0.24537\n",
            "Epoch: 01 [ 8404/20624 ( 41%)], Train Loss: 0.24492\n",
            "Epoch: 01 [ 8444/20624 ( 41%)], Train Loss: 0.24446\n",
            "Epoch: 01 [ 8484/20624 ( 41%)], Train Loss: 0.24389\n",
            "Epoch: 01 [ 8524/20624 ( 41%)], Train Loss: 0.24341\n",
            "Epoch: 01 [ 8564/20624 ( 42%)], Train Loss: 0.24274\n",
            "Epoch: 01 [ 8604/20624 ( 42%)], Train Loss: 0.24207\n",
            "Epoch: 01 [ 8644/20624 ( 42%)], Train Loss: 0.24168\n",
            "Epoch: 01 [ 8684/20624 ( 42%)], Train Loss: 0.24116\n",
            "Epoch: 01 [ 8724/20624 ( 42%)], Train Loss: 0.24027\n",
            "Epoch: 01 [ 8764/20624 ( 42%)], Train Loss: 0.23974\n",
            "Epoch: 01 [ 8804/20624 ( 43%)], Train Loss: 0.23946\n",
            "Epoch: 01 [ 8844/20624 ( 43%)], Train Loss: 0.23957\n",
            "Epoch: 01 [ 8884/20624 ( 43%)], Train Loss: 0.23946\n",
            "Epoch: 01 [ 8924/20624 ( 43%)], Train Loss: 0.24011\n",
            "Epoch: 01 [ 8964/20624 ( 43%)], Train Loss: 0.23951\n",
            "Epoch: 01 [ 9004/20624 ( 44%)], Train Loss: 0.23952\n",
            "Epoch: 01 [ 9044/20624 ( 44%)], Train Loss: 0.23890\n",
            "Epoch: 01 [ 9084/20624 ( 44%)], Train Loss: 0.23819\n",
            "Epoch: 01 [ 9124/20624 ( 44%)], Train Loss: 0.23803\n",
            "Epoch: 01 [ 9164/20624 ( 44%)], Train Loss: 0.23852\n",
            "Epoch: 01 [ 9204/20624 ( 45%)], Train Loss: 0.23833\n",
            "Epoch: 01 [ 9244/20624 ( 45%)], Train Loss: 0.23802\n",
            "Epoch: 01 [ 9284/20624 ( 45%)], Train Loss: 0.23768\n",
            "Epoch: 01 [ 9324/20624 ( 45%)], Train Loss: 0.23731\n",
            "Epoch: 01 [ 9364/20624 ( 45%)], Train Loss: 0.23693\n",
            "Epoch: 01 [ 9404/20624 ( 46%)], Train Loss: 0.23644\n",
            "Epoch: 01 [ 9444/20624 ( 46%)], Train Loss: 0.23575\n",
            "Epoch: 01 [ 9484/20624 ( 46%)], Train Loss: 0.23504\n",
            "Epoch: 01 [ 9524/20624 ( 46%)], Train Loss: 0.23498\n",
            "Epoch: 01 [ 9564/20624 ( 46%)], Train Loss: 0.23450\n",
            "Epoch: 01 [ 9604/20624 ( 47%)], Train Loss: 0.23438\n",
            "Epoch: 01 [ 9644/20624 ( 47%)], Train Loss: 0.23419\n",
            "Epoch: 01 [ 9684/20624 ( 47%)], Train Loss: 0.23424\n",
            "Epoch: 01 [ 9724/20624 ( 47%)], Train Loss: 0.23400\n",
            "Epoch: 01 [ 9764/20624 ( 47%)], Train Loss: 0.23449\n",
            "Epoch: 01 [ 9804/20624 ( 48%)], Train Loss: 0.23419\n",
            "Epoch: 01 [ 9844/20624 ( 48%)], Train Loss: 0.23394\n",
            "Epoch: 01 [ 9884/20624 ( 48%)], Train Loss: 0.23349\n",
            "Epoch: 01 [ 9924/20624 ( 48%)], Train Loss: 0.23315\n",
            "Epoch: 01 [ 9964/20624 ( 48%)], Train Loss: 0.23295\n",
            "Epoch: 01 [10004/20624 ( 49%)], Train Loss: 0.23231\n",
            "Epoch: 01 [10044/20624 ( 49%)], Train Loss: 0.23214\n",
            "Epoch: 01 [10084/20624 ( 49%)], Train Loss: 0.23186\n",
            "Epoch: 01 [10124/20624 ( 49%)], Train Loss: 0.23180\n",
            "Epoch: 01 [10164/20624 ( 49%)], Train Loss: 0.23205\n",
            "Epoch: 01 [10204/20624 ( 49%)], Train Loss: 0.23177\n",
            "Epoch: 01 [10244/20624 ( 50%)], Train Loss: 0.23116\n",
            "Epoch: 01 [10284/20624 ( 50%)], Train Loss: 0.23087\n",
            "Epoch: 01 [10324/20624 ( 50%)], Train Loss: 0.23094\n",
            "Epoch: 01 [10364/20624 ( 50%)], Train Loss: 0.23063\n",
            "Epoch: 01 [10404/20624 ( 50%)], Train Loss: 0.23051\n",
            "Epoch: 01 [10444/20624 ( 51%)], Train Loss: 0.23003\n",
            "Epoch: 01 [10484/20624 ( 51%)], Train Loss: 0.22983\n",
            "Epoch: 01 [10524/20624 ( 51%)], Train Loss: 0.22993\n",
            "Epoch: 01 [10564/20624 ( 51%)], Train Loss: 0.22958\n",
            "Epoch: 01 [10604/20624 ( 51%)], Train Loss: 0.22896\n",
            "Epoch: 01 [10644/20624 ( 52%)], Train Loss: 0.22842\n",
            "Epoch: 01 [10684/20624 ( 52%)], Train Loss: 0.22807\n",
            "Epoch: 01 [10724/20624 ( 52%)], Train Loss: 0.22777\n",
            "Epoch: 01 [10764/20624 ( 52%)], Train Loss: 0.22751\n",
            "Epoch: 01 [10804/20624 ( 52%)], Train Loss: 0.22746\n",
            "Epoch: 01 [10844/20624 ( 53%)], Train Loss: 0.22775\n",
            "Epoch: 01 [10884/20624 ( 53%)], Train Loss: 0.22763\n",
            "Epoch: 01 [10924/20624 ( 53%)], Train Loss: 0.22736\n",
            "Epoch: 01 [10964/20624 ( 53%)], Train Loss: 0.22739\n",
            "Epoch: 01 [11004/20624 ( 53%)], Train Loss: 0.22701\n",
            "Epoch: 01 [11044/20624 ( 54%)], Train Loss: 0.22679\n",
            "Epoch: 01 [11084/20624 ( 54%)], Train Loss: 0.22720\n",
            "Epoch: 01 [11124/20624 ( 54%)], Train Loss: 0.22671\n",
            "Epoch: 01 [11164/20624 ( 54%)], Train Loss: 0.22647\n",
            "Epoch: 01 [11204/20624 ( 54%)], Train Loss: 0.22637\n",
            "Epoch: 01 [11244/20624 ( 55%)], Train Loss: 0.22618\n",
            "Epoch: 01 [11284/20624 ( 55%)], Train Loss: 0.22609\n",
            "Epoch: 01 [11324/20624 ( 55%)], Train Loss: 0.22580\n",
            "Epoch: 01 [11364/20624 ( 55%)], Train Loss: 0.22553\n",
            "Epoch: 01 [11404/20624 ( 55%)], Train Loss: 0.22520\n",
            "Epoch: 01 [11444/20624 ( 55%)], Train Loss: 0.22521\n",
            "Epoch: 01 [11484/20624 ( 56%)], Train Loss: 0.22501\n",
            "Epoch: 01 [11524/20624 ( 56%)], Train Loss: 0.22499\n",
            "Epoch: 01 [11564/20624 ( 56%)], Train Loss: 0.22461\n",
            "Epoch: 01 [11604/20624 ( 56%)], Train Loss: 0.22433\n",
            "Epoch: 01 [11644/20624 ( 56%)], Train Loss: 0.22416\n",
            "Epoch: 01 [11684/20624 ( 57%)], Train Loss: 0.22362\n",
            "Epoch: 01 [11724/20624 ( 57%)], Train Loss: 0.22329\n",
            "Epoch: 01 [11764/20624 ( 57%)], Train Loss: 0.22310\n",
            "Epoch: 01 [11804/20624 ( 57%)], Train Loss: 0.22344\n",
            "Epoch: 01 [11844/20624 ( 57%)], Train Loss: 0.22299\n",
            "Epoch: 01 [11884/20624 ( 58%)], Train Loss: 0.22253\n",
            "Epoch: 01 [11924/20624 ( 58%)], Train Loss: 0.22239\n",
            "Epoch: 01 [11964/20624 ( 58%)], Train Loss: 0.22192\n",
            "Epoch: 01 [12004/20624 ( 58%)], Train Loss: 0.22207\n",
            "Epoch: 01 [12044/20624 ( 58%)], Train Loss: 0.22172\n",
            "Epoch: 01 [12084/20624 ( 59%)], Train Loss: 0.22151\n",
            "Epoch: 01 [12124/20624 ( 59%)], Train Loss: 0.22123\n",
            "Epoch: 01 [12164/20624 ( 59%)], Train Loss: 0.22085\n",
            "Epoch: 01 [12204/20624 ( 59%)], Train Loss: 0.22045\n",
            "Epoch: 01 [12244/20624 ( 59%)], Train Loss: 0.22017\n",
            "Epoch: 01 [12284/20624 ( 60%)], Train Loss: 0.22020\n",
            "Epoch: 01 [12324/20624 ( 60%)], Train Loss: 0.22013\n",
            "Epoch: 01 [12364/20624 ( 60%)], Train Loss: 0.21973\n",
            "Epoch: 01 [12404/20624 ( 60%)], Train Loss: 0.21968\n",
            "Epoch: 01 [12444/20624 ( 60%)], Train Loss: 0.21958\n",
            "Epoch: 01 [12484/20624 ( 61%)], Train Loss: 0.21905\n",
            "Epoch: 01 [12524/20624 ( 61%)], Train Loss: 0.21856\n",
            "Epoch: 01 [12564/20624 ( 61%)], Train Loss: 0.21831\n",
            "Epoch: 01 [12604/20624 ( 61%)], Train Loss: 0.21818\n",
            "Epoch: 01 [12644/20624 ( 61%)], Train Loss: 0.21784\n",
            "Epoch: 01 [12684/20624 ( 62%)], Train Loss: 0.21760\n",
            "Epoch: 01 [12724/20624 ( 62%)], Train Loss: 0.21771\n",
            "Epoch: 01 [12764/20624 ( 62%)], Train Loss: 0.21753\n",
            "Epoch: 01 [12804/20624 ( 62%)], Train Loss: 0.21727\n",
            "Epoch: 01 [12844/20624 ( 62%)], Train Loss: 0.21779\n",
            "Epoch: 01 [12884/20624 ( 62%)], Train Loss: 0.21770\n",
            "Epoch: 01 [12924/20624 ( 63%)], Train Loss: 0.21779\n",
            "Epoch: 01 [12964/20624 ( 63%)], Train Loss: 0.21776\n",
            "Epoch: 01 [13004/20624 ( 63%)], Train Loss: 0.21749\n",
            "Epoch: 01 [13044/20624 ( 63%)], Train Loss: 0.21743\n",
            "Epoch: 01 [13084/20624 ( 63%)], Train Loss: 0.21710\n",
            "Epoch: 01 [13124/20624 ( 64%)], Train Loss: 0.21680\n",
            "Epoch: 01 [13164/20624 ( 64%)], Train Loss: 0.21656\n",
            "Epoch: 01 [13204/20624 ( 64%)], Train Loss: 0.21658\n",
            "Epoch: 01 [13244/20624 ( 64%)], Train Loss: 0.21638\n",
            "Epoch: 01 [13284/20624 ( 64%)], Train Loss: 0.21611\n",
            "Epoch: 01 [13324/20624 ( 65%)], Train Loss: 0.21597\n",
            "Epoch: 01 [13364/20624 ( 65%)], Train Loss: 0.21562\n",
            "Epoch: 01 [13404/20624 ( 65%)], Train Loss: 0.21527\n",
            "Epoch: 01 [13444/20624 ( 65%)], Train Loss: 0.21513\n",
            "Epoch: 01 [13484/20624 ( 65%)], Train Loss: 0.21508\n",
            "Epoch: 01 [13524/20624 ( 66%)], Train Loss: 0.21490\n",
            "Epoch: 01 [13564/20624 ( 66%)], Train Loss: 0.21505\n",
            "Epoch: 01 [13604/20624 ( 66%)], Train Loss: 0.21462\n",
            "Epoch: 01 [13644/20624 ( 66%)], Train Loss: 0.21432\n",
            "Epoch: 01 [13684/20624 ( 66%)], Train Loss: 0.21419\n",
            "Epoch: 01 [13724/20624 ( 67%)], Train Loss: 0.21397\n",
            "Epoch: 01 [13764/20624 ( 67%)], Train Loss: 0.21368\n",
            "Epoch: 01 [13804/20624 ( 67%)], Train Loss: 0.21332\n",
            "Epoch: 01 [13844/20624 ( 67%)], Train Loss: 0.21306\n",
            "Epoch: 01 [13884/20624 ( 67%)], Train Loss: 0.21273\n",
            "Epoch: 01 [13924/20624 ( 68%)], Train Loss: 0.21228\n",
            "Epoch: 01 [13964/20624 ( 68%)], Train Loss: 0.21195\n",
            "Epoch: 01 [14004/20624 ( 68%)], Train Loss: 0.21198\n",
            "Epoch: 01 [14044/20624 ( 68%)], Train Loss: 0.21186\n",
            "Epoch: 01 [14084/20624 ( 68%)], Train Loss: 0.21173\n",
            "Epoch: 01 [14124/20624 ( 68%)], Train Loss: 0.21211\n",
            "Epoch: 01 [14164/20624 ( 69%)], Train Loss: 0.21179\n",
            "Epoch: 01 [14204/20624 ( 69%)], Train Loss: 0.21149\n",
            "Epoch: 01 [14244/20624 ( 69%)], Train Loss: 0.21112\n",
            "Epoch: 01 [14284/20624 ( 69%)], Train Loss: 0.21123\n",
            "Epoch: 01 [14324/20624 ( 69%)], Train Loss: 0.21111\n",
            "Epoch: 01 [14364/20624 ( 70%)], Train Loss: 0.21111\n",
            "Epoch: 01 [14404/20624 ( 70%)], Train Loss: 0.21094\n",
            "Epoch: 01 [14444/20624 ( 70%)], Train Loss: 0.21073\n",
            "Epoch: 01 [14484/20624 ( 70%)], Train Loss: 0.21057\n",
            "Epoch: 01 [14524/20624 ( 70%)], Train Loss: 0.21055\n",
            "Epoch: 01 [14564/20624 ( 71%)], Train Loss: 0.21045\n",
            "Epoch: 01 [14604/20624 ( 71%)], Train Loss: 0.21032\n",
            "Epoch: 01 [14644/20624 ( 71%)], Train Loss: 0.21004\n",
            "Epoch: 01 [14684/20624 ( 71%)], Train Loss: 0.20979\n",
            "Epoch: 01 [14724/20624 ( 71%)], Train Loss: 0.20989\n",
            "Epoch: 01 [14764/20624 ( 72%)], Train Loss: 0.21005\n",
            "Epoch: 01 [14804/20624 ( 72%)], Train Loss: 0.21016\n",
            "Epoch: 01 [14844/20624 ( 72%)], Train Loss: 0.21019\n",
            "Epoch: 01 [14884/20624 ( 72%)], Train Loss: 0.21014\n",
            "Epoch: 01 [14924/20624 ( 72%)], Train Loss: 0.21013\n",
            "Epoch: 01 [14964/20624 ( 73%)], Train Loss: 0.21046\n",
            "Epoch: 01 [15004/20624 ( 73%)], Train Loss: 0.21038\n",
            "Epoch: 01 [15044/20624 ( 73%)], Train Loss: 0.21040\n",
            "Epoch: 01 [15084/20624 ( 73%)], Train Loss: 0.21074\n",
            "Epoch: 01 [15124/20624 ( 73%)], Train Loss: 0.21067\n",
            "Epoch: 01 [15164/20624 ( 74%)], Train Loss: 0.21026\n",
            "Epoch: 01 [15204/20624 ( 74%)], Train Loss: 0.21007\n",
            "Epoch: 01 [15244/20624 ( 74%)], Train Loss: 0.20979\n",
            "Epoch: 01 [15284/20624 ( 74%)], Train Loss: 0.20944\n",
            "Epoch: 01 [15324/20624 ( 74%)], Train Loss: 0.20921\n",
            "Epoch: 01 [15364/20624 ( 74%)], Train Loss: 0.20941\n",
            "Epoch: 01 [15404/20624 ( 75%)], Train Loss: 0.20938\n",
            "Epoch: 01 [15444/20624 ( 75%)], Train Loss: 0.20910\n",
            "Epoch: 01 [15484/20624 ( 75%)], Train Loss: 0.20884\n",
            "Epoch: 01 [15524/20624 ( 75%)], Train Loss: 0.20858\n",
            "Epoch: 01 [15564/20624 ( 75%)], Train Loss: 0.20831\n",
            "Epoch: 01 [15604/20624 ( 76%)], Train Loss: 0.20808\n",
            "Epoch: 01 [15644/20624 ( 76%)], Train Loss: 0.20776\n",
            "Epoch: 01 [15684/20624 ( 76%)], Train Loss: 0.20746\n",
            "Epoch: 01 [15724/20624 ( 76%)], Train Loss: 0.20754\n",
            "Epoch: 01 [15764/20624 ( 76%)], Train Loss: 0.20725\n",
            "Epoch: 01 [15804/20624 ( 77%)], Train Loss: 0.20679\n",
            "Epoch: 01 [15844/20624 ( 77%)], Train Loss: 0.20659\n",
            "Epoch: 01 [15884/20624 ( 77%)], Train Loss: 0.20627\n",
            "Epoch: 01 [15924/20624 ( 77%)], Train Loss: 0.20604\n",
            "Epoch: 01 [15964/20624 ( 77%)], Train Loss: 0.20583\n",
            "Epoch: 01 [16004/20624 ( 78%)], Train Loss: 0.20567\n",
            "Epoch: 01 [16044/20624 ( 78%)], Train Loss: 0.20556\n",
            "Epoch: 01 [16084/20624 ( 78%)], Train Loss: 0.20553\n",
            "Epoch: 01 [16124/20624 ( 78%)], Train Loss: 0.20519\n",
            "Epoch: 01 [16164/20624 ( 78%)], Train Loss: 0.20524\n",
            "Epoch: 01 [16204/20624 ( 79%)], Train Loss: 0.20529\n",
            "Epoch: 01 [16244/20624 ( 79%)], Train Loss: 0.20520\n",
            "Epoch: 01 [16284/20624 ( 79%)], Train Loss: 0.20501\n",
            "Epoch: 01 [16324/20624 ( 79%)], Train Loss: 0.20469\n",
            "Epoch: 01 [16364/20624 ( 79%)], Train Loss: 0.20439\n",
            "Epoch: 01 [16404/20624 ( 80%)], Train Loss: 0.20428\n",
            "Epoch: 01 [16444/20624 ( 80%)], Train Loss: 0.20419\n",
            "Epoch: 01 [16484/20624 ( 80%)], Train Loss: 0.20392\n",
            "Epoch: 01 [16524/20624 ( 80%)], Train Loss: 0.20355\n",
            "Epoch: 01 [16564/20624 ( 80%)], Train Loss: 0.20317\n",
            "Epoch: 01 [16604/20624 ( 81%)], Train Loss: 0.20302\n",
            "Epoch: 01 [16644/20624 ( 81%)], Train Loss: 0.20261\n",
            "Epoch: 01 [16684/20624 ( 81%)], Train Loss: 0.20276\n",
            "Epoch: 01 [16724/20624 ( 81%)], Train Loss: 0.20249\n",
            "Epoch: 01 [16764/20624 ( 81%)], Train Loss: 0.20220\n",
            "Epoch: 01 [16804/20624 ( 81%)], Train Loss: 0.20201\n",
            "Epoch: 01 [16844/20624 ( 82%)], Train Loss: 0.20174\n",
            "Epoch: 01 [16884/20624 ( 82%)], Train Loss: 0.20166\n",
            "Epoch: 01 [16924/20624 ( 82%)], Train Loss: 0.20158\n",
            "Epoch: 01 [16964/20624 ( 82%)], Train Loss: 0.20143\n",
            "Epoch: 01 [17004/20624 ( 82%)], Train Loss: 0.20141\n",
            "Epoch: 01 [17044/20624 ( 83%)], Train Loss: 0.20131\n",
            "Epoch: 01 [17084/20624 ( 83%)], Train Loss: 0.20138\n",
            "Epoch: 01 [17124/20624 ( 83%)], Train Loss: 0.20112\n",
            "Epoch: 01 [17164/20624 ( 83%)], Train Loss: 0.20123\n",
            "Epoch: 01 [17204/20624 ( 83%)], Train Loss: 0.20110\n",
            "Epoch: 01 [17244/20624 ( 84%)], Train Loss: 0.20118\n",
            "Epoch: 01 [17284/20624 ( 84%)], Train Loss: 0.20132\n",
            "Epoch: 01 [17324/20624 ( 84%)], Train Loss: 0.20106\n",
            "Epoch: 01 [17364/20624 ( 84%)], Train Loss: 0.20093\n",
            "Epoch: 01 [17404/20624 ( 84%)], Train Loss: 0.20074\n",
            "Epoch: 01 [17444/20624 ( 85%)], Train Loss: 0.20044\n",
            "Epoch: 01 [17484/20624 ( 85%)], Train Loss: 0.20069\n",
            "Epoch: 01 [17524/20624 ( 85%)], Train Loss: 0.20084\n",
            "Epoch: 01 [17564/20624 ( 85%)], Train Loss: 0.20058\n",
            "Epoch: 01 [17604/20624 ( 85%)], Train Loss: 0.20057\n",
            "Epoch: 01 [17644/20624 ( 86%)], Train Loss: 0.20032\n",
            "Epoch: 01 [17684/20624 ( 86%)], Train Loss: 0.20020\n",
            "Epoch: 01 [17724/20624 ( 86%)], Train Loss: 0.20036\n",
            "Epoch: 01 [17764/20624 ( 86%)], Train Loss: 0.20020\n",
            "Epoch: 01 [17804/20624 ( 86%)], Train Loss: 0.19992\n",
            "Epoch: 01 [17844/20624 ( 87%)], Train Loss: 0.19974\n",
            "Epoch: 01 [17884/20624 ( 87%)], Train Loss: 0.19965\n",
            "Epoch: 01 [17924/20624 ( 87%)], Train Loss: 0.19937\n",
            "Epoch: 01 [17964/20624 ( 87%)], Train Loss: 0.19932\n",
            "Epoch: 01 [18004/20624 ( 87%)], Train Loss: 0.19958\n",
            "Epoch: 01 [18044/20624 ( 87%)], Train Loss: 0.19937\n",
            "Epoch: 01 [18084/20624 ( 88%)], Train Loss: 0.19930\n",
            "Epoch: 01 [18124/20624 ( 88%)], Train Loss: 0.19927\n",
            "Epoch: 01 [18164/20624 ( 88%)], Train Loss: 0.19925\n",
            "Epoch: 01 [18204/20624 ( 88%)], Train Loss: 0.19907\n",
            "Epoch: 01 [18244/20624 ( 88%)], Train Loss: 0.19891\n",
            "Epoch: 01 [18284/20624 ( 89%)], Train Loss: 0.19887\n",
            "Epoch: 01 [18324/20624 ( 89%)], Train Loss: 0.19879\n",
            "Epoch: 01 [18364/20624 ( 89%)], Train Loss: 0.19864\n",
            "Epoch: 01 [18404/20624 ( 89%)], Train Loss: 0.19854\n",
            "Epoch: 01 [18444/20624 ( 89%)], Train Loss: 0.19837\n",
            "Epoch: 01 [18484/20624 ( 90%)], Train Loss: 0.19847\n",
            "Epoch: 01 [18524/20624 ( 90%)], Train Loss: 0.19875\n",
            "Epoch: 01 [18564/20624 ( 90%)], Train Loss: 0.19858\n",
            "Epoch: 01 [18604/20624 ( 90%)], Train Loss: 0.19920\n",
            "Epoch: 01 [18644/20624 ( 90%)], Train Loss: 0.19903\n",
            "Epoch: 01 [18684/20624 ( 91%)], Train Loss: 0.19889\n",
            "Epoch: 01 [18724/20624 ( 91%)], Train Loss: 0.19865\n",
            "Epoch: 01 [18764/20624 ( 91%)], Train Loss: 0.19855\n",
            "Epoch: 01 [18804/20624 ( 91%)], Train Loss: 0.19825\n",
            "Epoch: 01 [18844/20624 ( 91%)], Train Loss: 0.19806\n",
            "Epoch: 01 [18884/20624 ( 92%)], Train Loss: 0.19807\n",
            "Epoch: 01 [18924/20624 ( 92%)], Train Loss: 0.19818\n",
            "Epoch: 01 [18964/20624 ( 92%)], Train Loss: 0.19796\n",
            "Epoch: 01 [19004/20624 ( 92%)], Train Loss: 0.19797\n",
            "Epoch: 01 [19044/20624 ( 92%)], Train Loss: 0.19802\n",
            "Epoch: 01 [19084/20624 ( 93%)], Train Loss: 0.19795\n",
            "Epoch: 01 [19124/20624 ( 93%)], Train Loss: 0.19789\n",
            "Epoch: 01 [19164/20624 ( 93%)], Train Loss: 0.19770\n",
            "Epoch: 01 [19204/20624 ( 93%)], Train Loss: 0.19763\n",
            "Epoch: 01 [19244/20624 ( 93%)], Train Loss: 0.19749\n",
            "Epoch: 01 [19284/20624 ( 94%)], Train Loss: 0.19739\n",
            "Epoch: 01 [19324/20624 ( 94%)], Train Loss: 0.19726\n",
            "Epoch: 01 [19364/20624 ( 94%)], Train Loss: 0.19717\n",
            "Epoch: 01 [19404/20624 ( 94%)], Train Loss: 0.19728\n",
            "Epoch: 01 [19444/20624 ( 94%)], Train Loss: 0.19730\n",
            "Epoch: 01 [19484/20624 ( 94%)], Train Loss: 0.19739\n",
            "Epoch: 01 [19524/20624 ( 95%)], Train Loss: 0.19724\n",
            "Epoch: 01 [19564/20624 ( 95%)], Train Loss: 0.19722\n",
            "Epoch: 01 [19604/20624 ( 95%)], Train Loss: 0.19713\n",
            "Epoch: 01 [19644/20624 ( 95%)], Train Loss: 0.19698\n",
            "Epoch: 01 [19684/20624 ( 95%)], Train Loss: 0.19708\n",
            "Epoch: 01 [19724/20624 ( 96%)], Train Loss: 0.19677\n",
            "Epoch: 01 [19764/20624 ( 96%)], Train Loss: 0.19661\n",
            "Epoch: 01 [19804/20624 ( 96%)], Train Loss: 0.19646\n",
            "Epoch: 01 [19844/20624 ( 96%)], Train Loss: 0.19631\n",
            "Epoch: 01 [19884/20624 ( 96%)], Train Loss: 0.19613\n",
            "Epoch: 01 [19924/20624 ( 97%)], Train Loss: 0.19591\n",
            "Epoch: 01 [19964/20624 ( 97%)], Train Loss: 0.19575\n",
            "Epoch: 01 [20004/20624 ( 97%)], Train Loss: 0.19559\n",
            "Epoch: 01 [20044/20624 ( 97%)], Train Loss: 0.19556\n",
            "Epoch: 01 [20084/20624 ( 97%)], Train Loss: 0.19538\n",
            "Epoch: 01 [20124/20624 ( 98%)], Train Loss: 0.19530\n",
            "Epoch: 01 [20164/20624 ( 98%)], Train Loss: 0.19522\n",
            "Epoch: 01 [20204/20624 ( 98%)], Train Loss: 0.19502\n",
            "Epoch: 01 [20244/20624 ( 98%)], Train Loss: 0.19494\n",
            "Epoch: 01 [20284/20624 ( 98%)], Train Loss: 0.19470\n",
            "Epoch: 01 [20324/20624 ( 99%)], Train Loss: 0.19451\n",
            "Epoch: 01 [20364/20624 ( 99%)], Train Loss: 0.19450\n",
            "Epoch: 01 [20404/20624 ( 99%)], Train Loss: 0.19432\n",
            "Epoch: 01 [20444/20624 ( 99%)], Train Loss: 0.19435\n",
            "Epoch: 01 [20484/20624 ( 99%)], Train Loss: 0.19425\n",
            "Epoch: 01 [20524/20624 (100%)], Train Loss: 0.19416\n",
            "Epoch: 01 [20564/20624 (100%)], Train Loss: 0.19402\n",
            "Epoch: 01 [20604/20624 (100%)], Train Loss: 0.19380\n",
            "Epoch: 01 [20624/20624 (100%)], Train Loss: 0.19370\n",
            "----Validation Results Summary----\n",
            "Epoch: [1] Valid Loss: 0.30796\n",
            "\n",
            "Total Training Time: 6051.952735424042secs, Average Training Time per Epoch: 3025.976367712021secs.\n",
            "Total Validation Time: 264.43067836761475secs, Average Validation Time per Epoch: 132.21533918380737secs.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "FOLD: 1\n",
            "--------------------------------------------------\n",
            "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
            "Num examples Train= 20928, Num examples Valid=2668\n",
            "Total Training Steps: 5232, Total Warmup Steps: 523\n",
            "Epoch: 00 [    4/20928 (  0%)], Train Loss: 3.85423\n",
            "Epoch: 00 [   44/20928 (  0%)], Train Loss: 3.81510\n",
            "Epoch: 00 [   84/20928 (  0%)], Train Loss: 3.92590\n",
            "Epoch: 00 [  124/20928 (  1%)], Train Loss: 3.78072\n",
            "Epoch: 00 [  164/20928 (  1%)], Train Loss: 3.69857\n",
            "Epoch: 00 [  204/20928 (  1%)], Train Loss: 3.61630\n",
            "Epoch: 00 [  244/20928 (  1%)], Train Loss: 3.52402\n",
            "Epoch: 00 [  284/20928 (  1%)], Train Loss: 3.31533\n",
            "Epoch: 00 [  324/20928 (  2%)], Train Loss: 3.20067\n",
            "Epoch: 00 [  364/20928 (  2%)], Train Loss: 3.04414\n",
            "Epoch: 00 [  404/20928 (  2%)], Train Loss: 2.88063\n",
            "Epoch: 00 [  444/20928 (  2%)], Train Loss: 2.73292\n",
            "Epoch: 00 [  484/20928 (  2%)], Train Loss: 2.60630\n",
            "Epoch: 00 [  524/20928 (  3%)], Train Loss: 2.49117\n",
            "Epoch: 00 [  564/20928 (  3%)], Train Loss: 2.41945\n",
            "Epoch: 00 [  604/20928 (  3%)], Train Loss: 2.32777\n",
            "Epoch: 00 [  644/20928 (  3%)], Train Loss: 2.24269\n",
            "Epoch: 00 [  684/20928 (  3%)], Train Loss: 2.15909\n",
            "Epoch: 00 [  724/20928 (  3%)], Train Loss: 2.10127\n",
            "Epoch: 00 [  764/20928 (  4%)], Train Loss: 2.05106\n",
            "Epoch: 00 [  804/20928 (  4%)], Train Loss: 1.97387\n",
            "Epoch: 00 [  844/20928 (  4%)], Train Loss: 1.93120\n",
            "Epoch: 00 [  884/20928 (  4%)], Train Loss: 1.88608\n",
            "Epoch: 00 [  924/20928 (  4%)], Train Loss: 1.84647\n",
            "Epoch: 00 [  964/20928 (  5%)], Train Loss: 1.79465\n",
            "Epoch: 00 [ 1004/20928 (  5%)], Train Loss: 1.74567\n",
            "Epoch: 00 [ 1044/20928 (  5%)], Train Loss: 1.70532\n",
            "Epoch: 00 [ 1084/20928 (  5%)], Train Loss: 1.66151\n",
            "Epoch: 00 [ 1124/20928 (  5%)], Train Loss: 1.63895\n",
            "Epoch: 00 [ 1164/20928 (  6%)], Train Loss: 1.60295\n",
            "Epoch: 00 [ 1204/20928 (  6%)], Train Loss: 1.57477\n",
            "Epoch: 00 [ 1244/20928 (  6%)], Train Loss: 1.54930\n",
            "Epoch: 00 [ 1284/20928 (  6%)], Train Loss: 1.51714\n",
            "Epoch: 00 [ 1324/20928 (  6%)], Train Loss: 1.50457\n",
            "Epoch: 00 [ 1364/20928 (  7%)], Train Loss: 1.47617\n",
            "Epoch: 00 [ 1404/20928 (  7%)], Train Loss: 1.44842\n",
            "Epoch: 00 [ 1444/20928 (  7%)], Train Loss: 1.41954\n",
            "Epoch: 00 [ 1484/20928 (  7%)], Train Loss: 1.40093\n",
            "Epoch: 00 [ 1524/20928 (  7%)], Train Loss: 1.38442\n",
            "Epoch: 00 [ 1564/20928 (  7%)], Train Loss: 1.36318\n",
            "Epoch: 00 [ 1604/20928 (  8%)], Train Loss: 1.34794\n",
            "Epoch: 00 [ 1644/20928 (  8%)], Train Loss: 1.33334\n",
            "Epoch: 00 [ 1684/20928 (  8%)], Train Loss: 1.31699\n",
            "Epoch: 00 [ 1724/20928 (  8%)], Train Loss: 1.29883\n",
            "Epoch: 00 [ 1764/20928 (  8%)], Train Loss: 1.28509\n",
            "Epoch: 00 [ 1804/20928 (  9%)], Train Loss: 1.26346\n",
            "Epoch: 00 [ 1844/20928 (  9%)], Train Loss: 1.25312\n",
            "Epoch: 00 [ 1884/20928 (  9%)], Train Loss: 1.24290\n",
            "Epoch: 00 [ 1924/20928 (  9%)], Train Loss: 1.22921\n",
            "Epoch: 00 [ 1964/20928 (  9%)], Train Loss: 1.21490\n",
            "Epoch: 00 [ 2004/20928 ( 10%)], Train Loss: 1.19785\n",
            "Epoch: 00 [ 2044/20928 ( 10%)], Train Loss: 1.18230\n",
            "Epoch: 00 [ 2084/20928 ( 10%)], Train Loss: 1.17089\n",
            "Epoch: 00 [ 2124/20928 ( 10%)], Train Loss: 1.15803\n",
            "Epoch: 00 [ 2164/20928 ( 10%)], Train Loss: 1.15153\n",
            "Epoch: 00 [ 2204/20928 ( 11%)], Train Loss: 1.13851\n",
            "Epoch: 00 [ 2244/20928 ( 11%)], Train Loss: 1.12864\n",
            "Epoch: 00 [ 2284/20928 ( 11%)], Train Loss: 1.12164\n",
            "Epoch: 00 [ 2324/20928 ( 11%)], Train Loss: 1.11385\n",
            "Epoch: 00 [ 2364/20928 ( 11%)], Train Loss: 1.10526\n",
            "Epoch: 00 [ 2404/20928 ( 11%)], Train Loss: 1.09941\n",
            "Epoch: 00 [ 2444/20928 ( 12%)], Train Loss: 1.09192\n",
            "Epoch: 00 [ 2484/20928 ( 12%)], Train Loss: 1.08230\n",
            "Epoch: 00 [ 2524/20928 ( 12%)], Train Loss: 1.07340\n",
            "Epoch: 00 [ 2564/20928 ( 12%)], Train Loss: 1.06760\n",
            "Epoch: 00 [ 2604/20928 ( 12%)], Train Loss: 1.05816\n",
            "Epoch: 00 [ 2644/20928 ( 13%)], Train Loss: 1.04734\n",
            "Epoch: 00 [ 2684/20928 ( 13%)], Train Loss: 1.03628\n",
            "Epoch: 00 [ 2724/20928 ( 13%)], Train Loss: 1.03015\n",
            "Epoch: 00 [ 2764/20928 ( 13%)], Train Loss: 1.02771\n",
            "Epoch: 00 [ 2804/20928 ( 13%)], Train Loss: 1.01998\n",
            "Epoch: 00 [ 2844/20928 ( 14%)], Train Loss: 1.01578\n",
            "Epoch: 00 [ 2884/20928 ( 14%)], Train Loss: 1.01056\n",
            "Epoch: 00 [ 2924/20928 ( 14%)], Train Loss: 1.00415\n",
            "Epoch: 00 [ 2964/20928 ( 14%)], Train Loss: 0.99532\n",
            "Epoch: 00 [ 3004/20928 ( 14%)], Train Loss: 0.98604\n",
            "Epoch: 00 [ 3044/20928 ( 15%)], Train Loss: 0.97943\n",
            "Epoch: 00 [ 3084/20928 ( 15%)], Train Loss: 0.97479\n",
            "Epoch: 00 [ 3124/20928 ( 15%)], Train Loss: 0.97182\n",
            "Epoch: 00 [ 3164/20928 ( 15%)], Train Loss: 0.96805\n",
            "Epoch: 00 [ 3204/20928 ( 15%)], Train Loss: 0.96426\n",
            "Epoch: 00 [ 3244/20928 ( 16%)], Train Loss: 0.96057\n",
            "Epoch: 00 [ 3284/20928 ( 16%)], Train Loss: 0.95519\n",
            "Epoch: 00 [ 3324/20928 ( 16%)], Train Loss: 0.95162\n",
            "Epoch: 00 [ 3364/20928 ( 16%)], Train Loss: 0.95215\n",
            "Epoch: 00 [ 3404/20928 ( 16%)], Train Loss: 0.94635\n",
            "Epoch: 00 [ 3444/20928 ( 16%)], Train Loss: 0.94211\n",
            "Epoch: 00 [ 3484/20928 ( 17%)], Train Loss: 0.93482\n",
            "Epoch: 00 [ 3524/20928 ( 17%)], Train Loss: 0.93216\n",
            "Epoch: 00 [ 3564/20928 ( 17%)], Train Loss: 0.92463\n",
            "Epoch: 00 [ 3604/20928 ( 17%)], Train Loss: 0.92018\n",
            "Epoch: 00 [ 3644/20928 ( 17%)], Train Loss: 0.91852\n",
            "Epoch: 00 [ 3684/20928 ( 18%)], Train Loss: 0.91516\n",
            "Epoch: 00 [ 3724/20928 ( 18%)], Train Loss: 0.91126\n",
            "Epoch: 00 [ 3764/20928 ( 18%)], Train Loss: 0.90511\n",
            "Epoch: 00 [ 3804/20928 ( 18%)], Train Loss: 0.90172\n",
            "Epoch: 00 [ 3844/20928 ( 18%)], Train Loss: 0.89879\n",
            "Epoch: 00 [ 3884/20928 ( 19%)], Train Loss: 0.89646\n",
            "Epoch: 00 [ 3924/20928 ( 19%)], Train Loss: 0.89375\n",
            "Epoch: 00 [ 3964/20928 ( 19%)], Train Loss: 0.88771\n",
            "Epoch: 00 [ 4004/20928 ( 19%)], Train Loss: 0.88487\n",
            "Epoch: 00 [ 4044/20928 ( 19%)], Train Loss: 0.87937\n",
            "Epoch: 00 [ 4084/20928 ( 20%)], Train Loss: 0.87595\n",
            "Epoch: 00 [ 4124/20928 ( 20%)], Train Loss: 0.87111\n",
            "Epoch: 00 [ 4164/20928 ( 20%)], Train Loss: 0.86794\n",
            "Epoch: 00 [ 4204/20928 ( 20%)], Train Loss: 0.86556\n",
            "Epoch: 00 [ 4244/20928 ( 20%)], Train Loss: 0.86349\n",
            "Epoch: 00 [ 4284/20928 ( 20%)], Train Loss: 0.85921\n",
            "Epoch: 00 [ 4324/20928 ( 21%)], Train Loss: 0.85817\n",
            "Epoch: 00 [ 4364/20928 ( 21%)], Train Loss: 0.85688\n",
            "Epoch: 00 [ 4404/20928 ( 21%)], Train Loss: 0.85521\n",
            "Epoch: 00 [ 4444/20928 ( 21%)], Train Loss: 0.85220\n",
            "Epoch: 00 [ 4484/20928 ( 21%)], Train Loss: 0.85051\n",
            "Epoch: 00 [ 4524/20928 ( 22%)], Train Loss: 0.85027\n",
            "Epoch: 00 [ 4564/20928 ( 22%)], Train Loss: 0.84741\n",
            "Epoch: 00 [ 4604/20928 ( 22%)], Train Loss: 0.84428\n",
            "Epoch: 00 [ 4644/20928 ( 22%)], Train Loss: 0.84116\n",
            "Epoch: 00 [ 4684/20928 ( 22%)], Train Loss: 0.83621\n",
            "Epoch: 00 [ 4724/20928 ( 23%)], Train Loss: 0.83368\n",
            "Epoch: 00 [ 4764/20928 ( 23%)], Train Loss: 0.83011\n",
            "Epoch: 00 [ 4804/20928 ( 23%)], Train Loss: 0.82966\n",
            "Epoch: 00 [ 4844/20928 ( 23%)], Train Loss: 0.82605\n",
            "Epoch: 00 [ 4884/20928 ( 23%)], Train Loss: 0.82335\n",
            "Epoch: 00 [ 4924/20928 ( 24%)], Train Loss: 0.82228\n",
            "Epoch: 00 [ 4964/20928 ( 24%)], Train Loss: 0.82067\n",
            "Epoch: 00 [ 5004/20928 ( 24%)], Train Loss: 0.81837\n",
            "Epoch: 00 [ 5044/20928 ( 24%)], Train Loss: 0.81594\n",
            "Epoch: 00 [ 5084/20928 ( 24%)], Train Loss: 0.81397\n",
            "Epoch: 00 [ 5124/20928 ( 24%)], Train Loss: 0.81176\n",
            "Epoch: 00 [ 5164/20928 ( 25%)], Train Loss: 0.80987\n",
            "Epoch: 00 [ 5204/20928 ( 25%)], Train Loss: 0.80985\n",
            "Epoch: 00 [ 5244/20928 ( 25%)], Train Loss: 0.81091\n",
            "Epoch: 00 [ 5284/20928 ( 25%)], Train Loss: 0.80900\n",
            "Epoch: 00 [ 5324/20928 ( 25%)], Train Loss: 0.80673\n",
            "Epoch: 00 [ 5364/20928 ( 26%)], Train Loss: 0.80336\n",
            "Epoch: 00 [ 5404/20928 ( 26%)], Train Loss: 0.80487\n",
            "Epoch: 00 [ 5444/20928 ( 26%)], Train Loss: 0.80210\n",
            "Epoch: 00 [ 5484/20928 ( 26%)], Train Loss: 0.80026\n",
            "Epoch: 00 [ 5524/20928 ( 26%)], Train Loss: 0.79861\n",
            "Epoch: 00 [ 5564/20928 ( 27%)], Train Loss: 0.79629\n",
            "Epoch: 00 [ 5604/20928 ( 27%)], Train Loss: 0.79608\n",
            "Epoch: 00 [ 5644/20928 ( 27%)], Train Loss: 0.79555\n",
            "Epoch: 00 [ 5684/20928 ( 27%)], Train Loss: 0.79365\n",
            "Epoch: 00 [ 5724/20928 ( 27%)], Train Loss: 0.79215\n",
            "Epoch: 00 [ 5764/20928 ( 28%)], Train Loss: 0.79051\n",
            "Epoch: 00 [ 5804/20928 ( 28%)], Train Loss: 0.78803\n",
            "Epoch: 00 [ 5844/20928 ( 28%)], Train Loss: 0.78527\n",
            "Epoch: 00 [ 5884/20928 ( 28%)], Train Loss: 0.78269\n",
            "Epoch: 00 [ 5924/20928 ( 28%)], Train Loss: 0.78021\n",
            "Epoch: 00 [ 5964/20928 ( 28%)], Train Loss: 0.77823\n",
            "Epoch: 00 [ 6004/20928 ( 29%)], Train Loss: 0.77613\n",
            "Epoch: 00 [ 6044/20928 ( 29%)], Train Loss: 0.77519\n",
            "Epoch: 00 [ 6084/20928 ( 29%)], Train Loss: 0.77260\n",
            "Epoch: 00 [ 6124/20928 ( 29%)], Train Loss: 0.77099\n",
            "Epoch: 00 [ 6164/20928 ( 29%)], Train Loss: 0.76723\n",
            "Epoch: 00 [ 6204/20928 ( 30%)], Train Loss: 0.76617\n",
            "Epoch: 00 [ 6244/20928 ( 30%)], Train Loss: 0.76470\n",
            "Epoch: 00 [ 6284/20928 ( 30%)], Train Loss: 0.76131\n",
            "Epoch: 00 [ 6324/20928 ( 30%)], Train Loss: 0.75829\n",
            "Epoch: 00 [ 6364/20928 ( 30%)], Train Loss: 0.75661\n",
            "Epoch: 00 [ 6404/20928 ( 31%)], Train Loss: 0.75453\n",
            "Epoch: 00 [ 6444/20928 ( 31%)], Train Loss: 0.75343\n",
            "Epoch: 00 [ 6484/20928 ( 31%)], Train Loss: 0.75089\n",
            "Epoch: 00 [ 6524/20928 ( 31%)], Train Loss: 0.74958\n",
            "Epoch: 00 [ 6564/20928 ( 31%)], Train Loss: 0.74902\n",
            "Epoch: 00 [ 6604/20928 ( 32%)], Train Loss: 0.74751\n",
            "Epoch: 00 [ 6644/20928 ( 32%)], Train Loss: 0.74669\n",
            "Epoch: 00 [ 6684/20928 ( 32%)], Train Loss: 0.74558\n",
            "Epoch: 00 [ 6724/20928 ( 32%)], Train Loss: 0.74331\n",
            "Epoch: 00 [ 6764/20928 ( 32%)], Train Loss: 0.74066\n",
            "Epoch: 00 [ 6804/20928 ( 33%)], Train Loss: 0.73823\n",
            "Epoch: 00 [ 6844/20928 ( 33%)], Train Loss: 0.73523\n",
            "Epoch: 00 [ 6884/20928 ( 33%)], Train Loss: 0.73390\n",
            "Epoch: 00 [ 6924/20928 ( 33%)], Train Loss: 0.73189\n",
            "Epoch: 00 [ 6964/20928 ( 33%)], Train Loss: 0.73166\n",
            "Epoch: 00 [ 7004/20928 ( 33%)], Train Loss: 0.73147\n",
            "Epoch: 00 [ 7044/20928 ( 34%)], Train Loss: 0.72901\n",
            "Epoch: 00 [ 7084/20928 ( 34%)], Train Loss: 0.72816\n",
            "Epoch: 00 [ 7124/20928 ( 34%)], Train Loss: 0.72699\n",
            "Epoch: 00 [ 7164/20928 ( 34%)], Train Loss: 0.72621\n",
            "Epoch: 00 [ 7204/20928 ( 34%)], Train Loss: 0.72519\n",
            "Epoch: 00 [ 7244/20928 ( 35%)], Train Loss: 0.72456\n",
            "Epoch: 00 [ 7284/20928 ( 35%)], Train Loss: 0.72325\n",
            "Epoch: 00 [ 7324/20928 ( 35%)], Train Loss: 0.72059\n",
            "Epoch: 00 [ 7364/20928 ( 35%)], Train Loss: 0.71953\n",
            "Epoch: 00 [ 7404/20928 ( 35%)], Train Loss: 0.71706\n",
            "Epoch: 00 [ 7444/20928 ( 36%)], Train Loss: 0.71897\n",
            "Epoch: 00 [ 7484/20928 ( 36%)], Train Loss: 0.71759\n",
            "Epoch: 00 [ 7524/20928 ( 36%)], Train Loss: 0.71564\n",
            "Epoch: 00 [ 7564/20928 ( 36%)], Train Loss: 0.71557\n",
            "Epoch: 00 [ 7604/20928 ( 36%)], Train Loss: 0.71437\n",
            "Epoch: 00 [ 7644/20928 ( 37%)], Train Loss: 0.71306\n",
            "Epoch: 00 [ 7684/20928 ( 37%)], Train Loss: 0.71284\n",
            "Epoch: 00 [ 7724/20928 ( 37%)], Train Loss: 0.71058\n",
            "Epoch: 00 [ 7764/20928 ( 37%)], Train Loss: 0.70945\n",
            "Epoch: 00 [ 7804/20928 ( 37%)], Train Loss: 0.70818\n",
            "Epoch: 00 [ 7844/20928 ( 37%)], Train Loss: 0.70758\n",
            "Epoch: 00 [ 7884/20928 ( 38%)], Train Loss: 0.70694\n",
            "Epoch: 00 [ 7924/20928 ( 38%)], Train Loss: 0.70518\n",
            "Epoch: 00 [ 7964/20928 ( 38%)], Train Loss: 0.70295\n",
            "Epoch: 00 [ 8004/20928 ( 38%)], Train Loss: 0.70083\n",
            "Epoch: 00 [ 8044/20928 ( 38%)], Train Loss: 0.69951\n",
            "Epoch: 00 [ 8084/20928 ( 39%)], Train Loss: 0.69789\n",
            "Epoch: 00 [ 8124/20928 ( 39%)], Train Loss: 0.69722\n",
            "Epoch: 00 [ 8164/20928 ( 39%)], Train Loss: 0.69687\n",
            "Epoch: 00 [ 8204/20928 ( 39%)], Train Loss: 0.69593\n",
            "Epoch: 00 [ 8244/20928 ( 39%)], Train Loss: 0.69577\n",
            "Epoch: 00 [ 8284/20928 ( 40%)], Train Loss: 0.69588\n",
            "Epoch: 00 [ 8324/20928 ( 40%)], Train Loss: 0.69474\n",
            "Epoch: 00 [ 8364/20928 ( 40%)], Train Loss: 0.69424\n",
            "Epoch: 00 [ 8404/20928 ( 40%)], Train Loss: 0.69271\n",
            "Epoch: 00 [ 8444/20928 ( 40%)], Train Loss: 0.69156\n",
            "Epoch: 00 [ 8484/20928 ( 41%)], Train Loss: 0.69023\n",
            "Epoch: 00 [ 8524/20928 ( 41%)], Train Loss: 0.68855\n",
            "Epoch: 00 [ 8564/20928 ( 41%)], Train Loss: 0.68711\n",
            "Epoch: 00 [ 8604/20928 ( 41%)], Train Loss: 0.68607\n",
            "Epoch: 00 [ 8644/20928 ( 41%)], Train Loss: 0.68602\n",
            "Epoch: 00 [ 8684/20928 ( 41%)], Train Loss: 0.68617\n",
            "Epoch: 00 [ 8724/20928 ( 42%)], Train Loss: 0.68576\n",
            "Epoch: 00 [ 8764/20928 ( 42%)], Train Loss: 0.68471\n",
            "Epoch: 00 [ 8804/20928 ( 42%)], Train Loss: 0.68304\n",
            "Epoch: 00 [ 8844/20928 ( 42%)], Train Loss: 0.68174\n",
            "Epoch: 00 [ 8884/20928 ( 42%)], Train Loss: 0.68101\n",
            "Epoch: 00 [ 8924/20928 ( 43%)], Train Loss: 0.68088\n",
            "Epoch: 00 [ 8964/20928 ( 43%)], Train Loss: 0.68088\n",
            "Epoch: 00 [ 9004/20928 ( 43%)], Train Loss: 0.68036\n",
            "Epoch: 00 [ 9044/20928 ( 43%)], Train Loss: 0.68018\n",
            "Epoch: 00 [ 9084/20928 ( 43%)], Train Loss: 0.67898\n",
            "Epoch: 00 [ 9124/20928 ( 44%)], Train Loss: 0.67774\n",
            "Epoch: 00 [ 9164/20928 ( 44%)], Train Loss: 0.67741\n",
            "Epoch: 00 [ 9204/20928 ( 44%)], Train Loss: 0.67696\n",
            "Epoch: 00 [ 9244/20928 ( 44%)], Train Loss: 0.67533\n",
            "Epoch: 00 [ 9284/20928 ( 44%)], Train Loss: 0.67388\n",
            "Epoch: 00 [ 9324/20928 ( 45%)], Train Loss: 0.67370\n",
            "Epoch: 00 [ 9364/20928 ( 45%)], Train Loss: 0.67292\n",
            "Epoch: 00 [ 9404/20928 ( 45%)], Train Loss: 0.67224\n",
            "Epoch: 00 [ 9444/20928 ( 45%)], Train Loss: 0.67229\n",
            "Epoch: 00 [ 9484/20928 ( 45%)], Train Loss: 0.67209\n",
            "Epoch: 00 [ 9524/20928 ( 46%)], Train Loss: 0.67121\n",
            "Epoch: 00 [ 9564/20928 ( 46%)], Train Loss: 0.67025\n",
            "Epoch: 00 [ 9604/20928 ( 46%)], Train Loss: 0.67001\n",
            "Epoch: 00 [ 9644/20928 ( 46%)], Train Loss: 0.67014\n",
            "Epoch: 00 [ 9684/20928 ( 46%)], Train Loss: 0.66904\n",
            "Epoch: 00 [ 9724/20928 ( 46%)], Train Loss: 0.66978\n",
            "Epoch: 00 [ 9764/20928 ( 47%)], Train Loss: 0.66970\n",
            "Epoch: 00 [ 9804/20928 ( 47%)], Train Loss: 0.66990\n",
            "Epoch: 00 [ 9844/20928 ( 47%)], Train Loss: 0.66924\n",
            "Epoch: 00 [ 9884/20928 ( 47%)], Train Loss: 0.66938\n",
            "Epoch: 00 [ 9924/20928 ( 47%)], Train Loss: 0.66868\n",
            "Epoch: 00 [ 9964/20928 ( 48%)], Train Loss: 0.66812\n",
            "Epoch: 00 [10004/20928 ( 48%)], Train Loss: 0.66942\n",
            "Epoch: 00 [10044/20928 ( 48%)], Train Loss: 0.66936\n",
            "Epoch: 00 [10084/20928 ( 48%)], Train Loss: 0.66810\n",
            "Epoch: 00 [10124/20928 ( 48%)], Train Loss: 0.66747\n",
            "Epoch: 00 [10164/20928 ( 49%)], Train Loss: 0.66809\n",
            "Epoch: 00 [10204/20928 ( 49%)], Train Loss: 0.66702\n",
            "Epoch: 00 [10244/20928 ( 49%)], Train Loss: 0.66688\n",
            "Epoch: 00 [10284/20928 ( 49%)], Train Loss: 0.66675\n",
            "Epoch: 00 [10324/20928 ( 49%)], Train Loss: 0.66568\n",
            "Epoch: 00 [10364/20928 ( 50%)], Train Loss: 0.66504\n",
            "Epoch: 00 [10404/20928 ( 50%)], Train Loss: 0.66499\n",
            "Epoch: 00 [10444/20928 ( 50%)], Train Loss: 0.66540\n",
            "Epoch: 00 [10484/20928 ( 50%)], Train Loss: 0.66466\n",
            "Epoch: 00 [10524/20928 ( 50%)], Train Loss: 0.66406\n",
            "Epoch: 00 [10564/20928 ( 50%)], Train Loss: 0.66255\n",
            "Epoch: 00 [10604/20928 ( 51%)], Train Loss: 0.66122\n",
            "Epoch: 00 [10644/20928 ( 51%)], Train Loss: 0.66130\n",
            "Epoch: 00 [10684/20928 ( 51%)], Train Loss: 0.66013\n",
            "Epoch: 00 [10724/20928 ( 51%)], Train Loss: 0.65952\n",
            "Epoch: 00 [10764/20928 ( 51%)], Train Loss: 0.65819\n",
            "Epoch: 00 [10804/20928 ( 52%)], Train Loss: 0.65761\n",
            "Epoch: 00 [10844/20928 ( 52%)], Train Loss: 0.65717\n",
            "Epoch: 00 [10884/20928 ( 52%)], Train Loss: 0.65636\n",
            "Epoch: 00 [10924/20928 ( 52%)], Train Loss: 0.65580\n",
            "Epoch: 00 [10964/20928 ( 52%)], Train Loss: 0.65472\n",
            "Epoch: 00 [11004/20928 ( 53%)], Train Loss: 0.65421\n",
            "Epoch: 00 [11044/20928 ( 53%)], Train Loss: 0.65377\n",
            "Epoch: 00 [11084/20928 ( 53%)], Train Loss: 0.65307\n",
            "Epoch: 00 [11124/20928 ( 53%)], Train Loss: 0.65275\n",
            "Epoch: 00 [11164/20928 ( 53%)], Train Loss: 0.65209\n",
            "Epoch: 00 [11204/20928 ( 54%)], Train Loss: 0.65166\n",
            "Epoch: 00 [11244/20928 ( 54%)], Train Loss: 0.65124\n",
            "Epoch: 00 [11284/20928 ( 54%)], Train Loss: 0.65033\n",
            "Epoch: 00 [11324/20928 ( 54%)], Train Loss: 0.65024\n",
            "Epoch: 00 [11364/20928 ( 54%)], Train Loss: 0.64952\n",
            "Epoch: 00 [11404/20928 ( 54%)], Train Loss: 0.64891\n",
            "Epoch: 00 [11444/20928 ( 55%)], Train Loss: 0.64736\n",
            "Epoch: 00 [11484/20928 ( 55%)], Train Loss: 0.64616\n",
            "Epoch: 00 [11524/20928 ( 55%)], Train Loss: 0.64585\n",
            "Epoch: 00 [11564/20928 ( 55%)], Train Loss: 0.64547\n",
            "Epoch: 00 [11604/20928 ( 55%)], Train Loss: 0.64512\n",
            "Epoch: 00 [11644/20928 ( 56%)], Train Loss: 0.64494\n",
            "Epoch: 00 [11684/20928 ( 56%)], Train Loss: 0.64437\n",
            "Epoch: 00 [11724/20928 ( 56%)], Train Loss: 0.64379\n",
            "Epoch: 00 [11764/20928 ( 56%)], Train Loss: 0.64287\n",
            "Epoch: 00 [11804/20928 ( 56%)], Train Loss: 0.64175\n",
            "Epoch: 00 [11844/20928 ( 57%)], Train Loss: 0.64133\n",
            "Epoch: 00 [11884/20928 ( 57%)], Train Loss: 0.64134\n",
            "Epoch: 00 [11924/20928 ( 57%)], Train Loss: 0.64086\n",
            "Epoch: 00 [11964/20928 ( 57%)], Train Loss: 0.63978\n",
            "Epoch: 00 [12004/20928 ( 57%)], Train Loss: 0.64020\n",
            "Epoch: 00 [12044/20928 ( 58%)], Train Loss: 0.63928\n",
            "Epoch: 00 [12084/20928 ( 58%)], Train Loss: 0.63859\n",
            "Epoch: 00 [12124/20928 ( 58%)], Train Loss: 0.63751\n",
            "Epoch: 00 [12164/20928 ( 58%)], Train Loss: 0.63650\n",
            "Epoch: 00 [12204/20928 ( 58%)], Train Loss: 0.63542\n",
            "Epoch: 00 [12244/20928 ( 59%)], Train Loss: 0.63458\n",
            "Epoch: 00 [12284/20928 ( 59%)], Train Loss: 0.63417\n",
            "Epoch: 00 [12324/20928 ( 59%)], Train Loss: 0.63431\n",
            "Epoch: 00 [12364/20928 ( 59%)], Train Loss: 0.63395\n",
            "Epoch: 00 [12404/20928 ( 59%)], Train Loss: 0.63364\n",
            "Epoch: 00 [12444/20928 ( 59%)], Train Loss: 0.63237\n",
            "Epoch: 00 [12484/20928 ( 60%)], Train Loss: 0.63132\n",
            "Epoch: 00 [12524/20928 ( 60%)], Train Loss: 0.63080\n",
            "Epoch: 00 [12564/20928 ( 60%)], Train Loss: 0.63041\n",
            "Epoch: 00 [12604/20928 ( 60%)], Train Loss: 0.63020\n",
            "Epoch: 00 [12644/20928 ( 60%)], Train Loss: 0.62959\n",
            "Epoch: 00 [12684/20928 ( 61%)], Train Loss: 0.62865\n",
            "Epoch: 00 [12724/20928 ( 61%)], Train Loss: 0.62914\n",
            "Epoch: 00 [12764/20928 ( 61%)], Train Loss: 0.62832\n",
            "Epoch: 00 [12804/20928 ( 61%)], Train Loss: 0.62861\n",
            "Epoch: 00 [12844/20928 ( 61%)], Train Loss: 0.62826\n",
            "Epoch: 00 [12884/20928 ( 62%)], Train Loss: 0.62836\n",
            "Epoch: 00 [12924/20928 ( 62%)], Train Loss: 0.62806\n",
            "Epoch: 00 [12964/20928 ( 62%)], Train Loss: 0.62733\n",
            "Epoch: 00 [13004/20928 ( 62%)], Train Loss: 0.62698\n",
            "Epoch: 00 [13044/20928 ( 62%)], Train Loss: 0.62632\n",
            "Epoch: 00 [13084/20928 ( 63%)], Train Loss: 0.62618\n",
            "Epoch: 00 [13124/20928 ( 63%)], Train Loss: 0.62578\n",
            "Epoch: 00 [13164/20928 ( 63%)], Train Loss: 0.62510\n",
            "Epoch: 00 [13204/20928 ( 63%)], Train Loss: 0.62427\n",
            "Epoch: 00 [13244/20928 ( 63%)], Train Loss: 0.62323\n",
            "Epoch: 00 [13284/20928 ( 63%)], Train Loss: 0.62272\n",
            "Epoch: 00 [13324/20928 ( 64%)], Train Loss: 0.62207\n",
            "Epoch: 00 [13364/20928 ( 64%)], Train Loss: 0.62120\n",
            "Epoch: 00 [13404/20928 ( 64%)], Train Loss: 0.62078\n",
            "Epoch: 00 [13444/20928 ( 64%)], Train Loss: 0.62039\n",
            "Epoch: 00 [13484/20928 ( 64%)], Train Loss: 0.62016\n",
            "Epoch: 00 [13524/20928 ( 65%)], Train Loss: 0.61924\n",
            "Epoch: 00 [13564/20928 ( 65%)], Train Loss: 0.61820\n",
            "Epoch: 00 [13604/20928 ( 65%)], Train Loss: 0.61784\n",
            "Epoch: 00 [13644/20928 ( 65%)], Train Loss: 0.61773\n",
            "Epoch: 00 [13684/20928 ( 65%)], Train Loss: 0.61718\n",
            "Epoch: 00 [13724/20928 ( 66%)], Train Loss: 0.61635\n",
            "Epoch: 00 [13764/20928 ( 66%)], Train Loss: 0.61591\n",
            "Epoch: 00 [13804/20928 ( 66%)], Train Loss: 0.61564\n",
            "Epoch: 00 [13844/20928 ( 66%)], Train Loss: 0.61497\n",
            "Epoch: 00 [13884/20928 ( 66%)], Train Loss: 0.61399\n",
            "Epoch: 00 [13924/20928 ( 67%)], Train Loss: 0.61378\n",
            "Epoch: 00 [13964/20928 ( 67%)], Train Loss: 0.61289\n",
            "Epoch: 00 [14004/20928 ( 67%)], Train Loss: 0.61231\n",
            "Epoch: 00 [14044/20928 ( 67%)], Train Loss: 0.61141\n",
            "Epoch: 00 [14084/20928 ( 67%)], Train Loss: 0.61117\n",
            "Epoch: 00 [14124/20928 ( 67%)], Train Loss: 0.61064\n",
            "Epoch: 00 [14164/20928 ( 68%)], Train Loss: 0.60991\n",
            "Epoch: 00 [14204/20928 ( 68%)], Train Loss: 0.60900\n",
            "Epoch: 00 [14244/20928 ( 68%)], Train Loss: 0.60803\n",
            "Epoch: 00 [14284/20928 ( 68%)], Train Loss: 0.60751\n",
            "Epoch: 00 [14324/20928 ( 68%)], Train Loss: 0.60756\n",
            "Epoch: 00 [14364/20928 ( 69%)], Train Loss: 0.60713\n",
            "Epoch: 00 [14404/20928 ( 69%)], Train Loss: 0.60598\n",
            "Epoch: 00 [14444/20928 ( 69%)], Train Loss: 0.60495\n",
            "Epoch: 00 [14484/20928 ( 69%)], Train Loss: 0.60463\n",
            "Epoch: 00 [14524/20928 ( 69%)], Train Loss: 0.60483\n",
            "Epoch: 00 [14564/20928 ( 70%)], Train Loss: 0.60388\n",
            "Epoch: 00 [14604/20928 ( 70%)], Train Loss: 0.60338\n",
            "Epoch: 00 [14644/20928 ( 70%)], Train Loss: 0.60277\n",
            "Epoch: 00 [14684/20928 ( 70%)], Train Loss: 0.60224\n",
            "Epoch: 00 [14724/20928 ( 70%)], Train Loss: 0.60148\n",
            "Epoch: 00 [14764/20928 ( 71%)], Train Loss: 0.60116\n",
            "Epoch: 00 [14804/20928 ( 71%)], Train Loss: 0.60136\n",
            "Epoch: 00 [14844/20928 ( 71%)], Train Loss: 0.60061\n",
            "Epoch: 00 [14884/20928 ( 71%)], Train Loss: 0.60066\n",
            "Epoch: 00 [14924/20928 ( 71%)], Train Loss: 0.60017\n",
            "Epoch: 00 [14964/20928 ( 72%)], Train Loss: 0.60020\n",
            "Epoch: 00 [15004/20928 ( 72%)], Train Loss: 0.59991\n",
            "Epoch: 00 [15044/20928 ( 72%)], Train Loss: 0.59915\n",
            "Epoch: 00 [15084/20928 ( 72%)], Train Loss: 0.59900\n",
            "Epoch: 00 [15124/20928 ( 72%)], Train Loss: 0.59791\n",
            "Epoch: 00 [15164/20928 ( 72%)], Train Loss: 0.59744\n",
            "Epoch: 00 [15204/20928 ( 73%)], Train Loss: 0.59729\n",
            "Epoch: 00 [15244/20928 ( 73%)], Train Loss: 0.59672\n",
            "Epoch: 00 [15284/20928 ( 73%)], Train Loss: 0.59647\n",
            "Epoch: 00 [15324/20928 ( 73%)], Train Loss: 0.59607\n",
            "Epoch: 00 [15364/20928 ( 73%)], Train Loss: 0.59507\n",
            "Epoch: 00 [15404/20928 ( 74%)], Train Loss: 0.59464\n",
            "Epoch: 00 [15444/20928 ( 74%)], Train Loss: 0.59478\n",
            "Epoch: 00 [15484/20928 ( 74%)], Train Loss: 0.59460\n",
            "Epoch: 00 [15524/20928 ( 74%)], Train Loss: 0.59465\n",
            "Epoch: 00 [15564/20928 ( 74%)], Train Loss: 0.59399\n",
            "Epoch: 00 [15604/20928 ( 75%)], Train Loss: 0.59331\n",
            "Epoch: 00 [15644/20928 ( 75%)], Train Loss: 0.59292\n",
            "Epoch: 00 [15684/20928 ( 75%)], Train Loss: 0.59255\n",
            "Epoch: 00 [15724/20928 ( 75%)], Train Loss: 0.59188\n",
            "Epoch: 00 [15764/20928 ( 75%)], Train Loss: 0.59094\n",
            "Epoch: 00 [15804/20928 ( 76%)], Train Loss: 0.59034\n",
            "Epoch: 00 [15844/20928 ( 76%)], Train Loss: 0.59032\n",
            "Epoch: 00 [15884/20928 ( 76%)], Train Loss: 0.59016\n",
            "Epoch: 00 [15924/20928 ( 76%)], Train Loss: 0.58975\n",
            "Epoch: 00 [15964/20928 ( 76%)], Train Loss: 0.58928\n",
            "Epoch: 00 [16004/20928 ( 76%)], Train Loss: 0.58900\n",
            "Epoch: 00 [16044/20928 ( 77%)], Train Loss: 0.58861\n",
            "Epoch: 00 [16084/20928 ( 77%)], Train Loss: 0.58803\n",
            "Epoch: 00 [16124/20928 ( 77%)], Train Loss: 0.58828\n",
            "Epoch: 00 [16164/20928 ( 77%)], Train Loss: 0.58850\n",
            "Epoch: 00 [16204/20928 ( 77%)], Train Loss: 0.58835\n",
            "Epoch: 00 [16244/20928 ( 78%)], Train Loss: 0.58796\n",
            "Epoch: 00 [16284/20928 ( 78%)], Train Loss: 0.58732\n",
            "Epoch: 00 [16324/20928 ( 78%)], Train Loss: 0.58668\n",
            "Epoch: 00 [16364/20928 ( 78%)], Train Loss: 0.58634\n",
            "Epoch: 00 [16404/20928 ( 78%)], Train Loss: 0.58575\n",
            "Epoch: 00 [16444/20928 ( 79%)], Train Loss: 0.58520\n",
            "Epoch: 00 [16484/20928 ( 79%)], Train Loss: 0.58508\n",
            "Epoch: 00 [16524/20928 ( 79%)], Train Loss: 0.58443\n",
            "Epoch: 00 [16564/20928 ( 79%)], Train Loss: 0.58409\n",
            "Epoch: 00 [16604/20928 ( 79%)], Train Loss: 0.58339\n",
            "Epoch: 00 [16644/20928 ( 80%)], Train Loss: 0.58295\n",
            "Epoch: 00 [16684/20928 ( 80%)], Train Loss: 0.58269\n",
            "Epoch: 00 [16724/20928 ( 80%)], Train Loss: 0.58246\n",
            "Epoch: 00 [16764/20928 ( 80%)], Train Loss: 0.58190\n",
            "Epoch: 00 [16804/20928 ( 80%)], Train Loss: 0.58193\n",
            "Epoch: 00 [16844/20928 ( 80%)], Train Loss: 0.58137\n",
            "Epoch: 00 [16884/20928 ( 81%)], Train Loss: 0.58068\n",
            "Epoch: 00 [16924/20928 ( 81%)], Train Loss: 0.58043\n",
            "Epoch: 00 [16964/20928 ( 81%)], Train Loss: 0.57964\n",
            "Epoch: 00 [17004/20928 ( 81%)], Train Loss: 0.57940\n",
            "Epoch: 00 [17044/20928 ( 81%)], Train Loss: 0.57892\n",
            "Epoch: 00 [17084/20928 ( 82%)], Train Loss: 0.57850\n",
            "Epoch: 00 [17124/20928 ( 82%)], Train Loss: 0.57781\n",
            "Epoch: 00 [17164/20928 ( 82%)], Train Loss: 0.57805\n",
            "Epoch: 00 [17204/20928 ( 82%)], Train Loss: 0.57756\n",
            "Epoch: 00 [17244/20928 ( 82%)], Train Loss: 0.57724\n",
            "Epoch: 00 [17284/20928 ( 83%)], Train Loss: 0.57671\n",
            "Epoch: 00 [17324/20928 ( 83%)], Train Loss: 0.57623\n",
            "Epoch: 00 [17364/20928 ( 83%)], Train Loss: 0.57610\n",
            "Epoch: 00 [17404/20928 ( 83%)], Train Loss: 0.57571\n",
            "Epoch: 00 [17444/20928 ( 83%)], Train Loss: 0.57501\n",
            "Epoch: 00 [17484/20928 ( 84%)], Train Loss: 0.57454\n",
            "Epoch: 00 [17524/20928 ( 84%)], Train Loss: 0.57437\n",
            "Epoch: 00 [17564/20928 ( 84%)], Train Loss: 0.57410\n",
            "Epoch: 00 [17604/20928 ( 84%)], Train Loss: 0.57370\n",
            "Epoch: 00 [17644/20928 ( 84%)], Train Loss: 0.57302\n",
            "Epoch: 00 [17684/20928 ( 84%)], Train Loss: 0.57241\n",
            "Epoch: 00 [17724/20928 ( 85%)], Train Loss: 0.57190\n",
            "Epoch: 00 [17764/20928 ( 85%)], Train Loss: 0.57175\n",
            "Epoch: 00 [17804/20928 ( 85%)], Train Loss: 0.57209\n",
            "Epoch: 00 [17844/20928 ( 85%)], Train Loss: 0.57222\n",
            "Epoch: 00 [17884/20928 ( 85%)], Train Loss: 0.57191\n",
            "Epoch: 00 [17924/20928 ( 86%)], Train Loss: 0.57211\n",
            "Epoch: 00 [17964/20928 ( 86%)], Train Loss: 0.57205\n",
            "Epoch: 00 [18004/20928 ( 86%)], Train Loss: 0.57196\n",
            "Epoch: 00 [18044/20928 ( 86%)], Train Loss: 0.57163\n",
            "Epoch: 00 [18084/20928 ( 86%)], Train Loss: 0.57122\n",
            "Epoch: 00 [18124/20928 ( 87%)], Train Loss: 0.57098\n",
            "Epoch: 00 [18164/20928 ( 87%)], Train Loss: 0.57023\n",
            "Epoch: 00 [18204/20928 ( 87%)], Train Loss: 0.56981\n",
            "Epoch: 00 [18244/20928 ( 87%)], Train Loss: 0.56925\n",
            "Epoch: 00 [18284/20928 ( 87%)], Train Loss: 0.56862\n",
            "Epoch: 00 [18324/20928 ( 88%)], Train Loss: 0.56845\n",
            "Epoch: 00 [18364/20928 ( 88%)], Train Loss: 0.56784\n",
            "Epoch: 00 [18404/20928 ( 88%)], Train Loss: 0.56786\n",
            "Epoch: 00 [18444/20928 ( 88%)], Train Loss: 0.56707\n",
            "Epoch: 00 [18484/20928 ( 88%)], Train Loss: 0.56703\n",
            "Epoch: 00 [18524/20928 ( 89%)], Train Loss: 0.56698\n",
            "Epoch: 00 [18564/20928 ( 89%)], Train Loss: 0.56647\n",
            "Epoch: 00 [18604/20928 ( 89%)], Train Loss: 0.56621\n",
            "Epoch: 00 [18644/20928 ( 89%)], Train Loss: 0.56622\n",
            "Epoch: 00 [18684/20928 ( 89%)], Train Loss: 0.56582\n",
            "Epoch: 00 [18724/20928 ( 89%)], Train Loss: 0.56568\n",
            "Epoch: 00 [18764/20928 ( 90%)], Train Loss: 0.56536\n",
            "Epoch: 00 [18804/20928 ( 90%)], Train Loss: 0.56461\n",
            "Epoch: 00 [18844/20928 ( 90%)], Train Loss: 0.56454\n",
            "Epoch: 00 [18884/20928 ( 90%)], Train Loss: 0.56454\n",
            "Epoch: 00 [18924/20928 ( 90%)], Train Loss: 0.56469\n",
            "Epoch: 00 [18964/20928 ( 91%)], Train Loss: 0.56500\n",
            "Epoch: 00 [19004/20928 ( 91%)], Train Loss: 0.56484\n",
            "Epoch: 00 [19044/20928 ( 91%)], Train Loss: 0.56466\n",
            "Epoch: 00 [19084/20928 ( 91%)], Train Loss: 0.56422\n",
            "Epoch: 00 [19124/20928 ( 91%)], Train Loss: 0.56360\n",
            "Epoch: 00 [19164/20928 ( 92%)], Train Loss: 0.56353\n",
            "Epoch: 00 [19204/20928 ( 92%)], Train Loss: 0.56316\n",
            "Epoch: 00 [19244/20928 ( 92%)], Train Loss: 0.56342\n",
            "Epoch: 00 [19284/20928 ( 92%)], Train Loss: 0.56321\n",
            "Epoch: 00 [19324/20928 ( 92%)], Train Loss: 0.56289\n",
            "Epoch: 00 [19364/20928 ( 93%)], Train Loss: 0.56284\n",
            "Epoch: 00 [19404/20928 ( 93%)], Train Loss: 0.56220\n",
            "Epoch: 00 [19444/20928 ( 93%)], Train Loss: 0.56183\n",
            "Epoch: 00 [19484/20928 ( 93%)], Train Loss: 0.56150\n",
            "Epoch: 00 [19524/20928 ( 93%)], Train Loss: 0.56130\n",
            "Epoch: 00 [19564/20928 ( 93%)], Train Loss: 0.56095\n",
            "Epoch: 00 [19604/20928 ( 94%)], Train Loss: 0.56066\n",
            "Epoch: 00 [19644/20928 ( 94%)], Train Loss: 0.56047\n",
            "Epoch: 00 [19684/20928 ( 94%)], Train Loss: 0.55981\n",
            "Epoch: 00 [19724/20928 ( 94%)], Train Loss: 0.55971\n",
            "Epoch: 00 [19764/20928 ( 94%)], Train Loss: 0.55885\n",
            "Epoch: 00 [19804/20928 ( 95%)], Train Loss: 0.55835\n",
            "Epoch: 00 [19844/20928 ( 95%)], Train Loss: 0.55842\n",
            "Epoch: 00 [19884/20928 ( 95%)], Train Loss: 0.55830\n",
            "Epoch: 00 [19924/20928 ( 95%)], Train Loss: 0.55824\n",
            "Epoch: 00 [19964/20928 ( 95%)], Train Loss: 0.55776\n",
            "Epoch: 00 [20004/20928 ( 96%)], Train Loss: 0.55740\n",
            "Epoch: 00 [20044/20928 ( 96%)], Train Loss: 0.55711\n",
            "Epoch: 00 [20084/20928 ( 96%)], Train Loss: 0.55690\n",
            "Epoch: 00 [20124/20928 ( 96%)], Train Loss: 0.55664\n",
            "Epoch: 00 [20164/20928 ( 96%)], Train Loss: 0.55650\n",
            "Epoch: 00 [20204/20928 ( 97%)], Train Loss: 0.55654\n",
            "Epoch: 00 [20244/20928 ( 97%)], Train Loss: 0.55622\n",
            "Epoch: 00 [20284/20928 ( 97%)], Train Loss: 0.55621\n",
            "Epoch: 00 [20324/20928 ( 97%)], Train Loss: 0.55594\n",
            "Epoch: 00 [20364/20928 ( 97%)], Train Loss: 0.55603\n",
            "Epoch: 00 [20404/20928 ( 97%)], Train Loss: 0.55559\n",
            "Epoch: 00 [20444/20928 ( 98%)], Train Loss: 0.55525\n",
            "Epoch: 00 [20484/20928 ( 98%)], Train Loss: 0.55501\n",
            "Epoch: 00 [20524/20928 ( 98%)], Train Loss: 0.55469\n",
            "Epoch: 00 [20564/20928 ( 98%)], Train Loss: 0.55461\n",
            "Epoch: 00 [20604/20928 ( 98%)], Train Loss: 0.55449\n",
            "Epoch: 00 [20644/20928 ( 99%)], Train Loss: 0.55416\n",
            "Epoch: 00 [20684/20928 ( 99%)], Train Loss: 0.55394\n",
            "Epoch: 00 [20724/20928 ( 99%)], Train Loss: 0.55371\n",
            "Epoch: 00 [20764/20928 ( 99%)], Train Loss: 0.55290\n",
            "Epoch: 00 [20804/20928 ( 99%)], Train Loss: 0.55229\n",
            "Epoch: 00 [20844/20928 (100%)], Train Loss: 0.55190\n",
            "Epoch: 00 [20884/20928 (100%)], Train Loss: 0.55141\n",
            "Epoch: 00 [20924/20928 (100%)], Train Loss: 0.55119\n",
            "Epoch: 00 [20928/20928 (100%)], Train Loss: 0.55110\n",
            "----Validation Results Summary----\n",
            "Epoch: [0] Valid Loss: 0.32137\n",
            "0 Epoch, Best epoch was updated! Valid Loss: 0.32137\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 01 [    4/20928 (  0%)], Train Loss: 0.17097\n",
            "Epoch: 01 [   44/20928 (  0%)], Train Loss: 0.40599\n",
            "Epoch: 01 [   84/20928 (  0%)], Train Loss: 0.47751\n",
            "Epoch: 01 [  124/20928 (  1%)], Train Loss: 0.52712\n",
            "Epoch: 01 [  164/20928 (  1%)], Train Loss: 0.50181\n",
            "Epoch: 01 [  204/20928 (  1%)], Train Loss: 0.52699\n",
            "Epoch: 01 [  244/20928 (  1%)], Train Loss: 0.52291\n",
            "Epoch: 01 [  284/20928 (  1%)], Train Loss: 0.48506\n",
            "Epoch: 01 [  324/20928 (  2%)], Train Loss: 0.47679\n",
            "Epoch: 01 [  364/20928 (  2%)], Train Loss: 0.47394\n",
            "Epoch: 01 [  404/20928 (  2%)], Train Loss: 0.45802\n",
            "Epoch: 01 [  444/20928 (  2%)], Train Loss: 0.43677\n",
            "Epoch: 01 [  484/20928 (  2%)], Train Loss: 0.43366\n",
            "Epoch: 01 [  524/20928 (  3%)], Train Loss: 0.41845\n",
            "Epoch: 01 [  564/20928 (  3%)], Train Loss: 0.42826\n",
            "Epoch: 01 [  604/20928 (  3%)], Train Loss: 0.42828\n",
            "Epoch: 01 [  644/20928 (  3%)], Train Loss: 0.42457\n",
            "Epoch: 01 [  684/20928 (  3%)], Train Loss: 0.41913\n",
            "Epoch: 01 [  724/20928 (  3%)], Train Loss: 0.41587\n",
            "Epoch: 01 [  764/20928 (  4%)], Train Loss: 0.41628\n",
            "Epoch: 01 [  804/20928 (  4%)], Train Loss: 0.40819\n",
            "Epoch: 01 [  844/20928 (  4%)], Train Loss: 0.41309\n",
            "Epoch: 01 [  884/20928 (  4%)], Train Loss: 0.41360\n",
            "Epoch: 01 [  924/20928 (  4%)], Train Loss: 0.41001\n",
            "Epoch: 01 [  964/20928 (  5%)], Train Loss: 0.40707\n",
            "Epoch: 01 [ 1004/20928 (  5%)], Train Loss: 0.40267\n",
            "Epoch: 01 [ 1044/20928 (  5%)], Train Loss: 0.39726\n",
            "Epoch: 01 [ 1084/20928 (  5%)], Train Loss: 0.38998\n",
            "Epoch: 01 [ 1124/20928 (  5%)], Train Loss: 0.39468\n",
            "Epoch: 01 [ 1164/20928 (  6%)], Train Loss: 0.39262\n",
            "Epoch: 01 [ 1204/20928 (  6%)], Train Loss: 0.39270\n",
            "Epoch: 01 [ 1244/20928 (  6%)], Train Loss: 0.39226\n",
            "Epoch: 01 [ 1284/20928 (  6%)], Train Loss: 0.38777\n",
            "Epoch: 01 [ 1324/20928 (  6%)], Train Loss: 0.39459\n",
            "Epoch: 01 [ 1364/20928 (  7%)], Train Loss: 0.39076\n",
            "Epoch: 01 [ 1404/20928 (  7%)], Train Loss: 0.38605\n",
            "Epoch: 01 [ 1444/20928 (  7%)], Train Loss: 0.38250\n",
            "Epoch: 01 [ 1484/20928 (  7%)], Train Loss: 0.38201\n",
            "Epoch: 01 [ 1524/20928 (  7%)], Train Loss: 0.38759\n",
            "Epoch: 01 [ 1564/20928 (  7%)], Train Loss: 0.38424\n",
            "Epoch: 01 [ 1604/20928 (  8%)], Train Loss: 0.38538\n",
            "Epoch: 01 [ 1644/20928 (  8%)], Train Loss: 0.38382\n",
            "Epoch: 01 [ 1684/20928 (  8%)], Train Loss: 0.38111\n",
            "Epoch: 01 [ 1724/20928 (  8%)], Train Loss: 0.37908\n",
            "Epoch: 01 [ 1764/20928 (  8%)], Train Loss: 0.37669\n",
            "Epoch: 01 [ 1804/20928 (  9%)], Train Loss: 0.37109\n",
            "Epoch: 01 [ 1844/20928 (  9%)], Train Loss: 0.37322\n",
            "Epoch: 01 [ 1884/20928 (  9%)], Train Loss: 0.37438\n",
            "Epoch: 01 [ 1924/20928 (  9%)], Train Loss: 0.37170\n",
            "Epoch: 01 [ 1964/20928 (  9%)], Train Loss: 0.36793\n",
            "Epoch: 01 [ 2004/20928 ( 10%)], Train Loss: 0.36391\n",
            "Epoch: 01 [ 2044/20928 ( 10%)], Train Loss: 0.36090\n",
            "Epoch: 01 [ 2084/20928 ( 10%)], Train Loss: 0.36145\n",
            "Epoch: 01 [ 2124/20928 ( 10%)], Train Loss: 0.35813\n",
            "Epoch: 01 [ 2164/20928 ( 10%)], Train Loss: 0.35874\n",
            "Epoch: 01 [ 2204/20928 ( 11%)], Train Loss: 0.35689\n",
            "Epoch: 01 [ 2244/20928 ( 11%)], Train Loss: 0.35727\n",
            "Epoch: 01 [ 2284/20928 ( 11%)], Train Loss: 0.35897\n",
            "Epoch: 01 [ 2324/20928 ( 11%)], Train Loss: 0.36019\n",
            "Epoch: 01 [ 2364/20928 ( 11%)], Train Loss: 0.35704\n",
            "Epoch: 01 [ 2404/20928 ( 11%)], Train Loss: 0.35707\n",
            "Epoch: 01 [ 2444/20928 ( 12%)], Train Loss: 0.35619\n",
            "Epoch: 01 [ 2484/20928 ( 12%)], Train Loss: 0.35551\n",
            "Epoch: 01 [ 2524/20928 ( 12%)], Train Loss: 0.35406\n",
            "Epoch: 01 [ 2564/20928 ( 12%)], Train Loss: 0.35311\n",
            "Epoch: 01 [ 2604/20928 ( 12%)], Train Loss: 0.35037\n",
            "Epoch: 01 [ 2644/20928 ( 13%)], Train Loss: 0.34675\n",
            "Epoch: 01 [ 2684/20928 ( 13%)], Train Loss: 0.34413\n",
            "Epoch: 01 [ 2724/20928 ( 13%)], Train Loss: 0.34285\n",
            "Epoch: 01 [ 2764/20928 ( 13%)], Train Loss: 0.34274\n",
            "Epoch: 01 [ 2804/20928 ( 13%)], Train Loss: 0.34042\n",
            "Epoch: 01 [ 2844/20928 ( 14%)], Train Loss: 0.33949\n",
            "Epoch: 01 [ 2884/20928 ( 14%)], Train Loss: 0.33921\n",
            "Epoch: 01 [ 2924/20928 ( 14%)], Train Loss: 0.33858\n",
            "Epoch: 01 [ 2964/20928 ( 14%)], Train Loss: 0.33593\n",
            "Epoch: 01 [ 3004/20928 ( 14%)], Train Loss: 0.33324\n",
            "Epoch: 01 [ 3044/20928 ( 15%)], Train Loss: 0.33103\n",
            "Epoch: 01 [ 3084/20928 ( 15%)], Train Loss: 0.32973\n",
            "Epoch: 01 [ 3124/20928 ( 15%)], Train Loss: 0.33006\n",
            "Epoch: 01 [ 3164/20928 ( 15%)], Train Loss: 0.33036\n",
            "Epoch: 01 [ 3204/20928 ( 15%)], Train Loss: 0.32938\n",
            "Epoch: 01 [ 3244/20928 ( 16%)], Train Loss: 0.32841\n",
            "Epoch: 01 [ 3284/20928 ( 16%)], Train Loss: 0.32652\n",
            "Epoch: 01 [ 3324/20928 ( 16%)], Train Loss: 0.32603\n",
            "Epoch: 01 [ 3364/20928 ( 16%)], Train Loss: 0.32550\n",
            "Epoch: 01 [ 3404/20928 ( 16%)], Train Loss: 0.32350\n",
            "Epoch: 01 [ 3444/20928 ( 16%)], Train Loss: 0.32235\n",
            "Epoch: 01 [ 3484/20928 ( 17%)], Train Loss: 0.32066\n",
            "Epoch: 01 [ 3524/20928 ( 17%)], Train Loss: 0.31949\n",
            "Epoch: 01 [ 3564/20928 ( 17%)], Train Loss: 0.31707\n",
            "Epoch: 01 [ 3604/20928 ( 17%)], Train Loss: 0.31590\n",
            "Epoch: 01 [ 3644/20928 ( 17%)], Train Loss: 0.31475\n",
            "Epoch: 01 [ 3684/20928 ( 18%)], Train Loss: 0.31438\n",
            "Epoch: 01 [ 3724/20928 ( 18%)], Train Loss: 0.31299\n",
            "Epoch: 01 [ 3764/20928 ( 18%)], Train Loss: 0.31069\n",
            "Epoch: 01 [ 3804/20928 ( 18%)], Train Loss: 0.30934\n",
            "Epoch: 01 [ 3844/20928 ( 18%)], Train Loss: 0.30774\n",
            "Epoch: 01 [ 3884/20928 ( 19%)], Train Loss: 0.30607\n",
            "Epoch: 01 [ 3924/20928 ( 19%)], Train Loss: 0.30614\n",
            "Epoch: 01 [ 3964/20928 ( 19%)], Train Loss: 0.30398\n",
            "Epoch: 01 [ 4004/20928 ( 19%)], Train Loss: 0.30279\n",
            "Epoch: 01 [ 4044/20928 ( 19%)], Train Loss: 0.30052\n",
            "Epoch: 01 [ 4084/20928 ( 20%)], Train Loss: 0.29949\n",
            "Epoch: 01 [ 4124/20928 ( 20%)], Train Loss: 0.29706\n",
            "Epoch: 01 [ 4164/20928 ( 20%)], Train Loss: 0.29594\n",
            "Epoch: 01 [ 4204/20928 ( 20%)], Train Loss: 0.29455\n",
            "Epoch: 01 [ 4244/20928 ( 20%)], Train Loss: 0.29371\n",
            "Epoch: 01 [ 4284/20928 ( 20%)], Train Loss: 0.29283\n",
            "Epoch: 01 [ 4324/20928 ( 21%)], Train Loss: 0.29214\n",
            "Epoch: 01 [ 4364/20928 ( 21%)], Train Loss: 0.29282\n",
            "Epoch: 01 [ 4404/20928 ( 21%)], Train Loss: 0.29248\n",
            "Epoch: 01 [ 4444/20928 ( 21%)], Train Loss: 0.29205\n",
            "Epoch: 01 [ 4484/20928 ( 21%)], Train Loss: 0.29161\n",
            "Epoch: 01 [ 4524/20928 ( 22%)], Train Loss: 0.29116\n",
            "Epoch: 01 [ 4564/20928 ( 22%)], Train Loss: 0.28990\n",
            "Epoch: 01 [ 4604/20928 ( 22%)], Train Loss: 0.28960\n",
            "Epoch: 01 [ 4644/20928 ( 22%)], Train Loss: 0.28856\n",
            "Epoch: 01 [ 4684/20928 ( 22%)], Train Loss: 0.28725\n",
            "Epoch: 01 [ 4724/20928 ( 23%)], Train Loss: 0.28655\n",
            "Epoch: 01 [ 4764/20928 ( 23%)], Train Loss: 0.28566\n",
            "Epoch: 01 [ 4804/20928 ( 23%)], Train Loss: 0.28568\n",
            "Epoch: 01 [ 4844/20928 ( 23%)], Train Loss: 0.28410\n",
            "Epoch: 01 [ 4884/20928 ( 23%)], Train Loss: 0.28388\n",
            "Epoch: 01 [ 4924/20928 ( 24%)], Train Loss: 0.28426\n",
            "Epoch: 01 [ 4964/20928 ( 24%)], Train Loss: 0.28399\n",
            "Epoch: 01 [ 5004/20928 ( 24%)], Train Loss: 0.28373\n",
            "Epoch: 01 [ 5044/20928 ( 24%)], Train Loss: 0.28316\n",
            "Epoch: 01 [ 5084/20928 ( 24%)], Train Loss: 0.28283\n",
            "Epoch: 01 [ 5124/20928 ( 24%)], Train Loss: 0.28241\n",
            "Epoch: 01 [ 5164/20928 ( 25%)], Train Loss: 0.28237\n",
            "Epoch: 01 [ 5204/20928 ( 25%)], Train Loss: 0.28381\n",
            "Epoch: 01 [ 5244/20928 ( 25%)], Train Loss: 0.28430\n",
            "Epoch: 01 [ 5284/20928 ( 25%)], Train Loss: 0.28396\n",
            "Epoch: 01 [ 5324/20928 ( 25%)], Train Loss: 0.28326\n",
            "Epoch: 01 [ 5364/20928 ( 26%)], Train Loss: 0.28209\n",
            "Epoch: 01 [ 5404/20928 ( 26%)], Train Loss: 0.28324\n",
            "Epoch: 01 [ 5444/20928 ( 26%)], Train Loss: 0.28228\n",
            "Epoch: 01 [ 5484/20928 ( 26%)], Train Loss: 0.28098\n",
            "Epoch: 01 [ 5524/20928 ( 26%)], Train Loss: 0.28053\n",
            "Epoch: 01 [ 5564/20928 ( 27%)], Train Loss: 0.28019\n",
            "Epoch: 01 [ 5604/20928 ( 27%)], Train Loss: 0.27953\n",
            "Epoch: 01 [ 5644/20928 ( 27%)], Train Loss: 0.27950\n",
            "Epoch: 01 [ 5684/20928 ( 27%)], Train Loss: 0.27879\n",
            "Epoch: 01 [ 5724/20928 ( 27%)], Train Loss: 0.27800\n",
            "Epoch: 01 [ 5764/20928 ( 28%)], Train Loss: 0.27729\n",
            "Epoch: 01 [ 5804/20928 ( 28%)], Train Loss: 0.27656\n",
            "Epoch: 01 [ 5844/20928 ( 28%)], Train Loss: 0.27642\n",
            "Epoch: 01 [ 5884/20928 ( 28%)], Train Loss: 0.27519\n",
            "Epoch: 01 [ 5924/20928 ( 28%)], Train Loss: 0.27419\n",
            "Epoch: 01 [ 5964/20928 ( 28%)], Train Loss: 0.27367\n",
            "Epoch: 01 [ 6004/20928 ( 29%)], Train Loss: 0.27363\n",
            "Epoch: 01 [ 6044/20928 ( 29%)], Train Loss: 0.27288\n",
            "Epoch: 01 [ 6084/20928 ( 29%)], Train Loss: 0.27181\n",
            "Epoch: 01 [ 6124/20928 ( 29%)], Train Loss: 0.27090\n",
            "Epoch: 01 [ 6164/20928 ( 29%)], Train Loss: 0.26986\n",
            "Epoch: 01 [ 6204/20928 ( 30%)], Train Loss: 0.26955\n",
            "Epoch: 01 [ 6244/20928 ( 30%)], Train Loss: 0.26935\n",
            "Epoch: 01 [ 6284/20928 ( 30%)], Train Loss: 0.26843\n",
            "Epoch: 01 [ 6324/20928 ( 30%)], Train Loss: 0.26723\n",
            "Epoch: 01 [ 6364/20928 ( 30%)], Train Loss: 0.26688\n",
            "Epoch: 01 [ 6404/20928 ( 31%)], Train Loss: 0.26587\n",
            "Epoch: 01 [ 6444/20928 ( 31%)], Train Loss: 0.26520\n",
            "Epoch: 01 [ 6484/20928 ( 31%)], Train Loss: 0.26455\n",
            "Epoch: 01 [ 6524/20928 ( 31%)], Train Loss: 0.26378\n",
            "Epoch: 01 [ 6564/20928 ( 31%)], Train Loss: 0.26349\n",
            "Epoch: 01 [ 6604/20928 ( 32%)], Train Loss: 0.26270\n",
            "Epoch: 01 [ 6644/20928 ( 32%)], Train Loss: 0.26358\n",
            "Epoch: 01 [ 6684/20928 ( 32%)], Train Loss: 0.26304\n",
            "Epoch: 01 [ 6724/20928 ( 32%)], Train Loss: 0.26215\n",
            "Epoch: 01 [ 6764/20928 ( 32%)], Train Loss: 0.26129\n",
            "Epoch: 01 [ 6804/20928 ( 33%)], Train Loss: 0.26054\n",
            "Epoch: 01 [ 6844/20928 ( 33%)], Train Loss: 0.25948\n",
            "Epoch: 01 [ 6884/20928 ( 33%)], Train Loss: 0.25866\n",
            "Epoch: 01 [ 6924/20928 ( 33%)], Train Loss: 0.25782\n",
            "Epoch: 01 [ 6964/20928 ( 33%)], Train Loss: 0.25815\n",
            "Epoch: 01 [ 7004/20928 ( 33%)], Train Loss: 0.25890\n",
            "Epoch: 01 [ 7044/20928 ( 34%)], Train Loss: 0.25787\n",
            "Epoch: 01 [ 7084/20928 ( 34%)], Train Loss: 0.25788\n",
            "Epoch: 01 [ 7124/20928 ( 34%)], Train Loss: 0.25744\n",
            "Epoch: 01 [ 7164/20928 ( 34%)], Train Loss: 0.25721\n",
            "Epoch: 01 [ 7204/20928 ( 34%)], Train Loss: 0.25697\n",
            "Epoch: 01 [ 7244/20928 ( 35%)], Train Loss: 0.25762\n",
            "Epoch: 01 [ 7284/20928 ( 35%)], Train Loss: 0.25705\n",
            "Epoch: 01 [ 7324/20928 ( 35%)], Train Loss: 0.25617\n",
            "Epoch: 01 [ 7364/20928 ( 35%)], Train Loss: 0.25563\n",
            "Epoch: 01 [ 7404/20928 ( 35%)], Train Loss: 0.25483\n",
            "Epoch: 01 [ 7444/20928 ( 36%)], Train Loss: 0.25483\n",
            "Epoch: 01 [ 7484/20928 ( 36%)], Train Loss: 0.25421\n",
            "Epoch: 01 [ 7524/20928 ( 36%)], Train Loss: 0.25336\n",
            "Epoch: 01 [ 7564/20928 ( 36%)], Train Loss: 0.25293\n",
            "Epoch: 01 [ 7604/20928 ( 36%)], Train Loss: 0.25233\n",
            "Epoch: 01 [ 7644/20928 ( 37%)], Train Loss: 0.25157\n",
            "Epoch: 01 [ 7684/20928 ( 37%)], Train Loss: 0.25103\n",
            "Epoch: 01 [ 7724/20928 ( 37%)], Train Loss: 0.25043\n",
            "Epoch: 01 [ 7764/20928 ( 37%)], Train Loss: 0.25026\n",
            "Epoch: 01 [ 7804/20928 ( 37%)], Train Loss: 0.24966\n",
            "Epoch: 01 [ 7844/20928 ( 37%)], Train Loss: 0.24992\n",
            "Epoch: 01 [ 7884/20928 ( 38%)], Train Loss: 0.24972\n",
            "Epoch: 01 [ 7924/20928 ( 38%)], Train Loss: 0.24909\n",
            "Epoch: 01 [ 7964/20928 ( 38%)], Train Loss: 0.24813\n",
            "Epoch: 01 [ 8004/20928 ( 38%)], Train Loss: 0.24729\n",
            "Epoch: 01 [ 8044/20928 ( 38%)], Train Loss: 0.24702\n",
            "Epoch: 01 [ 8084/20928 ( 39%)], Train Loss: 0.24637\n",
            "Epoch: 01 [ 8124/20928 ( 39%)], Train Loss: 0.24638\n",
            "Epoch: 01 [ 8164/20928 ( 39%)], Train Loss: 0.24613\n",
            "Epoch: 01 [ 8204/20928 ( 39%)], Train Loss: 0.24573\n",
            "Epoch: 01 [ 8244/20928 ( 39%)], Train Loss: 0.24529\n",
            "Epoch: 01 [ 8284/20928 ( 40%)], Train Loss: 0.24531\n",
            "Epoch: 01 [ 8324/20928 ( 40%)], Train Loss: 0.24462\n",
            "Epoch: 01 [ 8364/20928 ( 40%)], Train Loss: 0.24422\n",
            "Epoch: 01 [ 8404/20928 ( 40%)], Train Loss: 0.24340\n",
            "Epoch: 01 [ 8444/20928 ( 40%)], Train Loss: 0.24289\n",
            "Epoch: 01 [ 8484/20928 ( 41%)], Train Loss: 0.24214\n",
            "Epoch: 01 [ 8524/20928 ( 41%)], Train Loss: 0.24148\n",
            "Epoch: 01 [ 8564/20928 ( 41%)], Train Loss: 0.24058\n",
            "Epoch: 01 [ 8604/20928 ( 41%)], Train Loss: 0.24038\n",
            "Epoch: 01 [ 8644/20928 ( 41%)], Train Loss: 0.24049\n",
            "Epoch: 01 [ 8684/20928 ( 41%)], Train Loss: 0.24014\n",
            "Epoch: 01 [ 8724/20928 ( 42%)], Train Loss: 0.23982\n",
            "Epoch: 01 [ 8764/20928 ( 42%)], Train Loss: 0.23946\n",
            "Epoch: 01 [ 8804/20928 ( 42%)], Train Loss: 0.23901\n",
            "Epoch: 01 [ 8844/20928 ( 42%)], Train Loss: 0.23855\n",
            "Epoch: 01 [ 8884/20928 ( 42%)], Train Loss: 0.23831\n",
            "Epoch: 01 [ 8924/20928 ( 43%)], Train Loss: 0.23791\n",
            "Epoch: 01 [ 8964/20928 ( 43%)], Train Loss: 0.23787\n",
            "Epoch: 01 [ 9004/20928 ( 43%)], Train Loss: 0.23789\n",
            "Epoch: 01 [ 9044/20928 ( 43%)], Train Loss: 0.23751\n",
            "Epoch: 01 [ 9084/20928 ( 43%)], Train Loss: 0.23683\n",
            "Epoch: 01 [ 9124/20928 ( 44%)], Train Loss: 0.23638\n",
            "Epoch: 01 [ 9164/20928 ( 44%)], Train Loss: 0.23633\n",
            "Epoch: 01 [ 9204/20928 ( 44%)], Train Loss: 0.23612\n",
            "Epoch: 01 [ 9244/20928 ( 44%)], Train Loss: 0.23545\n",
            "Epoch: 01 [ 9284/20928 ( 44%)], Train Loss: 0.23483\n",
            "Epoch: 01 [ 9324/20928 ( 45%)], Train Loss: 0.23450\n",
            "Epoch: 01 [ 9364/20928 ( 45%)], Train Loss: 0.23410\n",
            "Epoch: 01 [ 9404/20928 ( 45%)], Train Loss: 0.23419\n",
            "Epoch: 01 [ 9444/20928 ( 45%)], Train Loss: 0.23416\n",
            "Epoch: 01 [ 9484/20928 ( 45%)], Train Loss: 0.23406\n",
            "Epoch: 01 [ 9524/20928 ( 46%)], Train Loss: 0.23392\n",
            "Epoch: 01 [ 9564/20928 ( 46%)], Train Loss: 0.23367\n",
            "Epoch: 01 [ 9604/20928 ( 46%)], Train Loss: 0.23391\n",
            "Epoch: 01 [ 9644/20928 ( 46%)], Train Loss: 0.23386\n",
            "Epoch: 01 [ 9684/20928 ( 46%)], Train Loss: 0.23333\n",
            "Epoch: 01 [ 9724/20928 ( 46%)], Train Loss: 0.23351\n",
            "Epoch: 01 [ 9764/20928 ( 47%)], Train Loss: 0.23349\n",
            "Epoch: 01 [ 9804/20928 ( 47%)], Train Loss: 0.23351\n",
            "Epoch: 01 [ 9844/20928 ( 47%)], Train Loss: 0.23314\n",
            "Epoch: 01 [ 9884/20928 ( 47%)], Train Loss: 0.23371\n",
            "Epoch: 01 [ 9924/20928 ( 47%)], Train Loss: 0.23345\n",
            "Epoch: 01 [ 9964/20928 ( 48%)], Train Loss: 0.23339\n",
            "Epoch: 01 [10004/20928 ( 48%)], Train Loss: 0.23359\n",
            "Epoch: 01 [10044/20928 ( 48%)], Train Loss: 0.23356\n",
            "Epoch: 01 [10084/20928 ( 48%)], Train Loss: 0.23316\n",
            "Epoch: 01 [10124/20928 ( 48%)], Train Loss: 0.23319\n",
            "Epoch: 01 [10164/20928 ( 49%)], Train Loss: 0.23332\n",
            "Epoch: 01 [10204/20928 ( 49%)], Train Loss: 0.23275\n",
            "Epoch: 01 [10244/20928 ( 49%)], Train Loss: 0.23246\n",
            "Epoch: 01 [10284/20928 ( 49%)], Train Loss: 0.23243\n",
            "Epoch: 01 [10324/20928 ( 49%)], Train Loss: 0.23189\n",
            "Epoch: 01 [10364/20928 ( 50%)], Train Loss: 0.23164\n",
            "Epoch: 01 [10404/20928 ( 50%)], Train Loss: 0.23147\n",
            "Epoch: 01 [10444/20928 ( 50%)], Train Loss: 0.23170\n",
            "Epoch: 01 [10484/20928 ( 50%)], Train Loss: 0.23148\n",
            "Epoch: 01 [10524/20928 ( 50%)], Train Loss: 0.23128\n",
            "Epoch: 01 [10564/20928 ( 50%)], Train Loss: 0.23070\n",
            "Epoch: 01 [10604/20928 ( 51%)], Train Loss: 0.23015\n",
            "Epoch: 01 [10644/20928 ( 51%)], Train Loss: 0.23043\n",
            "Epoch: 01 [10684/20928 ( 51%)], Train Loss: 0.23021\n",
            "Epoch: 01 [10724/20928 ( 51%)], Train Loss: 0.22968\n",
            "Epoch: 01 [10764/20928 ( 51%)], Train Loss: 0.22921\n",
            "Epoch: 01 [10804/20928 ( 52%)], Train Loss: 0.22898\n",
            "Epoch: 01 [10844/20928 ( 52%)], Train Loss: 0.22880\n",
            "Epoch: 01 [10884/20928 ( 52%)], Train Loss: 0.22848\n",
            "Epoch: 01 [10924/20928 ( 52%)], Train Loss: 0.22801\n",
            "Epoch: 01 [10964/20928 ( 52%)], Train Loss: 0.22772\n",
            "Epoch: 01 [11004/20928 ( 53%)], Train Loss: 0.22779\n",
            "Epoch: 01 [11044/20928 ( 53%)], Train Loss: 0.22745\n",
            "Epoch: 01 [11084/20928 ( 53%)], Train Loss: 0.22717\n",
            "Epoch: 01 [11124/20928 ( 53%)], Train Loss: 0.22721\n",
            "Epoch: 01 [11164/20928 ( 53%)], Train Loss: 0.22707\n",
            "Epoch: 01 [11204/20928 ( 54%)], Train Loss: 0.22707\n",
            "Epoch: 01 [11244/20928 ( 54%)], Train Loss: 0.22685\n",
            "Epoch: 01 [11284/20928 ( 54%)], Train Loss: 0.22659\n",
            "Epoch: 01 [11324/20928 ( 54%)], Train Loss: 0.22673\n",
            "Epoch: 01 [11364/20928 ( 54%)], Train Loss: 0.22659\n",
            "Epoch: 01 [11404/20928 ( 54%)], Train Loss: 0.22675\n",
            "Epoch: 01 [11444/20928 ( 55%)], Train Loss: 0.22624\n",
            "Epoch: 01 [11484/20928 ( 55%)], Train Loss: 0.22588\n",
            "Epoch: 01 [11524/20928 ( 55%)], Train Loss: 0.22586\n",
            "Epoch: 01 [11564/20928 ( 55%)], Train Loss: 0.22574\n",
            "Epoch: 01 [11604/20928 ( 55%)], Train Loss: 0.22559\n",
            "Epoch: 01 [11644/20928 ( 56%)], Train Loss: 0.22561\n",
            "Epoch: 01 [11684/20928 ( 56%)], Train Loss: 0.22568\n",
            "Epoch: 01 [11724/20928 ( 56%)], Train Loss: 0.22544\n",
            "Epoch: 01 [11764/20928 ( 56%)], Train Loss: 0.22498\n",
            "Epoch: 01 [11804/20928 ( 56%)], Train Loss: 0.22470\n",
            "Epoch: 01 [11844/20928 ( 57%)], Train Loss: 0.22448\n",
            "Epoch: 01 [11884/20928 ( 57%)], Train Loss: 0.22419\n",
            "Epoch: 01 [11924/20928 ( 57%)], Train Loss: 0.22389\n",
            "Epoch: 01 [11964/20928 ( 57%)], Train Loss: 0.22338\n",
            "Epoch: 01 [12004/20928 ( 57%)], Train Loss: 0.22339\n",
            "Epoch: 01 [12044/20928 ( 58%)], Train Loss: 0.22304\n",
            "Epoch: 01 [12084/20928 ( 58%)], Train Loss: 0.22283\n",
            "Epoch: 01 [12124/20928 ( 58%)], Train Loss: 0.22242\n",
            "Epoch: 01 [12164/20928 ( 58%)], Train Loss: 0.22204\n",
            "Epoch: 01 [12204/20928 ( 58%)], Train Loss: 0.22173\n",
            "Epoch: 01 [12244/20928 ( 59%)], Train Loss: 0.22137\n",
            "Epoch: 01 [12284/20928 ( 59%)], Train Loss: 0.22116\n",
            "Epoch: 01 [12324/20928 ( 59%)], Train Loss: 0.22117\n",
            "Epoch: 01 [12364/20928 ( 59%)], Train Loss: 0.22109\n",
            "Epoch: 01 [12404/20928 ( 59%)], Train Loss: 0.22090\n",
            "Epoch: 01 [12444/20928 ( 59%)], Train Loss: 0.22035\n",
            "Epoch: 01 [12484/20928 ( 60%)], Train Loss: 0.21997\n",
            "Epoch: 01 [12524/20928 ( 60%)], Train Loss: 0.21973\n",
            "Epoch: 01 [12564/20928 ( 60%)], Train Loss: 0.21963\n",
            "Epoch: 01 [12604/20928 ( 60%)], Train Loss: 0.21935\n",
            "Epoch: 01 [12644/20928 ( 60%)], Train Loss: 0.21921\n",
            "Epoch: 01 [12684/20928 ( 61%)], Train Loss: 0.21885\n",
            "Epoch: 01 [12724/20928 ( 61%)], Train Loss: 0.21890\n",
            "Epoch: 01 [12764/20928 ( 61%)], Train Loss: 0.21871\n",
            "Epoch: 01 [12804/20928 ( 61%)], Train Loss: 0.21888\n",
            "Epoch: 01 [12844/20928 ( 61%)], Train Loss: 0.21907\n",
            "Epoch: 01 [12884/20928 ( 62%)], Train Loss: 0.21943\n",
            "Epoch: 01 [12924/20928 ( 62%)], Train Loss: 0.21959\n",
            "Epoch: 01 [12964/20928 ( 62%)], Train Loss: 0.21915\n",
            "Epoch: 01 [13004/20928 ( 62%)], Train Loss: 0.21911\n",
            "Epoch: 01 [13044/20928 ( 62%)], Train Loss: 0.21903\n",
            "Epoch: 01 [13084/20928 ( 63%)], Train Loss: 0.21901\n",
            "Epoch: 01 [13124/20928 ( 63%)], Train Loss: 0.21866\n",
            "Epoch: 01 [13164/20928 ( 63%)], Train Loss: 0.21820\n",
            "Epoch: 01 [13204/20928 ( 63%)], Train Loss: 0.21782\n",
            "Epoch: 01 [13244/20928 ( 63%)], Train Loss: 0.21753\n",
            "Epoch: 01 [13284/20928 ( 63%)], Train Loss: 0.21713\n",
            "Epoch: 01 [13324/20928 ( 64%)], Train Loss: 0.21679\n",
            "Epoch: 01 [13364/20928 ( 64%)], Train Loss: 0.21638\n",
            "Epoch: 01 [13404/20928 ( 64%)], Train Loss: 0.21646\n",
            "Epoch: 01 [13444/20928 ( 64%)], Train Loss: 0.21608\n",
            "Epoch: 01 [13484/20928 ( 64%)], Train Loss: 0.21578\n",
            "Epoch: 01 [13524/20928 ( 65%)], Train Loss: 0.21558\n",
            "Epoch: 01 [13564/20928 ( 65%)], Train Loss: 0.21524\n",
            "Epoch: 01 [13604/20928 ( 65%)], Train Loss: 0.21499\n",
            "Epoch: 01 [13644/20928 ( 65%)], Train Loss: 0.21480\n",
            "Epoch: 01 [13684/20928 ( 65%)], Train Loss: 0.21440\n",
            "Epoch: 01 [13724/20928 ( 66%)], Train Loss: 0.21417\n",
            "Epoch: 01 [13764/20928 ( 66%)], Train Loss: 0.21411\n",
            "Epoch: 01 [13804/20928 ( 66%)], Train Loss: 0.21395\n",
            "Epoch: 01 [13844/20928 ( 66%)], Train Loss: 0.21366\n",
            "Epoch: 01 [13884/20928 ( 66%)], Train Loss: 0.21314\n",
            "Epoch: 01 [13924/20928 ( 67%)], Train Loss: 0.21299\n",
            "Epoch: 01 [13964/20928 ( 67%)], Train Loss: 0.21265\n",
            "Epoch: 01 [14004/20928 ( 67%)], Train Loss: 0.21234\n",
            "Epoch: 01 [14044/20928 ( 67%)], Train Loss: 0.21203\n",
            "Epoch: 01 [14084/20928 ( 67%)], Train Loss: 0.21214\n",
            "Epoch: 01 [14124/20928 ( 67%)], Train Loss: 0.21185\n",
            "Epoch: 01 [14164/20928 ( 68%)], Train Loss: 0.21145\n",
            "Epoch: 01 [14204/20928 ( 68%)], Train Loss: 0.21116\n",
            "Epoch: 01 [14244/20928 ( 68%)], Train Loss: 0.21073\n",
            "Epoch: 01 [14284/20928 ( 68%)], Train Loss: 0.21064\n",
            "Epoch: 01 [14324/20928 ( 68%)], Train Loss: 0.21062\n",
            "Epoch: 01 [14364/20928 ( 69%)], Train Loss: 0.21059\n",
            "Epoch: 01 [14404/20928 ( 69%)], Train Loss: 0.21017\n",
            "Epoch: 01 [14444/20928 ( 69%)], Train Loss: 0.20986\n",
            "Epoch: 01 [14484/20928 ( 69%)], Train Loss: 0.20969\n",
            "Epoch: 01 [14524/20928 ( 69%)], Train Loss: 0.20989\n",
            "Epoch: 01 [14564/20928 ( 70%)], Train Loss: 0.20963\n",
            "Epoch: 01 [14604/20928 ( 70%)], Train Loss: 0.20942\n",
            "Epoch: 01 [14644/20928 ( 70%)], Train Loss: 0.20912\n",
            "Epoch: 01 [14684/20928 ( 70%)], Train Loss: 0.20903\n",
            "Epoch: 01 [14724/20928 ( 70%)], Train Loss: 0.20883\n",
            "Epoch: 01 [14764/20928 ( 71%)], Train Loss: 0.20844\n",
            "Epoch: 01 [14804/20928 ( 71%)], Train Loss: 0.20879\n",
            "Epoch: 01 [14844/20928 ( 71%)], Train Loss: 0.20843\n",
            "Epoch: 01 [14884/20928 ( 71%)], Train Loss: 0.20875\n",
            "Epoch: 01 [14924/20928 ( 71%)], Train Loss: 0.20859\n",
            "Epoch: 01 [14964/20928 ( 72%)], Train Loss: 0.20853\n",
            "Epoch: 01 [15004/20928 ( 72%)], Train Loss: 0.20846\n",
            "Epoch: 01 [15044/20928 ( 72%)], Train Loss: 0.20828\n",
            "Epoch: 01 [15084/20928 ( 72%)], Train Loss: 0.20828\n",
            "Epoch: 01 [15124/20928 ( 72%)], Train Loss: 0.20792\n",
            "Epoch: 01 [15164/20928 ( 72%)], Train Loss: 0.20768\n",
            "Epoch: 01 [15204/20928 ( 73%)], Train Loss: 0.20779\n",
            "Epoch: 01 [15244/20928 ( 73%)], Train Loss: 0.20748\n",
            "Epoch: 01 [15284/20928 ( 73%)], Train Loss: 0.20749\n",
            "Epoch: 01 [15324/20928 ( 73%)], Train Loss: 0.20731\n",
            "Epoch: 01 [15364/20928 ( 73%)], Train Loss: 0.20700\n",
            "Epoch: 01 [15404/20928 ( 74%)], Train Loss: 0.20681\n",
            "Epoch: 01 [15444/20928 ( 74%)], Train Loss: 0.20667\n",
            "Epoch: 01 [15484/20928 ( 74%)], Train Loss: 0.20661\n",
            "Epoch: 01 [15524/20928 ( 74%)], Train Loss: 0.20678\n",
            "Epoch: 01 [15564/20928 ( 74%)], Train Loss: 0.20644\n",
            "Epoch: 01 [15604/20928 ( 75%)], Train Loss: 0.20621\n",
            "Epoch: 01 [15644/20928 ( 75%)], Train Loss: 0.20628\n",
            "Epoch: 01 [15684/20928 ( 75%)], Train Loss: 0.20633\n",
            "Epoch: 01 [15724/20928 ( 75%)], Train Loss: 0.20600\n",
            "Epoch: 01 [15764/20928 ( 75%)], Train Loss: 0.20566\n",
            "Epoch: 01 [15804/20928 ( 76%)], Train Loss: 0.20539\n",
            "Epoch: 01 [15844/20928 ( 76%)], Train Loss: 0.20540\n",
            "Epoch: 01 [15884/20928 ( 76%)], Train Loss: 0.20532\n",
            "Epoch: 01 [15924/20928 ( 76%)], Train Loss: 0.20522\n",
            "Epoch: 01 [15964/20928 ( 76%)], Train Loss: 0.20508\n",
            "Epoch: 01 [16004/20928 ( 76%)], Train Loss: 0.20515\n",
            "Epoch: 01 [16044/20928 ( 77%)], Train Loss: 0.20493\n",
            "Epoch: 01 [16084/20928 ( 77%)], Train Loss: 0.20485\n",
            "Epoch: 01 [16124/20928 ( 77%)], Train Loss: 0.20488\n",
            "Epoch: 01 [16164/20928 ( 77%)], Train Loss: 0.20498\n",
            "Epoch: 01 [16204/20928 ( 77%)], Train Loss: 0.20494\n",
            "Epoch: 01 [16244/20928 ( 78%)], Train Loss: 0.20480\n",
            "Epoch: 01 [16284/20928 ( 78%)], Train Loss: 0.20457\n",
            "Epoch: 01 [16324/20928 ( 78%)], Train Loss: 0.20433\n",
            "Epoch: 01 [16364/20928 ( 78%)], Train Loss: 0.20425\n",
            "Epoch: 01 [16404/20928 ( 78%)], Train Loss: 0.20403\n",
            "Epoch: 01 [16444/20928 ( 79%)], Train Loss: 0.20374\n",
            "Epoch: 01 [16484/20928 ( 79%)], Train Loss: 0.20370\n",
            "Epoch: 01 [16524/20928 ( 79%)], Train Loss: 0.20360\n",
            "Epoch: 01 [16564/20928 ( 79%)], Train Loss: 0.20347\n",
            "Epoch: 01 [16604/20928 ( 79%)], Train Loss: 0.20344\n",
            "Epoch: 01 [16644/20928 ( 80%)], Train Loss: 0.20315\n",
            "Epoch: 01 [16684/20928 ( 80%)], Train Loss: 0.20305\n",
            "Epoch: 01 [16724/20928 ( 80%)], Train Loss: 0.20317\n",
            "Epoch: 01 [16764/20928 ( 80%)], Train Loss: 0.20287\n",
            "Epoch: 01 [16804/20928 ( 80%)], Train Loss: 0.20311\n",
            "Epoch: 01 [16844/20928 ( 80%)], Train Loss: 0.20291\n",
            "Epoch: 01 [16884/20928 ( 81%)], Train Loss: 0.20263\n",
            "Epoch: 01 [16924/20928 ( 81%)], Train Loss: 0.20251\n",
            "Epoch: 01 [16964/20928 ( 81%)], Train Loss: 0.20232\n",
            "Epoch: 01 [17004/20928 ( 81%)], Train Loss: 0.20204\n",
            "Epoch: 01 [17044/20928 ( 81%)], Train Loss: 0.20189\n",
            "Epoch: 01 [17084/20928 ( 82%)], Train Loss: 0.20189\n",
            "Epoch: 01 [17124/20928 ( 82%)], Train Loss: 0.20162\n",
            "Epoch: 01 [17164/20928 ( 82%)], Train Loss: 0.20157\n",
            "Epoch: 01 [17204/20928 ( 82%)], Train Loss: 0.20144\n",
            "Epoch: 01 [17244/20928 ( 82%)], Train Loss: 0.20123\n",
            "Epoch: 01 [17284/20928 ( 83%)], Train Loss: 0.20089\n",
            "Epoch: 01 [17324/20928 ( 83%)], Train Loss: 0.20070\n",
            "Epoch: 01 [17364/20928 ( 83%)], Train Loss: 0.20066\n",
            "Epoch: 01 [17404/20928 ( 83%)], Train Loss: 0.20056\n",
            "Epoch: 01 [17444/20928 ( 83%)], Train Loss: 0.20033\n",
            "Epoch: 01 [17484/20928 ( 84%)], Train Loss: 0.20035\n",
            "Epoch: 01 [17524/20928 ( 84%)], Train Loss: 0.20017\n",
            "Epoch: 01 [17564/20928 ( 84%)], Train Loss: 0.20013\n",
            "Epoch: 01 [17604/20928 ( 84%)], Train Loss: 0.20006\n",
            "Epoch: 01 [17644/20928 ( 84%)], Train Loss: 0.19973\n",
            "Epoch: 01 [17684/20928 ( 84%)], Train Loss: 0.19945\n",
            "Epoch: 01 [17724/20928 ( 85%)], Train Loss: 0.19916\n",
            "Epoch: 01 [17764/20928 ( 85%)], Train Loss: 0.19927\n",
            "Epoch: 01 [17804/20928 ( 85%)], Train Loss: 0.19956\n",
            "Epoch: 01 [17844/20928 ( 85%)], Train Loss: 0.19972\n",
            "Epoch: 01 [17884/20928 ( 85%)], Train Loss: 0.19957\n",
            "Epoch: 01 [17924/20928 ( 86%)], Train Loss: 0.19954\n",
            "Epoch: 01 [17964/20928 ( 86%)], Train Loss: 0.19971\n",
            "Epoch: 01 [18004/20928 ( 86%)], Train Loss: 0.19982\n",
            "Epoch: 01 [18044/20928 ( 86%)], Train Loss: 0.19960\n",
            "Epoch: 01 [18084/20928 ( 86%)], Train Loss: 0.19942\n",
            "Epoch: 01 [18124/20928 ( 87%)], Train Loss: 0.19929\n",
            "Epoch: 01 [18164/20928 ( 87%)], Train Loss: 0.19902\n",
            "Epoch: 01 [18204/20928 ( 87%)], Train Loss: 0.19881\n",
            "Epoch: 01 [18244/20928 ( 87%)], Train Loss: 0.19869\n",
            "Epoch: 01 [18284/20928 ( 87%)], Train Loss: 0.19837\n",
            "Epoch: 01 [18324/20928 ( 88%)], Train Loss: 0.19844\n",
            "Epoch: 01 [18364/20928 ( 88%)], Train Loss: 0.19821\n",
            "Epoch: 01 [18404/20928 ( 88%)], Train Loss: 0.19811\n",
            "Epoch: 01 [18444/20928 ( 88%)], Train Loss: 0.19778\n",
            "Epoch: 01 [18484/20928 ( 88%)], Train Loss: 0.19770\n",
            "Epoch: 01 [18524/20928 ( 89%)], Train Loss: 0.19790\n",
            "Epoch: 01 [18564/20928 ( 89%)], Train Loss: 0.19774\n",
            "Epoch: 01 [18604/20928 ( 89%)], Train Loss: 0.19760\n",
            "Epoch: 01 [18644/20928 ( 89%)], Train Loss: 0.19761\n",
            "Epoch: 01 [18684/20928 ( 89%)], Train Loss: 0.19743\n",
            "Epoch: 01 [18724/20928 ( 89%)], Train Loss: 0.19723\n",
            "Epoch: 01 [18764/20928 ( 90%)], Train Loss: 0.19708\n",
            "Epoch: 01 [18804/20928 ( 90%)], Train Loss: 0.19677\n",
            "Epoch: 01 [18844/20928 ( 90%)], Train Loss: 0.19666\n",
            "Epoch: 01 [18884/20928 ( 90%)], Train Loss: 0.19665\n",
            "Epoch: 01 [18924/20928 ( 90%)], Train Loss: 0.19674\n",
            "Epoch: 01 [18964/20928 ( 91%)], Train Loss: 0.19709\n",
            "Epoch: 01 [19004/20928 ( 91%)], Train Loss: 0.19716\n",
            "Epoch: 01 [19044/20928 ( 91%)], Train Loss: 0.19723\n",
            "Epoch: 01 [19084/20928 ( 91%)], Train Loss: 0.19713\n",
            "Epoch: 01 [19124/20928 ( 91%)], Train Loss: 0.19690\n",
            "Epoch: 01 [19164/20928 ( 92%)], Train Loss: 0.19677\n",
            "Epoch: 01 [19204/20928 ( 92%)], Train Loss: 0.19677\n",
            "Epoch: 01 [19244/20928 ( 92%)], Train Loss: 0.19698\n",
            "Epoch: 01 [19284/20928 ( 92%)], Train Loss: 0.19691\n",
            "Epoch: 01 [19324/20928 ( 92%)], Train Loss: 0.19667\n",
            "Epoch: 01 [19364/20928 ( 93%)], Train Loss: 0.19669\n",
            "Epoch: 01 [19404/20928 ( 93%)], Train Loss: 0.19640\n",
            "Epoch: 01 [19444/20928 ( 93%)], Train Loss: 0.19633\n",
            "Epoch: 01 [19484/20928 ( 93%)], Train Loss: 0.19620\n",
            "Epoch: 01 [19524/20928 ( 93%)], Train Loss: 0.19612\n",
            "Epoch: 01 [19564/20928 ( 93%)], Train Loss: 0.19603\n",
            "Epoch: 01 [19604/20928 ( 94%)], Train Loss: 0.19598\n",
            "Epoch: 01 [19644/20928 ( 94%)], Train Loss: 0.19590\n",
            "Epoch: 01 [19684/20928 ( 94%)], Train Loss: 0.19558\n",
            "Epoch: 01 [19724/20928 ( 94%)], Train Loss: 0.19550\n",
            "Epoch: 01 [19764/20928 ( 94%)], Train Loss: 0.19519\n",
            "Epoch: 01 [19804/20928 ( 95%)], Train Loss: 0.19510\n",
            "Epoch: 01 [19844/20928 ( 95%)], Train Loss: 0.19504\n",
            "Epoch: 01 [19884/20928 ( 95%)], Train Loss: 0.19499\n",
            "Epoch: 01 [19924/20928 ( 95%)], Train Loss: 0.19480\n",
            "Epoch: 01 [19964/20928 ( 95%)], Train Loss: 0.19462\n",
            "Epoch: 01 [20004/20928 ( 96%)], Train Loss: 0.19455\n",
            "Epoch: 01 [20044/20928 ( 96%)], Train Loss: 0.19448\n",
            "Epoch: 01 [20084/20928 ( 96%)], Train Loss: 0.19449\n",
            "Epoch: 01 [20124/20928 ( 96%)], Train Loss: 0.19440\n",
            "Epoch: 01 [20164/20928 ( 96%)], Train Loss: 0.19430\n",
            "Epoch: 01 [20204/20928 ( 97%)], Train Loss: 0.19428\n",
            "Epoch: 01 [20244/20928 ( 97%)], Train Loss: 0.19421\n",
            "Epoch: 01 [20284/20928 ( 97%)], Train Loss: 0.19419\n",
            "Epoch: 01 [20324/20928 ( 97%)], Train Loss: 0.19412\n",
            "Epoch: 01 [20364/20928 ( 97%)], Train Loss: 0.19431\n",
            "Epoch: 01 [20404/20928 ( 97%)], Train Loss: 0.19417\n",
            "Epoch: 01 [20444/20928 ( 98%)], Train Loss: 0.19398\n",
            "Epoch: 01 [20484/20928 ( 98%)], Train Loss: 0.19399\n",
            "Epoch: 01 [20524/20928 ( 98%)], Train Loss: 0.19400\n",
            "Epoch: 01 [20564/20928 ( 98%)], Train Loss: 0.19409\n",
            "Epoch: 01 [20604/20928 ( 98%)], Train Loss: 0.19411\n",
            "Epoch: 01 [20644/20928 ( 99%)], Train Loss: 0.19415\n",
            "Epoch: 01 [20684/20928 ( 99%)], Train Loss: 0.19400\n",
            "Epoch: 01 [20724/20928 ( 99%)], Train Loss: 0.19398\n",
            "Epoch: 01 [20764/20928 ( 99%)], Train Loss: 0.19366\n",
            "Epoch: 01 [20804/20928 ( 99%)], Train Loss: 0.19342\n",
            "Epoch: 01 [20844/20928 (100%)], Train Loss: 0.19320\n",
            "Epoch: 01 [20884/20928 (100%)], Train Loss: 0.19293\n",
            "Epoch: 01 [20924/20928 (100%)], Train Loss: 0.19282\n",
            "Epoch: 01 [20928/20928 (100%)], Train Loss: 0.19278\n",
            "----Validation Results Summary----\n",
            "Epoch: [1] Valid Loss: 0.34344\n",
            "\n",
            "Total Training Time: 6149.612866640091secs, Average Training Time per Epoch: 3074.8064333200455secs.\n",
            "Total Validation Time: 238.14197492599487secs, Average Validation Time per Epoch: 119.07098746299744secs.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "FOLD: 2\n",
            "--------------------------------------------------\n",
            "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
            "Num examples Train= 20602, Num examples Valid=2994\n",
            "Total Training Steps: 5152, Total Warmup Steps: 515\n",
            "Epoch: 00 [    4/20602 (  0%)], Train Loss: 4.83856\n",
            "Epoch: 00 [   44/20602 (  0%)], Train Loss: 3.87797\n",
            "Epoch: 00 [   84/20602 (  0%)], Train Loss: 3.87801\n",
            "Epoch: 00 [  124/20602 (  1%)], Train Loss: 3.85926\n",
            "Epoch: 00 [  164/20602 (  1%)], Train Loss: 3.71853\n",
            "Epoch: 00 [  204/20602 (  1%)], Train Loss: 3.55530\n",
            "Epoch: 00 [  244/20602 (  1%)], Train Loss: 3.46562\n",
            "Epoch: 00 [  284/20602 (  1%)], Train Loss: 3.34304\n",
            "Epoch: 00 [  324/20602 (  2%)], Train Loss: 3.22711\n",
            "Epoch: 00 [  364/20602 (  2%)], Train Loss: 3.13003\n",
            "Epoch: 00 [  404/20602 (  2%)], Train Loss: 2.97631\n",
            "Epoch: 00 [  444/20602 (  2%)], Train Loss: 2.84029\n",
            "Epoch: 00 [  484/20602 (  2%)], Train Loss: 2.69109\n",
            "Epoch: 00 [  524/20602 (  3%)], Train Loss: 2.58556\n",
            "Epoch: 00 [  564/20602 (  3%)], Train Loss: 2.49023\n",
            "Epoch: 00 [  604/20602 (  3%)], Train Loss: 2.39360\n",
            "Epoch: 00 [  644/20602 (  3%)], Train Loss: 2.32435\n",
            "Epoch: 00 [  684/20602 (  3%)], Train Loss: 2.27076\n",
            "Epoch: 00 [  724/20602 (  4%)], Train Loss: 2.21840\n",
            "Epoch: 00 [  764/20602 (  4%)], Train Loss: 2.15417\n",
            "Epoch: 00 [  804/20602 (  4%)], Train Loss: 2.07946\n",
            "Epoch: 00 [  844/20602 (  4%)], Train Loss: 2.03762\n",
            "Epoch: 00 [  884/20602 (  4%)], Train Loss: 1.98065\n",
            "Epoch: 00 [  924/20602 (  4%)], Train Loss: 1.91823\n",
            "Epoch: 00 [  964/20602 (  5%)], Train Loss: 1.85838\n",
            "Epoch: 00 [ 1004/20602 (  5%)], Train Loss: 1.81564\n",
            "Epoch: 00 [ 1044/20602 (  5%)], Train Loss: 1.77631\n",
            "Epoch: 00 [ 1084/20602 (  5%)], Train Loss: 1.73249\n",
            "Epoch: 00 [ 1124/20602 (  5%)], Train Loss: 1.69675\n",
            "Epoch: 00 [ 1164/20602 (  6%)], Train Loss: 1.66439\n",
            "Epoch: 00 [ 1204/20602 (  6%)], Train Loss: 1.62824\n",
            "Epoch: 00 [ 1244/20602 (  6%)], Train Loss: 1.60462\n",
            "Epoch: 00 [ 1284/20602 (  6%)], Train Loss: 1.57872\n",
            "Epoch: 00 [ 1324/20602 (  6%)], Train Loss: 1.55250\n",
            "Epoch: 00 [ 1364/20602 (  7%)], Train Loss: 1.52107\n",
            "Epoch: 00 [ 1404/20602 (  7%)], Train Loss: 1.49625\n",
            "Epoch: 00 [ 1444/20602 (  7%)], Train Loss: 1.47648\n",
            "Epoch: 00 [ 1484/20602 (  7%)], Train Loss: 1.45248\n",
            "Epoch: 00 [ 1524/20602 (  7%)], Train Loss: 1.42336\n",
            "Epoch: 00 [ 1564/20602 (  8%)], Train Loss: 1.40448\n",
            "Epoch: 00 [ 1604/20602 (  8%)], Train Loss: 1.38273\n",
            "Epoch: 00 [ 1644/20602 (  8%)], Train Loss: 1.36102\n",
            "Epoch: 00 [ 1684/20602 (  8%)], Train Loss: 1.35162\n",
            "Epoch: 00 [ 1724/20602 (  8%)], Train Loss: 1.33401\n",
            "Epoch: 00 [ 1764/20602 (  9%)], Train Loss: 1.31501\n",
            "Epoch: 00 [ 1804/20602 (  9%)], Train Loss: 1.29889\n",
            "Epoch: 00 [ 1844/20602 (  9%)], Train Loss: 1.27830\n",
            "Epoch: 00 [ 1884/20602 (  9%)], Train Loss: 1.26619\n",
            "Epoch: 00 [ 1924/20602 (  9%)], Train Loss: 1.25005\n",
            "Epoch: 00 [ 1964/20602 ( 10%)], Train Loss: 1.23103\n",
            "Epoch: 00 [ 2004/20602 ( 10%)], Train Loss: 1.21895\n",
            "Epoch: 00 [ 2044/20602 ( 10%)], Train Loss: 1.21034\n",
            "Epoch: 00 [ 2084/20602 ( 10%)], Train Loss: 1.19627\n",
            "Epoch: 00 [ 2124/20602 ( 10%)], Train Loss: 1.18120\n",
            "Epoch: 00 [ 2164/20602 ( 11%)], Train Loss: 1.17706\n",
            "Epoch: 00 [ 2204/20602 ( 11%)], Train Loss: 1.16481\n",
            "Epoch: 00 [ 2244/20602 ( 11%)], Train Loss: 1.15115\n",
            "Epoch: 00 [ 2284/20602 ( 11%)], Train Loss: 1.14020\n",
            "Epoch: 00 [ 2324/20602 ( 11%)], Train Loss: 1.13247\n",
            "Epoch: 00 [ 2364/20602 ( 11%)], Train Loss: 1.12025\n",
            "Epoch: 00 [ 2404/20602 ( 12%)], Train Loss: 1.11161\n",
            "Epoch: 00 [ 2444/20602 ( 12%)], Train Loss: 1.10339\n",
            "Epoch: 00 [ 2484/20602 ( 12%)], Train Loss: 1.09238\n",
            "Epoch: 00 [ 2524/20602 ( 12%)], Train Loss: 1.08498\n",
            "Epoch: 00 [ 2564/20602 ( 12%)], Train Loss: 1.07981\n",
            "Epoch: 00 [ 2604/20602 ( 13%)], Train Loss: 1.07620\n",
            "Epoch: 00 [ 2644/20602 ( 13%)], Train Loss: 1.07367\n",
            "Epoch: 00 [ 2684/20602 ( 13%)], Train Loss: 1.06721\n",
            "Epoch: 00 [ 2724/20602 ( 13%)], Train Loss: 1.06048\n",
            "Epoch: 00 [ 2764/20602 ( 13%)], Train Loss: 1.05406\n",
            "Epoch: 00 [ 2804/20602 ( 14%)], Train Loss: 1.04529\n",
            "Epoch: 00 [ 2844/20602 ( 14%)], Train Loss: 1.04109\n",
            "Epoch: 00 [ 2884/20602 ( 14%)], Train Loss: 1.03392\n",
            "Epoch: 00 [ 2924/20602 ( 14%)], Train Loss: 1.03122\n",
            "Epoch: 00 [ 2964/20602 ( 14%)], Train Loss: 1.02722\n",
            "Epoch: 00 [ 3004/20602 ( 15%)], Train Loss: 1.01877\n",
            "Epoch: 00 [ 3044/20602 ( 15%)], Train Loss: 1.01277\n",
            "Epoch: 00 [ 3084/20602 ( 15%)], Train Loss: 1.00927\n",
            "Epoch: 00 [ 3124/20602 ( 15%)], Train Loss: 1.00692\n",
            "Epoch: 00 [ 3164/20602 ( 15%)], Train Loss: 0.99983\n",
            "Epoch: 00 [ 3204/20602 ( 16%)], Train Loss: 0.99619\n",
            "Epoch: 00 [ 3244/20602 ( 16%)], Train Loss: 0.99001\n",
            "Epoch: 00 [ 3284/20602 ( 16%)], Train Loss: 0.98290\n",
            "Epoch: 00 [ 3324/20602 ( 16%)], Train Loss: 0.97718\n",
            "Epoch: 00 [ 3364/20602 ( 16%)], Train Loss: 0.97565\n",
            "Epoch: 00 [ 3404/20602 ( 17%)], Train Loss: 0.97261\n",
            "Epoch: 00 [ 3444/20602 ( 17%)], Train Loss: 0.96722\n",
            "Epoch: 00 [ 3484/20602 ( 17%)], Train Loss: 0.96204\n",
            "Epoch: 00 [ 3524/20602 ( 17%)], Train Loss: 0.95566\n",
            "Epoch: 00 [ 3564/20602 ( 17%)], Train Loss: 0.95154\n",
            "Epoch: 00 [ 3604/20602 ( 17%)], Train Loss: 0.95084\n",
            "Epoch: 00 [ 3644/20602 ( 18%)], Train Loss: 0.94440\n",
            "Epoch: 00 [ 3684/20602 ( 18%)], Train Loss: 0.93754\n",
            "Epoch: 00 [ 3724/20602 ( 18%)], Train Loss: 0.93387\n",
            "Epoch: 00 [ 3764/20602 ( 18%)], Train Loss: 0.92891\n",
            "Epoch: 00 [ 3804/20602 ( 18%)], Train Loss: 0.92301\n",
            "Epoch: 00 [ 3844/20602 ( 19%)], Train Loss: 0.92007\n",
            "Epoch: 00 [ 3884/20602 ( 19%)], Train Loss: 0.91505\n",
            "Epoch: 00 [ 3924/20602 ( 19%)], Train Loss: 0.91273\n",
            "Epoch: 00 [ 3964/20602 ( 19%)], Train Loss: 0.90839\n",
            "Epoch: 00 [ 4004/20602 ( 19%)], Train Loss: 0.90669\n",
            "Epoch: 00 [ 4044/20602 ( 20%)], Train Loss: 0.90296\n",
            "Epoch: 00 [ 4084/20602 ( 20%)], Train Loss: 0.89964\n",
            "Epoch: 00 [ 4124/20602 ( 20%)], Train Loss: 0.89575\n",
            "Epoch: 00 [ 4164/20602 ( 20%)], Train Loss: 0.89280\n",
            "Epoch: 00 [ 4204/20602 ( 20%)], Train Loss: 0.89132\n",
            "Epoch: 00 [ 4244/20602 ( 21%)], Train Loss: 0.88936\n",
            "Epoch: 00 [ 4284/20602 ( 21%)], Train Loss: 0.88708\n",
            "Epoch: 00 [ 4324/20602 ( 21%)], Train Loss: 0.88315\n",
            "Epoch: 00 [ 4364/20602 ( 21%)], Train Loss: 0.88015\n",
            "Epoch: 00 [ 4404/20602 ( 21%)], Train Loss: 0.87646\n",
            "Epoch: 00 [ 4444/20602 ( 22%)], Train Loss: 0.87516\n",
            "Epoch: 00 [ 4484/20602 ( 22%)], Train Loss: 0.87154\n",
            "Epoch: 00 [ 4524/20602 ( 22%)], Train Loss: 0.86784\n",
            "Epoch: 00 [ 4564/20602 ( 22%)], Train Loss: 0.86499\n",
            "Epoch: 00 [ 4604/20602 ( 22%)], Train Loss: 0.86335\n",
            "Epoch: 00 [ 4644/20602 ( 23%)], Train Loss: 0.86056\n",
            "Epoch: 00 [ 4684/20602 ( 23%)], Train Loss: 0.85858\n",
            "Epoch: 00 [ 4724/20602 ( 23%)], Train Loss: 0.85427\n",
            "Epoch: 00 [ 4764/20602 ( 23%)], Train Loss: 0.85248\n",
            "Epoch: 00 [ 4804/20602 ( 23%)], Train Loss: 0.85029\n",
            "Epoch: 00 [ 4844/20602 ( 24%)], Train Loss: 0.84878\n",
            "Epoch: 00 [ 4884/20602 ( 24%)], Train Loss: 0.84779\n",
            "Epoch: 00 [ 4924/20602 ( 24%)], Train Loss: 0.84447\n",
            "Epoch: 00 [ 4964/20602 ( 24%)], Train Loss: 0.84484\n",
            "Epoch: 00 [ 5004/20602 ( 24%)], Train Loss: 0.84583\n",
            "Epoch: 00 [ 5044/20602 ( 24%)], Train Loss: 0.84507\n",
            "Epoch: 00 [ 5084/20602 ( 25%)], Train Loss: 0.84199\n",
            "Epoch: 00 [ 5124/20602 ( 25%)], Train Loss: 0.83805\n",
            "Epoch: 00 [ 5164/20602 ( 25%)], Train Loss: 0.83312\n",
            "Epoch: 00 [ 5204/20602 ( 25%)], Train Loss: 0.83203\n",
            "Epoch: 00 [ 5244/20602 ( 25%)], Train Loss: 0.82924\n",
            "Epoch: 00 [ 5284/20602 ( 26%)], Train Loss: 0.82577\n",
            "Epoch: 00 [ 5324/20602 ( 26%)], Train Loss: 0.82241\n",
            "Epoch: 00 [ 5364/20602 ( 26%)], Train Loss: 0.82114\n",
            "Epoch: 00 [ 5404/20602 ( 26%)], Train Loss: 0.81676\n",
            "Epoch: 00 [ 5444/20602 ( 26%)], Train Loss: 0.81534\n",
            "Epoch: 00 [ 5484/20602 ( 27%)], Train Loss: 0.81214\n",
            "Epoch: 00 [ 5524/20602 ( 27%)], Train Loss: 0.81233\n",
            "Epoch: 00 [ 5564/20602 ( 27%)], Train Loss: 0.80939\n",
            "Epoch: 00 [ 5604/20602 ( 27%)], Train Loss: 0.80654\n",
            "Epoch: 00 [ 5644/20602 ( 27%)], Train Loss: 0.80483\n",
            "Epoch: 00 [ 5684/20602 ( 28%)], Train Loss: 0.80304\n",
            "Epoch: 00 [ 5724/20602 ( 28%)], Train Loss: 0.80342\n",
            "Epoch: 00 [ 5764/20602 ( 28%)], Train Loss: 0.80106\n",
            "Epoch: 00 [ 5804/20602 ( 28%)], Train Loss: 0.79976\n",
            "Epoch: 00 [ 5844/20602 ( 28%)], Train Loss: 0.79801\n",
            "Epoch: 00 [ 5884/20602 ( 29%)], Train Loss: 0.79507\n",
            "Epoch: 00 [ 5924/20602 ( 29%)], Train Loss: 0.79425\n",
            "Epoch: 00 [ 5964/20602 ( 29%)], Train Loss: 0.79186\n",
            "Epoch: 00 [ 6004/20602 ( 29%)], Train Loss: 0.78887\n",
            "Epoch: 00 [ 6044/20602 ( 29%)], Train Loss: 0.78813\n",
            "Epoch: 00 [ 6084/20602 ( 30%)], Train Loss: 0.78871\n",
            "Epoch: 00 [ 6124/20602 ( 30%)], Train Loss: 0.78584\n",
            "Epoch: 00 [ 6164/20602 ( 30%)], Train Loss: 0.78319\n",
            "Epoch: 00 [ 6204/20602 ( 30%)], Train Loss: 0.78119\n",
            "Epoch: 00 [ 6244/20602 ( 30%)], Train Loss: 0.77922\n",
            "Epoch: 00 [ 6284/20602 ( 31%)], Train Loss: 0.77758\n",
            "Epoch: 00 [ 6324/20602 ( 31%)], Train Loss: 0.77624\n",
            "Epoch: 00 [ 6364/20602 ( 31%)], Train Loss: 0.77459\n",
            "Epoch: 00 [ 6404/20602 ( 31%)], Train Loss: 0.77386\n",
            "Epoch: 00 [ 6444/20602 ( 31%)], Train Loss: 0.77247\n",
            "Epoch: 00 [ 6484/20602 ( 31%)], Train Loss: 0.76942\n",
            "Epoch: 00 [ 6524/20602 ( 32%)], Train Loss: 0.76912\n",
            "Epoch: 00 [ 6564/20602 ( 32%)], Train Loss: 0.76750\n",
            "Epoch: 00 [ 6604/20602 ( 32%)], Train Loss: 0.76456\n",
            "Epoch: 00 [ 6644/20602 ( 32%)], Train Loss: 0.76387\n",
            "Epoch: 00 [ 6684/20602 ( 32%)], Train Loss: 0.76335\n",
            "Epoch: 00 [ 6724/20602 ( 33%)], Train Loss: 0.76239\n",
            "Epoch: 00 [ 6764/20602 ( 33%)], Train Loss: 0.76335\n",
            "Epoch: 00 [ 6804/20602 ( 33%)], Train Loss: 0.76095\n",
            "Epoch: 00 [ 6844/20602 ( 33%)], Train Loss: 0.75977\n",
            "Epoch: 00 [ 6884/20602 ( 33%)], Train Loss: 0.75755\n",
            "Epoch: 00 [ 6924/20602 ( 34%)], Train Loss: 0.75770\n",
            "Epoch: 00 [ 6964/20602 ( 34%)], Train Loss: 0.75571\n",
            "Epoch: 00 [ 7004/20602 ( 34%)], Train Loss: 0.75427\n",
            "Epoch: 00 [ 7044/20602 ( 34%)], Train Loss: 0.75275\n",
            "Epoch: 00 [ 7084/20602 ( 34%)], Train Loss: 0.75112\n",
            "Epoch: 00 [ 7124/20602 ( 35%)], Train Loss: 0.75025\n",
            "Epoch: 00 [ 7164/20602 ( 35%)], Train Loss: 0.75077\n",
            "Epoch: 00 [ 7204/20602 ( 35%)], Train Loss: 0.74981\n",
            "Epoch: 00 [ 7244/20602 ( 35%)], Train Loss: 0.74864\n",
            "Epoch: 00 [ 7284/20602 ( 35%)], Train Loss: 0.74660\n",
            "Epoch: 00 [ 7324/20602 ( 36%)], Train Loss: 0.74481\n",
            "Epoch: 00 [ 7364/20602 ( 36%)], Train Loss: 0.74443\n",
            "Epoch: 00 [ 7404/20602 ( 36%)], Train Loss: 0.74291\n",
            "Epoch: 00 [ 7444/20602 ( 36%)], Train Loss: 0.74135\n",
            "Epoch: 00 [ 7484/20602 ( 36%)], Train Loss: 0.73958\n",
            "Epoch: 00 [ 7524/20602 ( 37%)], Train Loss: 0.73881\n",
            "Epoch: 00 [ 7564/20602 ( 37%)], Train Loss: 0.73751\n",
            "Epoch: 00 [ 7604/20602 ( 37%)], Train Loss: 0.73566\n",
            "Epoch: 00 [ 7644/20602 ( 37%)], Train Loss: 0.73366\n",
            "Epoch: 00 [ 7684/20602 ( 37%)], Train Loss: 0.73243\n",
            "Epoch: 00 [ 7724/20602 ( 37%)], Train Loss: 0.73071\n",
            "Epoch: 00 [ 7764/20602 ( 38%)], Train Loss: 0.72926\n",
            "Epoch: 00 [ 7804/20602 ( 38%)], Train Loss: 0.72764\n",
            "Epoch: 00 [ 7844/20602 ( 38%)], Train Loss: 0.72752\n",
            "Epoch: 00 [ 7884/20602 ( 38%)], Train Loss: 0.72549\n",
            "Epoch: 00 [ 7924/20602 ( 38%)], Train Loss: 0.72442\n",
            "Epoch: 00 [ 7964/20602 ( 39%)], Train Loss: 0.72301\n",
            "Epoch: 00 [ 8004/20602 ( 39%)], Train Loss: 0.72178\n",
            "Epoch: 00 [ 8044/20602 ( 39%)], Train Loss: 0.72196\n",
            "Epoch: 00 [ 8084/20602 ( 39%)], Train Loss: 0.72152\n",
            "Epoch: 00 [ 8124/20602 ( 39%)], Train Loss: 0.72145\n",
            "Epoch: 00 [ 8164/20602 ( 40%)], Train Loss: 0.72008\n",
            "Epoch: 00 [ 8204/20602 ( 40%)], Train Loss: 0.71936\n",
            "Epoch: 00 [ 8244/20602 ( 40%)], Train Loss: 0.71823\n",
            "Epoch: 00 [ 8284/20602 ( 40%)], Train Loss: 0.71654\n",
            "Epoch: 00 [ 8324/20602 ( 40%)], Train Loss: 0.71417\n",
            "Epoch: 00 [ 8364/20602 ( 41%)], Train Loss: 0.71269\n",
            "Epoch: 00 [ 8404/20602 ( 41%)], Train Loss: 0.71152\n",
            "Epoch: 00 [ 8444/20602 ( 41%)], Train Loss: 0.71069\n",
            "Epoch: 00 [ 8484/20602 ( 41%)], Train Loss: 0.71019\n",
            "Epoch: 00 [ 8524/20602 ( 41%)], Train Loss: 0.70896\n",
            "Epoch: 00 [ 8564/20602 ( 42%)], Train Loss: 0.70795\n",
            "Epoch: 00 [ 8604/20602 ( 42%)], Train Loss: 0.70654\n",
            "Epoch: 00 [ 8644/20602 ( 42%)], Train Loss: 0.70532\n",
            "Epoch: 00 [ 8684/20602 ( 42%)], Train Loss: 0.70412\n",
            "Epoch: 00 [ 8724/20602 ( 42%)], Train Loss: 0.70380\n",
            "Epoch: 00 [ 8764/20602 ( 43%)], Train Loss: 0.70357\n",
            "Epoch: 00 [ 8804/20602 ( 43%)], Train Loss: 0.70395\n",
            "Epoch: 00 [ 8844/20602 ( 43%)], Train Loss: 0.70241\n",
            "Epoch: 00 [ 8884/20602 ( 43%)], Train Loss: 0.70209\n",
            "Epoch: 00 [ 8924/20602 ( 43%)], Train Loss: 0.70127\n",
            "Epoch: 00 [ 8964/20602 ( 44%)], Train Loss: 0.69932\n",
            "Epoch: 00 [ 9004/20602 ( 44%)], Train Loss: 0.69884\n",
            "Epoch: 00 [ 9044/20602 ( 44%)], Train Loss: 0.69786\n",
            "Epoch: 00 [ 9084/20602 ( 44%)], Train Loss: 0.69660\n",
            "Epoch: 00 [ 9124/20602 ( 44%)], Train Loss: 0.69654\n",
            "Epoch: 00 [ 9164/20602 ( 44%)], Train Loss: 0.69553\n",
            "Epoch: 00 [ 9204/20602 ( 45%)], Train Loss: 0.69432\n",
            "Epoch: 00 [ 9244/20602 ( 45%)], Train Loss: 0.69216\n",
            "Epoch: 00 [ 9284/20602 ( 45%)], Train Loss: 0.69042\n",
            "Epoch: 00 [ 9324/20602 ( 45%)], Train Loss: 0.68920\n",
            "Epoch: 00 [ 9364/20602 ( 45%)], Train Loss: 0.68734\n",
            "Epoch: 00 [ 9404/20602 ( 46%)], Train Loss: 0.68656\n",
            "Epoch: 00 [ 9444/20602 ( 46%)], Train Loss: 0.68486\n",
            "Epoch: 00 [ 9484/20602 ( 46%)], Train Loss: 0.68522\n",
            "Epoch: 00 [ 9524/20602 ( 46%)], Train Loss: 0.68484\n",
            "Epoch: 00 [ 9564/20602 ( 46%)], Train Loss: 0.68371\n",
            "Epoch: 00 [ 9604/20602 ( 47%)], Train Loss: 0.68254\n",
            "Epoch: 00 [ 9644/20602 ( 47%)], Train Loss: 0.68264\n",
            "Epoch: 00 [ 9684/20602 ( 47%)], Train Loss: 0.68229\n",
            "Epoch: 00 [ 9724/20602 ( 47%)], Train Loss: 0.68119\n",
            "Epoch: 00 [ 9764/20602 ( 47%)], Train Loss: 0.68047\n",
            "Epoch: 00 [ 9804/20602 ( 48%)], Train Loss: 0.67956\n",
            "Epoch: 00 [ 9844/20602 ( 48%)], Train Loss: 0.67878\n",
            "Epoch: 00 [ 9884/20602 ( 48%)], Train Loss: 0.67884\n",
            "Epoch: 00 [ 9924/20602 ( 48%)], Train Loss: 0.67804\n",
            "Epoch: 00 [ 9964/20602 ( 48%)], Train Loss: 0.67705\n",
            "Epoch: 00 [10004/20602 ( 49%)], Train Loss: 0.67699\n",
            "Epoch: 00 [10044/20602 ( 49%)], Train Loss: 0.67626\n",
            "Epoch: 00 [10084/20602 ( 49%)], Train Loss: 0.67576\n",
            "Epoch: 00 [10124/20602 ( 49%)], Train Loss: 0.67693\n",
            "Epoch: 00 [10164/20602 ( 49%)], Train Loss: 0.67611\n",
            "Epoch: 00 [10204/20602 ( 50%)], Train Loss: 0.67474\n",
            "Epoch: 00 [10244/20602 ( 50%)], Train Loss: 0.67450\n",
            "Epoch: 00 [10284/20602 ( 50%)], Train Loss: 0.67341\n",
            "Epoch: 00 [10324/20602 ( 50%)], Train Loss: 0.67386\n",
            "Epoch: 00 [10364/20602 ( 50%)], Train Loss: 0.67237\n",
            "Epoch: 00 [10404/20602 ( 50%)], Train Loss: 0.67210\n",
            "Epoch: 00 [10444/20602 ( 51%)], Train Loss: 0.67126\n",
            "Epoch: 00 [10484/20602 ( 51%)], Train Loss: 0.67136\n",
            "Epoch: 00 [10524/20602 ( 51%)], Train Loss: 0.67086\n",
            "Epoch: 00 [10564/20602 ( 51%)], Train Loss: 0.66984\n",
            "Epoch: 00 [10604/20602 ( 51%)], Train Loss: 0.67004\n",
            "Epoch: 00 [10644/20602 ( 52%)], Train Loss: 0.66892\n",
            "Epoch: 00 [10684/20602 ( 52%)], Train Loss: 0.66788\n",
            "Epoch: 00 [10724/20602 ( 52%)], Train Loss: 0.66845\n",
            "Epoch: 00 [10764/20602 ( 52%)], Train Loss: 0.66843\n",
            "Epoch: 00 [10804/20602 ( 52%)], Train Loss: 0.66798\n",
            "Epoch: 00 [10844/20602 ( 53%)], Train Loss: 0.66772\n",
            "Epoch: 00 [10884/20602 ( 53%)], Train Loss: 0.66654\n",
            "Epoch: 00 [10924/20602 ( 53%)], Train Loss: 0.66566\n",
            "Epoch: 00 [10964/20602 ( 53%)], Train Loss: 0.66591\n",
            "Epoch: 00 [11004/20602 ( 53%)], Train Loss: 0.66561\n",
            "Epoch: 00 [11044/20602 ( 54%)], Train Loss: 0.66514\n",
            "Epoch: 00 [11084/20602 ( 54%)], Train Loss: 0.66433\n",
            "Epoch: 00 [11124/20602 ( 54%)], Train Loss: 0.66393\n",
            "Epoch: 00 [11164/20602 ( 54%)], Train Loss: 0.66352\n",
            "Epoch: 00 [11204/20602 ( 54%)], Train Loss: 0.66228\n",
            "Epoch: 00 [11244/20602 ( 55%)], Train Loss: 0.66104\n",
            "Epoch: 00 [11284/20602 ( 55%)], Train Loss: 0.66151\n",
            "Epoch: 00 [11324/20602 ( 55%)], Train Loss: 0.66080\n",
            "Epoch: 00 [11364/20602 ( 55%)], Train Loss: 0.65988\n",
            "Epoch: 00 [11404/20602 ( 55%)], Train Loss: 0.66006\n",
            "Epoch: 00 [11444/20602 ( 56%)], Train Loss: 0.65923\n",
            "Epoch: 00 [11484/20602 ( 56%)], Train Loss: 0.65801\n",
            "Epoch: 00 [11524/20602 ( 56%)], Train Loss: 0.65811\n",
            "Epoch: 00 [11564/20602 ( 56%)], Train Loss: 0.65810\n",
            "Epoch: 00 [11604/20602 ( 56%)], Train Loss: 0.65721\n",
            "Epoch: 00 [11644/20602 ( 57%)], Train Loss: 0.65687\n",
            "Epoch: 00 [11684/20602 ( 57%)], Train Loss: 0.65599\n",
            "Epoch: 00 [11724/20602 ( 57%)], Train Loss: 0.65553\n",
            "Epoch: 00 [11764/20602 ( 57%)], Train Loss: 0.65495\n",
            "Epoch: 00 [11804/20602 ( 57%)], Train Loss: 0.65446\n",
            "Epoch: 00 [11844/20602 ( 57%)], Train Loss: 0.65316\n",
            "Epoch: 00 [11884/20602 ( 58%)], Train Loss: 0.65216\n",
            "Epoch: 00 [11924/20602 ( 58%)], Train Loss: 0.65164\n",
            "Epoch: 00 [11964/20602 ( 58%)], Train Loss: 0.65023\n",
            "Epoch: 00 [12004/20602 ( 58%)], Train Loss: 0.64891\n",
            "Epoch: 00 [12044/20602 ( 58%)], Train Loss: 0.64790\n",
            "Epoch: 00 [12084/20602 ( 59%)], Train Loss: 0.64686\n",
            "Epoch: 00 [12124/20602 ( 59%)], Train Loss: 0.64617\n",
            "Epoch: 00 [12164/20602 ( 59%)], Train Loss: 0.64504\n",
            "Epoch: 00 [12204/20602 ( 59%)], Train Loss: 0.64411\n",
            "Epoch: 00 [12244/20602 ( 59%)], Train Loss: 0.64481\n",
            "Epoch: 00 [12284/20602 ( 60%)], Train Loss: 0.64388\n",
            "Epoch: 00 [12324/20602 ( 60%)], Train Loss: 0.64322\n",
            "Epoch: 00 [12364/20602 ( 60%)], Train Loss: 0.64249\n",
            "Epoch: 00 [12404/20602 ( 60%)], Train Loss: 0.64202\n",
            "Epoch: 00 [12444/20602 ( 60%)], Train Loss: 0.64109\n",
            "Epoch: 00 [12484/20602 ( 61%)], Train Loss: 0.64014\n",
            "Epoch: 00 [12524/20602 ( 61%)], Train Loss: 0.64013\n",
            "Epoch: 00 [12564/20602 ( 61%)], Train Loss: 0.63982\n",
            "Epoch: 00 [12604/20602 ( 61%)], Train Loss: 0.63910\n",
            "Epoch: 00 [12644/20602 ( 61%)], Train Loss: 0.63852\n",
            "Epoch: 00 [12684/20602 ( 62%)], Train Loss: 0.63747\n",
            "Epoch: 00 [12724/20602 ( 62%)], Train Loss: 0.63638\n",
            "Epoch: 00 [12764/20602 ( 62%)], Train Loss: 0.63577\n",
            "Epoch: 00 [12804/20602 ( 62%)], Train Loss: 0.63621\n",
            "Epoch: 00 [12844/20602 ( 62%)], Train Loss: 0.63584\n",
            "Epoch: 00 [12884/20602 ( 63%)], Train Loss: 0.63501\n",
            "Epoch: 00 [12924/20602 ( 63%)], Train Loss: 0.63448\n",
            "Epoch: 00 [12964/20602 ( 63%)], Train Loss: 0.63365\n",
            "Epoch: 00 [13004/20602 ( 63%)], Train Loss: 0.63313\n",
            "Epoch: 00 [13044/20602 ( 63%)], Train Loss: 0.63302\n",
            "Epoch: 00 [13084/20602 ( 64%)], Train Loss: 0.63294\n",
            "Epoch: 00 [13124/20602 ( 64%)], Train Loss: 0.63206\n",
            "Epoch: 00 [13164/20602 ( 64%)], Train Loss: 0.63170\n",
            "Epoch: 00 [13204/20602 ( 64%)], Train Loss: 0.63103\n",
            "Epoch: 00 [13244/20602 ( 64%)], Train Loss: 0.63082\n",
            "Epoch: 00 [13284/20602 ( 64%)], Train Loss: 0.63035\n",
            "Epoch: 00 [13324/20602 ( 65%)], Train Loss: 0.62989\n",
            "Epoch: 00 [13364/20602 ( 65%)], Train Loss: 0.62974\n",
            "Epoch: 00 [13404/20602 ( 65%)], Train Loss: 0.62868\n",
            "Epoch: 00 [13444/20602 ( 65%)], Train Loss: 0.62777\n",
            "Epoch: 00 [13484/20602 ( 65%)], Train Loss: 0.62747\n",
            "Epoch: 00 [13524/20602 ( 66%)], Train Loss: 0.62682\n",
            "Epoch: 00 [13564/20602 ( 66%)], Train Loss: 0.62698\n",
            "Epoch: 00 [13604/20602 ( 66%)], Train Loss: 0.62616\n",
            "Epoch: 00 [13644/20602 ( 66%)], Train Loss: 0.62545\n",
            "Epoch: 00 [13684/20602 ( 66%)], Train Loss: 0.62504\n",
            "Epoch: 00 [13724/20602 ( 67%)], Train Loss: 0.62427\n",
            "Epoch: 00 [13764/20602 ( 67%)], Train Loss: 0.62388\n",
            "Epoch: 00 [13804/20602 ( 67%)], Train Loss: 0.62316\n",
            "Epoch: 00 [13844/20602 ( 67%)], Train Loss: 0.62230\n",
            "Epoch: 00 [13884/20602 ( 67%)], Train Loss: 0.62181\n",
            "Epoch: 00 [13924/20602 ( 68%)], Train Loss: 0.62111\n",
            "Epoch: 00 [13964/20602 ( 68%)], Train Loss: 0.62027\n",
            "Epoch: 00 [14004/20602 ( 68%)], Train Loss: 0.61924\n",
            "Epoch: 00 [14044/20602 ( 68%)], Train Loss: 0.61844\n",
            "Epoch: 00 [14084/20602 ( 68%)], Train Loss: 0.61820\n",
            "Epoch: 00 [14124/20602 ( 69%)], Train Loss: 0.61763\n",
            "Epoch: 00 [14164/20602 ( 69%)], Train Loss: 0.61781\n",
            "Epoch: 00 [14204/20602 ( 69%)], Train Loss: 0.61686\n",
            "Epoch: 00 [14244/20602 ( 69%)], Train Loss: 0.61683\n",
            "Epoch: 00 [14284/20602 ( 69%)], Train Loss: 0.61714\n",
            "Epoch: 00 [14324/20602 ( 70%)], Train Loss: 0.61627\n",
            "Epoch: 00 [14364/20602 ( 70%)], Train Loss: 0.61554\n",
            "Epoch: 00 [14404/20602 ( 70%)], Train Loss: 0.61553\n",
            "Epoch: 00 [14444/20602 ( 70%)], Train Loss: 0.61518\n",
            "Epoch: 00 [14484/20602 ( 70%)], Train Loss: 0.61486\n",
            "Epoch: 00 [14524/20602 ( 70%)], Train Loss: 0.61470\n",
            "Epoch: 00 [14564/20602 ( 71%)], Train Loss: 0.61451\n",
            "Epoch: 00 [14604/20602 ( 71%)], Train Loss: 0.61380\n",
            "Epoch: 00 [14644/20602 ( 71%)], Train Loss: 0.61328\n",
            "Epoch: 00 [14684/20602 ( 71%)], Train Loss: 0.61291\n",
            "Epoch: 00 [14724/20602 ( 71%)], Train Loss: 0.61207\n",
            "Epoch: 00 [14764/20602 ( 72%)], Train Loss: 0.61178\n",
            "Epoch: 00 [14804/20602 ( 72%)], Train Loss: 0.61112\n",
            "Epoch: 00 [14844/20602 ( 72%)], Train Loss: 0.61029\n",
            "Epoch: 00 [14884/20602 ( 72%)], Train Loss: 0.60987\n",
            "Epoch: 00 [14924/20602 ( 72%)], Train Loss: 0.60928\n",
            "Epoch: 00 [14964/20602 ( 73%)], Train Loss: 0.60888\n",
            "Epoch: 00 [15004/20602 ( 73%)], Train Loss: 0.60805\n",
            "Epoch: 00 [15044/20602 ( 73%)], Train Loss: 0.60783\n",
            "Epoch: 00 [15084/20602 ( 73%)], Train Loss: 0.60704\n",
            "Epoch: 00 [15124/20602 ( 73%)], Train Loss: 0.60704\n",
            "Epoch: 00 [15164/20602 ( 74%)], Train Loss: 0.60754\n",
            "Epoch: 00 [15204/20602 ( 74%)], Train Loss: 0.60691\n",
            "Epoch: 00 [15244/20602 ( 74%)], Train Loss: 0.60643\n",
            "Epoch: 00 [15284/20602 ( 74%)], Train Loss: 0.60603\n",
            "Epoch: 00 [15324/20602 ( 74%)], Train Loss: 0.60577\n",
            "Epoch: 00 [15364/20602 ( 75%)], Train Loss: 0.60526\n",
            "Epoch: 00 [15404/20602 ( 75%)], Train Loss: 0.60449\n",
            "Epoch: 00 [15444/20602 ( 75%)], Train Loss: 0.60468\n",
            "Epoch: 00 [15484/20602 ( 75%)], Train Loss: 0.60448\n",
            "Epoch: 00 [15524/20602 ( 75%)], Train Loss: 0.60389\n",
            "Epoch: 00 [15564/20602 ( 76%)], Train Loss: 0.60369\n",
            "Epoch: 00 [15604/20602 ( 76%)], Train Loss: 0.60302\n",
            "Epoch: 00 [15644/20602 ( 76%)], Train Loss: 0.60203\n",
            "Epoch: 00 [15684/20602 ( 76%)], Train Loss: 0.60196\n",
            "Epoch: 00 [15724/20602 ( 76%)], Train Loss: 0.60186\n",
            "Epoch: 00 [15764/20602 ( 77%)], Train Loss: 0.60150\n",
            "Epoch: 00 [15804/20602 ( 77%)], Train Loss: 0.60168\n",
            "Epoch: 00 [15844/20602 ( 77%)], Train Loss: 0.60118\n",
            "Epoch: 00 [15884/20602 ( 77%)], Train Loss: 0.60112\n",
            "Epoch: 00 [15924/20602 ( 77%)], Train Loss: 0.60080\n",
            "Epoch: 00 [15964/20602 ( 77%)], Train Loss: 0.60010\n",
            "Epoch: 00 [16004/20602 ( 78%)], Train Loss: 0.59958\n",
            "Epoch: 00 [16044/20602 ( 78%)], Train Loss: 0.59964\n",
            "Epoch: 00 [16084/20602 ( 78%)], Train Loss: 0.59973\n",
            "Epoch: 00 [16124/20602 ( 78%)], Train Loss: 0.59893\n",
            "Epoch: 00 [16164/20602 ( 78%)], Train Loss: 0.59875\n",
            "Epoch: 00 [16204/20602 ( 79%)], Train Loss: 0.59812\n",
            "Epoch: 00 [16244/20602 ( 79%)], Train Loss: 0.59803\n",
            "Epoch: 00 [16284/20602 ( 79%)], Train Loss: 0.59751\n",
            "Epoch: 00 [16324/20602 ( 79%)], Train Loss: 0.59790\n",
            "Epoch: 00 [16364/20602 ( 79%)], Train Loss: 0.59720\n",
            "Epoch: 00 [16404/20602 ( 80%)], Train Loss: 0.59635\n",
            "Epoch: 00 [16444/20602 ( 80%)], Train Loss: 0.59550\n",
            "Epoch: 00 [16484/20602 ( 80%)], Train Loss: 0.59519\n",
            "Epoch: 00 [16524/20602 ( 80%)], Train Loss: 0.59552\n",
            "Epoch: 00 [16564/20602 ( 80%)], Train Loss: 0.59545\n",
            "Epoch: 00 [16604/20602 ( 81%)], Train Loss: 0.59526\n",
            "Epoch: 00 [16644/20602 ( 81%)], Train Loss: 0.59568\n",
            "Epoch: 00 [16684/20602 ( 81%)], Train Loss: 0.59557\n",
            "Epoch: 00 [16724/20602 ( 81%)], Train Loss: 0.59492\n",
            "Epoch: 00 [16764/20602 ( 81%)], Train Loss: 0.59433\n",
            "Epoch: 00 [16804/20602 ( 82%)], Train Loss: 0.59452\n",
            "Epoch: 00 [16844/20602 ( 82%)], Train Loss: 0.59421\n",
            "Epoch: 00 [16884/20602 ( 82%)], Train Loss: 0.59398\n",
            "Epoch: 00 [16924/20602 ( 82%)], Train Loss: 0.59421\n",
            "Epoch: 00 [16964/20602 ( 82%)], Train Loss: 0.59352\n",
            "Epoch: 00 [17004/20602 ( 83%)], Train Loss: 0.59332\n",
            "Epoch: 00 [17044/20602 ( 83%)], Train Loss: 0.59327\n",
            "Epoch: 00 [17084/20602 ( 83%)], Train Loss: 0.59269\n",
            "Epoch: 00 [17124/20602 ( 83%)], Train Loss: 0.59222\n",
            "Epoch: 00 [17164/20602 ( 83%)], Train Loss: 0.59179\n",
            "Epoch: 00 [17204/20602 ( 84%)], Train Loss: 0.59152\n",
            "Epoch: 00 [17244/20602 ( 84%)], Train Loss: 0.59087\n",
            "Epoch: 00 [17284/20602 ( 84%)], Train Loss: 0.59044\n",
            "Epoch: 00 [17324/20602 ( 84%)], Train Loss: 0.59002\n",
            "Epoch: 00 [17364/20602 ( 84%)], Train Loss: 0.58958\n",
            "Epoch: 00 [17404/20602 ( 84%)], Train Loss: 0.58902\n",
            "Epoch: 00 [17444/20602 ( 85%)], Train Loss: 0.58879\n",
            "Epoch: 00 [17484/20602 ( 85%)], Train Loss: 0.58794\n",
            "Epoch: 00 [17524/20602 ( 85%)], Train Loss: 0.58786\n",
            "Epoch: 00 [17564/20602 ( 85%)], Train Loss: 0.58783\n",
            "Epoch: 00 [17604/20602 ( 85%)], Train Loss: 0.58725\n",
            "Epoch: 00 [17644/20602 ( 86%)], Train Loss: 0.58685\n",
            "Epoch: 00 [17684/20602 ( 86%)], Train Loss: 0.58596\n",
            "Epoch: 00 [17724/20602 ( 86%)], Train Loss: 0.58550\n",
            "Epoch: 00 [17764/20602 ( 86%)], Train Loss: 0.58471\n",
            "Epoch: 00 [17804/20602 ( 86%)], Train Loss: 0.58505\n",
            "Epoch: 00 [17844/20602 ( 87%)], Train Loss: 0.58493\n",
            "Epoch: 00 [17884/20602 ( 87%)], Train Loss: 0.58434\n",
            "Epoch: 00 [17924/20602 ( 87%)], Train Loss: 0.58395\n",
            "Epoch: 00 [17964/20602 ( 87%)], Train Loss: 0.58402\n",
            "Epoch: 00 [18004/20602 ( 87%)], Train Loss: 0.58385\n",
            "Epoch: 00 [18044/20602 ( 88%)], Train Loss: 0.58389\n",
            "Epoch: 00 [18084/20602 ( 88%)], Train Loss: 0.58385\n",
            "Epoch: 00 [18124/20602 ( 88%)], Train Loss: 0.58365\n",
            "Epoch: 00 [18164/20602 ( 88%)], Train Loss: 0.58324\n",
            "Epoch: 00 [18204/20602 ( 88%)], Train Loss: 0.58285\n",
            "Epoch: 00 [18244/20602 ( 89%)], Train Loss: 0.58246\n",
            "Epoch: 00 [18284/20602 ( 89%)], Train Loss: 0.58193\n",
            "Epoch: 00 [18324/20602 ( 89%)], Train Loss: 0.58130\n",
            "Epoch: 00 [18364/20602 ( 89%)], Train Loss: 0.58137\n",
            "Epoch: 00 [18404/20602 ( 89%)], Train Loss: 0.58198\n",
            "Epoch: 00 [18444/20602 ( 90%)], Train Loss: 0.58191\n",
            "Epoch: 00 [18484/20602 ( 90%)], Train Loss: 0.58115\n",
            "Epoch: 00 [18524/20602 ( 90%)], Train Loss: 0.58066\n",
            "Epoch: 00 [18564/20602 ( 90%)], Train Loss: 0.58024\n",
            "Epoch: 00 [18604/20602 ( 90%)], Train Loss: 0.58035\n",
            "Epoch: 00 [18644/20602 ( 90%)], Train Loss: 0.58010\n",
            "Epoch: 00 [18684/20602 ( 91%)], Train Loss: 0.57934\n",
            "Epoch: 00 [18724/20602 ( 91%)], Train Loss: 0.57864\n",
            "Epoch: 00 [18764/20602 ( 91%)], Train Loss: 0.57797\n",
            "Epoch: 00 [18804/20602 ( 91%)], Train Loss: 0.57741\n",
            "Epoch: 00 [18844/20602 ( 91%)], Train Loss: 0.57718\n",
            "Epoch: 00 [18884/20602 ( 92%)], Train Loss: 0.57664\n",
            "Epoch: 00 [18924/20602 ( 92%)], Train Loss: 0.57665\n",
            "Epoch: 00 [18964/20602 ( 92%)], Train Loss: 0.57657\n",
            "Epoch: 00 [19004/20602 ( 92%)], Train Loss: 0.57678\n",
            "Epoch: 00 [19044/20602 ( 92%)], Train Loss: 0.57693\n",
            "Epoch: 00 [19084/20602 ( 93%)], Train Loss: 0.57731\n",
            "Epoch: 00 [19124/20602 ( 93%)], Train Loss: 0.57721\n",
            "Epoch: 00 [19164/20602 ( 93%)], Train Loss: 0.57689\n",
            "Epoch: 00 [19204/20602 ( 93%)], Train Loss: 0.57649\n",
            "Epoch: 00 [19244/20602 ( 93%)], Train Loss: 0.57615\n",
            "Epoch: 00 [19284/20602 ( 94%)], Train Loss: 0.57594\n",
            "Epoch: 00 [19324/20602 ( 94%)], Train Loss: 0.57599\n",
            "Epoch: 00 [19364/20602 ( 94%)], Train Loss: 0.57584\n",
            "Epoch: 00 [19404/20602 ( 94%)], Train Loss: 0.57571\n",
            "Epoch: 00 [19444/20602 ( 94%)], Train Loss: 0.57550\n",
            "Epoch: 00 [19484/20602 ( 95%)], Train Loss: 0.57539\n",
            "Epoch: 00 [19524/20602 ( 95%)], Train Loss: 0.57486\n",
            "Epoch: 00 [19564/20602 ( 95%)], Train Loss: 0.57493\n",
            "Epoch: 00 [19604/20602 ( 95%)], Train Loss: 0.57443\n",
            "Epoch: 00 [19644/20602 ( 95%)], Train Loss: 0.57401\n",
            "Epoch: 00 [19684/20602 ( 96%)], Train Loss: 0.57377\n",
            "Epoch: 00 [19724/20602 ( 96%)], Train Loss: 0.57348\n",
            "Epoch: 00 [19764/20602 ( 96%)], Train Loss: 0.57343\n",
            "Epoch: 00 [19804/20602 ( 96%)], Train Loss: 0.57272\n",
            "Epoch: 00 [19844/20602 ( 96%)], Train Loss: 0.57209\n",
            "Epoch: 00 [19884/20602 ( 97%)], Train Loss: 0.57206\n",
            "Epoch: 00 [19924/20602 ( 97%)], Train Loss: 0.57149\n",
            "Epoch: 00 [19964/20602 ( 97%)], Train Loss: 0.57102\n",
            "Epoch: 00 [20004/20602 ( 97%)], Train Loss: 0.57127\n",
            "Epoch: 00 [20044/20602 ( 97%)], Train Loss: 0.57107\n",
            "Epoch: 00 [20084/20602 ( 97%)], Train Loss: 0.57072\n",
            "Epoch: 00 [20124/20602 ( 98%)], Train Loss: 0.57074\n",
            "Epoch: 00 [20164/20602 ( 98%)], Train Loss: 0.57056\n",
            "Epoch: 00 [20204/20602 ( 98%)], Train Loss: 0.57033\n",
            "Epoch: 00 [20244/20602 ( 98%)], Train Loss: 0.57017\n",
            "Epoch: 00 [20284/20602 ( 98%)], Train Loss: 0.56979\n",
            "Epoch: 00 [20324/20602 ( 99%)], Train Loss: 0.56932\n",
            "Epoch: 00 [20364/20602 ( 99%)], Train Loss: 0.56872\n",
            "Epoch: 00 [20404/20602 ( 99%)], Train Loss: 0.56855\n",
            "Epoch: 00 [20444/20602 ( 99%)], Train Loss: 0.56853\n",
            "Epoch: 00 [20484/20602 ( 99%)], Train Loss: 0.56787\n",
            "Epoch: 00 [20524/20602 (100%)], Train Loss: 0.56717\n",
            "Epoch: 00 [20564/20602 (100%)], Train Loss: 0.56686\n",
            "Epoch: 00 [20602/20602 (100%)], Train Loss: 0.56662\n",
            "----Validation Results Summary----\n",
            "Epoch: [0] Valid Loss: 0.22518\n",
            "0 Epoch, Best epoch was updated! Valid Loss: 0.22518\n",
            "Saving model checkpoint to output/checkpoint-fold-2.\n",
            "\n",
            "Epoch: 01 [    4/20602 (  0%)], Train Loss: 0.42173\n",
            "Epoch: 01 [   44/20602 (  0%)], Train Loss: 0.43560\n",
            "Epoch: 01 [   84/20602 (  0%)], Train Loss: 0.40568\n",
            "Epoch: 01 [  124/20602 (  1%)], Train Loss: 0.38932\n",
            "Epoch: 01 [  164/20602 (  1%)], Train Loss: 0.36708\n",
            "Epoch: 01 [  204/20602 (  1%)], Train Loss: 0.41213\n",
            "Epoch: 01 [  244/20602 (  1%)], Train Loss: 0.42135\n",
            "Epoch: 01 [  284/20602 (  1%)], Train Loss: 0.43429\n",
            "Epoch: 01 [  324/20602 (  2%)], Train Loss: 0.44448\n",
            "Epoch: 01 [  364/20602 (  2%)], Train Loss: 0.45383\n",
            "Epoch: 01 [  404/20602 (  2%)], Train Loss: 0.44076\n",
            "Epoch: 01 [  444/20602 (  2%)], Train Loss: 0.44080\n",
            "Epoch: 01 [  484/20602 (  2%)], Train Loss: 0.42188\n",
            "Epoch: 01 [  524/20602 (  3%)], Train Loss: 0.42764\n",
            "Epoch: 01 [  564/20602 (  3%)], Train Loss: 0.44139\n",
            "Epoch: 01 [  604/20602 (  3%)], Train Loss: 0.44004\n",
            "Epoch: 01 [  644/20602 (  3%)], Train Loss: 0.44501\n",
            "Epoch: 01 [  684/20602 (  3%)], Train Loss: 0.44530\n",
            "Epoch: 01 [  724/20602 (  4%)], Train Loss: 0.45392\n",
            "Epoch: 01 [  764/20602 (  4%)], Train Loss: 0.45030\n",
            "Epoch: 01 [  804/20602 (  4%)], Train Loss: 0.44184\n",
            "Epoch: 01 [  844/20602 (  4%)], Train Loss: 0.45355\n",
            "Epoch: 01 [  884/20602 (  4%)], Train Loss: 0.45207\n",
            "Epoch: 01 [  924/20602 (  4%)], Train Loss: 0.44515\n",
            "Epoch: 01 [  964/20602 (  5%)], Train Loss: 0.43546\n",
            "Epoch: 01 [ 1004/20602 (  5%)], Train Loss: 0.43882\n",
            "Epoch: 01 [ 1044/20602 (  5%)], Train Loss: 0.43352\n",
            "Epoch: 01 [ 1084/20602 (  5%)], Train Loss: 0.42567\n",
            "Epoch: 01 [ 1124/20602 (  5%)], Train Loss: 0.42386\n",
            "Epoch: 01 [ 1164/20602 (  6%)], Train Loss: 0.42541\n",
            "Epoch: 01 [ 1204/20602 (  6%)], Train Loss: 0.42183\n",
            "Epoch: 01 [ 1244/20602 (  6%)], Train Loss: 0.42182\n",
            "Epoch: 01 [ 1284/20602 (  6%)], Train Loss: 0.42068\n",
            "Epoch: 01 [ 1324/20602 (  6%)], Train Loss: 0.41909\n",
            "Epoch: 01 [ 1364/20602 (  7%)], Train Loss: 0.41526\n",
            "Epoch: 01 [ 1404/20602 (  7%)], Train Loss: 0.41540\n",
            "Epoch: 01 [ 1444/20602 (  7%)], Train Loss: 0.41165\n",
            "Epoch: 01 [ 1484/20602 (  7%)], Train Loss: 0.40632\n",
            "Epoch: 01 [ 1524/20602 (  7%)], Train Loss: 0.39924\n",
            "Epoch: 01 [ 1564/20602 (  8%)], Train Loss: 0.39668\n",
            "Epoch: 01 [ 1604/20602 (  8%)], Train Loss: 0.39190\n",
            "Epoch: 01 [ 1644/20602 (  8%)], Train Loss: 0.38879\n",
            "Epoch: 01 [ 1684/20602 (  8%)], Train Loss: 0.39171\n",
            "Epoch: 01 [ 1724/20602 (  8%)], Train Loss: 0.38838\n",
            "Epoch: 01 [ 1764/20602 (  9%)], Train Loss: 0.38410\n",
            "Epoch: 01 [ 1804/20602 (  9%)], Train Loss: 0.38147\n",
            "Epoch: 01 [ 1844/20602 (  9%)], Train Loss: 0.37670\n",
            "Epoch: 01 [ 1884/20602 (  9%)], Train Loss: 0.37616\n",
            "Epoch: 01 [ 1924/20602 (  9%)], Train Loss: 0.37191\n",
            "Epoch: 01 [ 1964/20602 ( 10%)], Train Loss: 0.36839\n",
            "Epoch: 01 [ 2004/20602 ( 10%)], Train Loss: 0.36906\n",
            "Epoch: 01 [ 2044/20602 ( 10%)], Train Loss: 0.36955\n",
            "Epoch: 01 [ 2084/20602 ( 10%)], Train Loss: 0.36923\n",
            "Epoch: 01 [ 2124/20602 ( 10%)], Train Loss: 0.36633\n",
            "Epoch: 01 [ 2164/20602 ( 11%)], Train Loss: 0.37197\n",
            "Epoch: 01 [ 2204/20602 ( 11%)], Train Loss: 0.37089\n",
            "Epoch: 01 [ 2244/20602 ( 11%)], Train Loss: 0.36915\n",
            "Epoch: 01 [ 2284/20602 ( 11%)], Train Loss: 0.36752\n",
            "Epoch: 01 [ 2324/20602 ( 11%)], Train Loss: 0.36768\n",
            "Epoch: 01 [ 2364/20602 ( 11%)], Train Loss: 0.36424\n",
            "Epoch: 01 [ 2404/20602 ( 12%)], Train Loss: 0.36217\n",
            "Epoch: 01 [ 2444/20602 ( 12%)], Train Loss: 0.36056\n",
            "Epoch: 01 [ 2484/20602 ( 12%)], Train Loss: 0.35726\n",
            "Epoch: 01 [ 2524/20602 ( 12%)], Train Loss: 0.35675\n",
            "Epoch: 01 [ 2564/20602 ( 12%)], Train Loss: 0.35885\n",
            "Epoch: 01 [ 2604/20602 ( 13%)], Train Loss: 0.35967\n",
            "Epoch: 01 [ 2644/20602 ( 13%)], Train Loss: 0.36125\n",
            "Epoch: 01 [ 2684/20602 ( 13%)], Train Loss: 0.35930\n",
            "Epoch: 01 [ 2724/20602 ( 13%)], Train Loss: 0.35850\n",
            "Epoch: 01 [ 2764/20602 ( 13%)], Train Loss: 0.35668\n",
            "Epoch: 01 [ 2804/20602 ( 14%)], Train Loss: 0.35528\n",
            "Epoch: 01 [ 2844/20602 ( 14%)], Train Loss: 0.35454\n",
            "Epoch: 01 [ 2884/20602 ( 14%)], Train Loss: 0.35161\n",
            "Epoch: 01 [ 2924/20602 ( 14%)], Train Loss: 0.35138\n",
            "Epoch: 01 [ 2964/20602 ( 14%)], Train Loss: 0.35329\n",
            "Epoch: 01 [ 3004/20602 ( 15%)], Train Loss: 0.35045\n",
            "Epoch: 01 [ 3044/20602 ( 15%)], Train Loss: 0.35033\n",
            "Epoch: 01 [ 3084/20602 ( 15%)], Train Loss: 0.35046\n",
            "Epoch: 01 [ 3124/20602 ( 15%)], Train Loss: 0.35004\n",
            "Epoch: 01 [ 3164/20602 ( 15%)], Train Loss: 0.34702\n",
            "Epoch: 01 [ 3204/20602 ( 16%)], Train Loss: 0.34719\n",
            "Epoch: 01 [ 3244/20602 ( 16%)], Train Loss: 0.34563\n",
            "Epoch: 01 [ 3284/20602 ( 16%)], Train Loss: 0.34342\n",
            "Epoch: 01 [ 3324/20602 ( 16%)], Train Loss: 0.34149\n",
            "Epoch: 01 [ 3364/20602 ( 16%)], Train Loss: 0.34063\n",
            "Epoch: 01 [ 3404/20602 ( 17%)], Train Loss: 0.33902\n",
            "Epoch: 01 [ 3444/20602 ( 17%)], Train Loss: 0.33746\n",
            "Epoch: 01 [ 3484/20602 ( 17%)], Train Loss: 0.33551\n",
            "Epoch: 01 [ 3524/20602 ( 17%)], Train Loss: 0.33309\n",
            "Epoch: 01 [ 3564/20602 ( 17%)], Train Loss: 0.33142\n",
            "Epoch: 01 [ 3604/20602 ( 17%)], Train Loss: 0.33227\n",
            "Epoch: 01 [ 3644/20602 ( 18%)], Train Loss: 0.33009\n",
            "Epoch: 01 [ 3684/20602 ( 18%)], Train Loss: 0.32788\n",
            "Epoch: 01 [ 3724/20602 ( 18%)], Train Loss: 0.32589\n",
            "Epoch: 01 [ 3764/20602 ( 18%)], Train Loss: 0.32399\n",
            "Epoch: 01 [ 3804/20602 ( 18%)], Train Loss: 0.32151\n",
            "Epoch: 01 [ 3844/20602 ( 19%)], Train Loss: 0.32109\n",
            "Epoch: 01 [ 3884/20602 ( 19%)], Train Loss: 0.31921\n",
            "Epoch: 01 [ 3924/20602 ( 19%)], Train Loss: 0.31841\n",
            "Epoch: 01 [ 3964/20602 ( 19%)], Train Loss: 0.31709\n",
            "Epoch: 01 [ 4004/20602 ( 19%)], Train Loss: 0.31754\n",
            "Epoch: 01 [ 4044/20602 ( 20%)], Train Loss: 0.31626\n",
            "Epoch: 01 [ 4084/20602 ( 20%)], Train Loss: 0.31569\n",
            "Epoch: 01 [ 4124/20602 ( 20%)], Train Loss: 0.31468\n",
            "Epoch: 01 [ 4164/20602 ( 20%)], Train Loss: 0.31328\n",
            "Epoch: 01 [ 4204/20602 ( 20%)], Train Loss: 0.31326\n",
            "Epoch: 01 [ 4244/20602 ( 21%)], Train Loss: 0.31314\n",
            "Epoch: 01 [ 4284/20602 ( 21%)], Train Loss: 0.31291\n",
            "Epoch: 01 [ 4324/20602 ( 21%)], Train Loss: 0.31164\n",
            "Epoch: 01 [ 4364/20602 ( 21%)], Train Loss: 0.31113\n",
            "Epoch: 01 [ 4404/20602 ( 21%)], Train Loss: 0.31040\n",
            "Epoch: 01 [ 4444/20602 ( 22%)], Train Loss: 0.31129\n",
            "Epoch: 01 [ 4484/20602 ( 22%)], Train Loss: 0.30999\n",
            "Epoch: 01 [ 4524/20602 ( 22%)], Train Loss: 0.30858\n",
            "Epoch: 01 [ 4564/20602 ( 22%)], Train Loss: 0.30814\n",
            "Epoch: 01 [ 4604/20602 ( 22%)], Train Loss: 0.30713\n",
            "Epoch: 01 [ 4644/20602 ( 23%)], Train Loss: 0.30588\n",
            "Epoch: 01 [ 4684/20602 ( 23%)], Train Loss: 0.30500\n",
            "Epoch: 01 [ 4724/20602 ( 23%)], Train Loss: 0.30337\n",
            "Epoch: 01 [ 4764/20602 ( 23%)], Train Loss: 0.30276\n",
            "Epoch: 01 [ 4804/20602 ( 23%)], Train Loss: 0.30182\n",
            "Epoch: 01 [ 4844/20602 ( 24%)], Train Loss: 0.30116\n",
            "Epoch: 01 [ 4884/20602 ( 24%)], Train Loss: 0.30142\n",
            "Epoch: 01 [ 4924/20602 ( 24%)], Train Loss: 0.30110\n",
            "Epoch: 01 [ 4964/20602 ( 24%)], Train Loss: 0.30218\n",
            "Epoch: 01 [ 5004/20602 ( 24%)], Train Loss: 0.30350\n",
            "Epoch: 01 [ 5044/20602 ( 24%)], Train Loss: 0.30339\n",
            "Epoch: 01 [ 5084/20602 ( 25%)], Train Loss: 0.30209\n",
            "Epoch: 01 [ 5124/20602 ( 25%)], Train Loss: 0.30079\n",
            "Epoch: 01 [ 5164/20602 ( 25%)], Train Loss: 0.29931\n",
            "Epoch: 01 [ 5204/20602 ( 25%)], Train Loss: 0.29840\n",
            "Epoch: 01 [ 5244/20602 ( 25%)], Train Loss: 0.29721\n",
            "Epoch: 01 [ 5284/20602 ( 26%)], Train Loss: 0.29666\n",
            "Epoch: 01 [ 5324/20602 ( 26%)], Train Loss: 0.29528\n",
            "Epoch: 01 [ 5364/20602 ( 26%)], Train Loss: 0.29523\n",
            "Epoch: 01 [ 5404/20602 ( 26%)], Train Loss: 0.29329\n",
            "Epoch: 01 [ 5444/20602 ( 26%)], Train Loss: 0.29307\n",
            "Epoch: 01 [ 5484/20602 ( 27%)], Train Loss: 0.29191\n",
            "Epoch: 01 [ 5524/20602 ( 27%)], Train Loss: 0.29159\n",
            "Epoch: 01 [ 5564/20602 ( 27%)], Train Loss: 0.29094\n",
            "Epoch: 01 [ 5604/20602 ( 27%)], Train Loss: 0.28992\n",
            "Epoch: 01 [ 5644/20602 ( 27%)], Train Loss: 0.28932\n",
            "Epoch: 01 [ 5684/20602 ( 28%)], Train Loss: 0.28867\n",
            "Epoch: 01 [ 5724/20602 ( 28%)], Train Loss: 0.28908\n",
            "Epoch: 01 [ 5764/20602 ( 28%)], Train Loss: 0.28861\n",
            "Epoch: 01 [ 5804/20602 ( 28%)], Train Loss: 0.28843\n",
            "Epoch: 01 [ 5844/20602 ( 28%)], Train Loss: 0.28798\n",
            "Epoch: 01 [ 5884/20602 ( 29%)], Train Loss: 0.28745\n",
            "Epoch: 01 [ 5924/20602 ( 29%)], Train Loss: 0.28699\n",
            "Epoch: 01 [ 5964/20602 ( 29%)], Train Loss: 0.28633\n",
            "Epoch: 01 [ 6004/20602 ( 29%)], Train Loss: 0.28500\n",
            "Epoch: 01 [ 6044/20602 ( 29%)], Train Loss: 0.28469\n",
            "Epoch: 01 [ 6084/20602 ( 30%)], Train Loss: 0.28520\n",
            "Epoch: 01 [ 6124/20602 ( 30%)], Train Loss: 0.28397\n",
            "Epoch: 01 [ 6164/20602 ( 30%)], Train Loss: 0.28283\n",
            "Epoch: 01 [ 6204/20602 ( 30%)], Train Loss: 0.28169\n",
            "Epoch: 01 [ 6244/20602 ( 30%)], Train Loss: 0.28083\n",
            "Epoch: 01 [ 6284/20602 ( 31%)], Train Loss: 0.28008\n",
            "Epoch: 01 [ 6324/20602 ( 31%)], Train Loss: 0.27970\n",
            "Epoch: 01 [ 6364/20602 ( 31%)], Train Loss: 0.27939\n",
            "Epoch: 01 [ 6404/20602 ( 31%)], Train Loss: 0.27915\n",
            "Epoch: 01 [ 6444/20602 ( 31%)], Train Loss: 0.27900\n",
            "Epoch: 01 [ 6484/20602 ( 31%)], Train Loss: 0.27807\n",
            "Epoch: 01 [ 6524/20602 ( 32%)], Train Loss: 0.27817\n",
            "Epoch: 01 [ 6564/20602 ( 32%)], Train Loss: 0.27732\n",
            "Epoch: 01 [ 6604/20602 ( 32%)], Train Loss: 0.27644\n",
            "Epoch: 01 [ 6644/20602 ( 32%)], Train Loss: 0.27604\n",
            "Epoch: 01 [ 6684/20602 ( 32%)], Train Loss: 0.27597\n",
            "Epoch: 01 [ 6724/20602 ( 33%)], Train Loss: 0.27545\n",
            "Epoch: 01 [ 6764/20602 ( 33%)], Train Loss: 0.27653\n",
            "Epoch: 01 [ 6804/20602 ( 33%)], Train Loss: 0.27562\n",
            "Epoch: 01 [ 6844/20602 ( 33%)], Train Loss: 0.27517\n",
            "Epoch: 01 [ 6884/20602 ( 33%)], Train Loss: 0.27415\n",
            "Epoch: 01 [ 6924/20602 ( 34%)], Train Loss: 0.27404\n",
            "Epoch: 01 [ 6964/20602 ( 34%)], Train Loss: 0.27338\n",
            "Epoch: 01 [ 7004/20602 ( 34%)], Train Loss: 0.27297\n",
            "Epoch: 01 [ 7044/20602 ( 34%)], Train Loss: 0.27201\n",
            "Epoch: 01 [ 7084/20602 ( 34%)], Train Loss: 0.27161\n",
            "Epoch: 01 [ 7124/20602 ( 35%)], Train Loss: 0.27110\n",
            "Epoch: 01 [ 7164/20602 ( 35%)], Train Loss: 0.27110\n",
            "Epoch: 01 [ 7204/20602 ( 35%)], Train Loss: 0.27101\n",
            "Epoch: 01 [ 7244/20602 ( 35%)], Train Loss: 0.27075\n",
            "Epoch: 01 [ 7284/20602 ( 35%)], Train Loss: 0.27009\n",
            "Epoch: 01 [ 7324/20602 ( 36%)], Train Loss: 0.26916\n",
            "Epoch: 01 [ 7364/20602 ( 36%)], Train Loss: 0.26877\n",
            "Epoch: 01 [ 7404/20602 ( 36%)], Train Loss: 0.26798\n",
            "Epoch: 01 [ 7444/20602 ( 36%)], Train Loss: 0.26748\n",
            "Epoch: 01 [ 7484/20602 ( 36%)], Train Loss: 0.26675\n",
            "Epoch: 01 [ 7524/20602 ( 37%)], Train Loss: 0.26601\n",
            "Epoch: 01 [ 7564/20602 ( 37%)], Train Loss: 0.26640\n",
            "Epoch: 01 [ 7604/20602 ( 37%)], Train Loss: 0.26567\n",
            "Epoch: 01 [ 7644/20602 ( 37%)], Train Loss: 0.26457\n",
            "Epoch: 01 [ 7684/20602 ( 37%)], Train Loss: 0.26437\n",
            "Epoch: 01 [ 7724/20602 ( 37%)], Train Loss: 0.26360\n",
            "Epoch: 01 [ 7764/20602 ( 38%)], Train Loss: 0.26338\n",
            "Epoch: 01 [ 7804/20602 ( 38%)], Train Loss: 0.26268\n",
            "Epoch: 01 [ 7844/20602 ( 38%)], Train Loss: 0.26230\n",
            "Epoch: 01 [ 7884/20602 ( 38%)], Train Loss: 0.26143\n",
            "Epoch: 01 [ 7924/20602 ( 38%)], Train Loss: 0.26077\n",
            "Epoch: 01 [ 7964/20602 ( 39%)], Train Loss: 0.26023\n",
            "Epoch: 01 [ 8004/20602 ( 39%)], Train Loss: 0.25978\n",
            "Epoch: 01 [ 8044/20602 ( 39%)], Train Loss: 0.25969\n",
            "Epoch: 01 [ 8084/20602 ( 39%)], Train Loss: 0.25932\n",
            "Epoch: 01 [ 8124/20602 ( 39%)], Train Loss: 0.25968\n",
            "Epoch: 01 [ 8164/20602 ( 40%)], Train Loss: 0.25911\n",
            "Epoch: 01 [ 8204/20602 ( 40%)], Train Loss: 0.25913\n",
            "Epoch: 01 [ 8244/20602 ( 40%)], Train Loss: 0.25844\n",
            "Epoch: 01 [ 8284/20602 ( 40%)], Train Loss: 0.25769\n",
            "Epoch: 01 [ 8324/20602 ( 40%)], Train Loss: 0.25682\n",
            "Epoch: 01 [ 8364/20602 ( 41%)], Train Loss: 0.25615\n",
            "Epoch: 01 [ 8404/20602 ( 41%)], Train Loss: 0.25547\n",
            "Epoch: 01 [ 8444/20602 ( 41%)], Train Loss: 0.25489\n",
            "Epoch: 01 [ 8484/20602 ( 41%)], Train Loss: 0.25492\n",
            "Epoch: 01 [ 8524/20602 ( 41%)], Train Loss: 0.25408\n",
            "Epoch: 01 [ 8564/20602 ( 42%)], Train Loss: 0.25337\n",
            "Epoch: 01 [ 8604/20602 ( 42%)], Train Loss: 0.25266\n",
            "Epoch: 01 [ 8644/20602 ( 42%)], Train Loss: 0.25201\n",
            "Epoch: 01 [ 8684/20602 ( 42%)], Train Loss: 0.25138\n",
            "Epoch: 01 [ 8724/20602 ( 42%)], Train Loss: 0.25119\n",
            "Epoch: 01 [ 8764/20602 ( 43%)], Train Loss: 0.25111\n",
            "Epoch: 01 [ 8804/20602 ( 43%)], Train Loss: 0.25080\n",
            "Epoch: 01 [ 8844/20602 ( 43%)], Train Loss: 0.25012\n",
            "Epoch: 01 [ 8884/20602 ( 43%)], Train Loss: 0.25103\n",
            "Epoch: 01 [ 8924/20602 ( 43%)], Train Loss: 0.25083\n",
            "Epoch: 01 [ 8964/20602 ( 44%)], Train Loss: 0.24999\n",
            "Epoch: 01 [ 9004/20602 ( 44%)], Train Loss: 0.24957\n",
            "Epoch: 01 [ 9044/20602 ( 44%)], Train Loss: 0.24898\n",
            "Epoch: 01 [ 9084/20602 ( 44%)], Train Loss: 0.24828\n",
            "Epoch: 01 [ 9124/20602 ( 44%)], Train Loss: 0.24818\n",
            "Epoch: 01 [ 9164/20602 ( 44%)], Train Loss: 0.24782\n",
            "Epoch: 01 [ 9204/20602 ( 45%)], Train Loss: 0.24719\n",
            "Epoch: 01 [ 9244/20602 ( 45%)], Train Loss: 0.24637\n",
            "Epoch: 01 [ 9284/20602 ( 45%)], Train Loss: 0.24577\n",
            "Epoch: 01 [ 9324/20602 ( 45%)], Train Loss: 0.24504\n",
            "Epoch: 01 [ 9364/20602 ( 45%)], Train Loss: 0.24429\n",
            "Epoch: 01 [ 9404/20602 ( 46%)], Train Loss: 0.24398\n",
            "Epoch: 01 [ 9444/20602 ( 46%)], Train Loss: 0.24355\n",
            "Epoch: 01 [ 9484/20602 ( 46%)], Train Loss: 0.24360\n",
            "Epoch: 01 [ 9524/20602 ( 46%)], Train Loss: 0.24341\n",
            "Epoch: 01 [ 9564/20602 ( 46%)], Train Loss: 0.24279\n",
            "Epoch: 01 [ 9604/20602 ( 47%)], Train Loss: 0.24257\n",
            "Epoch: 01 [ 9644/20602 ( 47%)], Train Loss: 0.24267\n",
            "Epoch: 01 [ 9684/20602 ( 47%)], Train Loss: 0.24248\n",
            "Epoch: 01 [ 9724/20602 ( 47%)], Train Loss: 0.24181\n",
            "Epoch: 01 [ 9764/20602 ( 47%)], Train Loss: 0.24146\n",
            "Epoch: 01 [ 9804/20602 ( 48%)], Train Loss: 0.24097\n",
            "Epoch: 01 [ 9844/20602 ( 48%)], Train Loss: 0.24047\n",
            "Epoch: 01 [ 9884/20602 ( 48%)], Train Loss: 0.24074\n",
            "Epoch: 01 [ 9924/20602 ( 48%)], Train Loss: 0.24036\n",
            "Epoch: 01 [ 9964/20602 ( 48%)], Train Loss: 0.23979\n",
            "Epoch: 01 [10004/20602 ( 49%)], Train Loss: 0.23972\n",
            "Epoch: 01 [10044/20602 ( 49%)], Train Loss: 0.23927\n",
            "Epoch: 01 [10084/20602 ( 49%)], Train Loss: 0.23884\n",
            "Epoch: 01 [10124/20602 ( 49%)], Train Loss: 0.23870\n",
            "Epoch: 01 [10164/20602 ( 49%)], Train Loss: 0.23866\n",
            "Epoch: 01 [10204/20602 ( 50%)], Train Loss: 0.23782\n",
            "Epoch: 01 [10244/20602 ( 50%)], Train Loss: 0.23765\n",
            "Epoch: 01 [10284/20602 ( 50%)], Train Loss: 0.23721\n",
            "Epoch: 01 [10324/20602 ( 50%)], Train Loss: 0.23732\n",
            "Epoch: 01 [10364/20602 ( 50%)], Train Loss: 0.23670\n",
            "Epoch: 01 [10404/20602 ( 50%)], Train Loss: 0.23663\n",
            "Epoch: 01 [10444/20602 ( 51%)], Train Loss: 0.23634\n",
            "Epoch: 01 [10484/20602 ( 51%)], Train Loss: 0.23633\n",
            "Epoch: 01 [10524/20602 ( 51%)], Train Loss: 0.23627\n",
            "Epoch: 01 [10564/20602 ( 51%)], Train Loss: 0.23580\n",
            "Epoch: 01 [10604/20602 ( 51%)], Train Loss: 0.23640\n",
            "Epoch: 01 [10644/20602 ( 52%)], Train Loss: 0.23606\n",
            "Epoch: 01 [10684/20602 ( 52%)], Train Loss: 0.23577\n",
            "Epoch: 01 [10724/20602 ( 52%)], Train Loss: 0.23579\n",
            "Epoch: 01 [10764/20602 ( 52%)], Train Loss: 0.23598\n",
            "Epoch: 01 [10804/20602 ( 52%)], Train Loss: 0.23582\n",
            "Epoch: 01 [10844/20602 ( 53%)], Train Loss: 0.23556\n",
            "Epoch: 01 [10884/20602 ( 53%)], Train Loss: 0.23502\n",
            "Epoch: 01 [10924/20602 ( 53%)], Train Loss: 0.23449\n",
            "Epoch: 01 [10964/20602 ( 53%)], Train Loss: 0.23475\n",
            "Epoch: 01 [11004/20602 ( 53%)], Train Loss: 0.23462\n",
            "Epoch: 01 [11044/20602 ( 54%)], Train Loss: 0.23446\n",
            "Epoch: 01 [11084/20602 ( 54%)], Train Loss: 0.23395\n",
            "Epoch: 01 [11124/20602 ( 54%)], Train Loss: 0.23393\n",
            "Epoch: 01 [11164/20602 ( 54%)], Train Loss: 0.23383\n",
            "Epoch: 01 [11204/20602 ( 54%)], Train Loss: 0.23311\n",
            "Epoch: 01 [11244/20602 ( 55%)], Train Loss: 0.23277\n",
            "Epoch: 01 [11284/20602 ( 55%)], Train Loss: 0.23281\n",
            "Epoch: 01 [11324/20602 ( 55%)], Train Loss: 0.23243\n",
            "Epoch: 01 [11364/20602 ( 55%)], Train Loss: 0.23219\n",
            "Epoch: 01 [11404/20602 ( 55%)], Train Loss: 0.23230\n",
            "Epoch: 01 [11444/20602 ( 56%)], Train Loss: 0.23190\n",
            "Epoch: 01 [11484/20602 ( 56%)], Train Loss: 0.23130\n",
            "Epoch: 01 [11524/20602 ( 56%)], Train Loss: 0.23137\n",
            "Epoch: 01 [11564/20602 ( 56%)], Train Loss: 0.23169\n",
            "Epoch: 01 [11604/20602 ( 56%)], Train Loss: 0.23147\n",
            "Epoch: 01 [11644/20602 ( 57%)], Train Loss: 0.23114\n",
            "Epoch: 01 [11684/20602 ( 57%)], Train Loss: 0.23058\n",
            "Epoch: 01 [11724/20602 ( 57%)], Train Loss: 0.23031\n",
            "Epoch: 01 [11764/20602 ( 57%)], Train Loss: 0.22994\n",
            "Epoch: 01 [11804/20602 ( 57%)], Train Loss: 0.22984\n",
            "Epoch: 01 [11844/20602 ( 57%)], Train Loss: 0.22939\n",
            "Epoch: 01 [11884/20602 ( 58%)], Train Loss: 0.22911\n",
            "Epoch: 01 [11924/20602 ( 58%)], Train Loss: 0.22919\n",
            "Epoch: 01 [11964/20602 ( 58%)], Train Loss: 0.22866\n",
            "Epoch: 01 [12004/20602 ( 58%)], Train Loss: 0.22805\n",
            "Epoch: 01 [12044/20602 ( 58%)], Train Loss: 0.22747\n",
            "Epoch: 01 [12084/20602 ( 59%)], Train Loss: 0.22702\n",
            "Epoch: 01 [12124/20602 ( 59%)], Train Loss: 0.22667\n",
            "Epoch: 01 [12164/20602 ( 59%)], Train Loss: 0.22621\n",
            "Epoch: 01 [12204/20602 ( 59%)], Train Loss: 0.22593\n",
            "Epoch: 01 [12244/20602 ( 59%)], Train Loss: 0.22608\n",
            "Epoch: 01 [12284/20602 ( 60%)], Train Loss: 0.22563\n",
            "Epoch: 01 [12324/20602 ( 60%)], Train Loss: 0.22559\n",
            "Epoch: 01 [12364/20602 ( 60%)], Train Loss: 0.22519\n",
            "Epoch: 01 [12404/20602 ( 60%)], Train Loss: 0.22493\n",
            "Epoch: 01 [12444/20602 ( 60%)], Train Loss: 0.22445\n",
            "Epoch: 01 [12484/20602 ( 61%)], Train Loss: 0.22404\n",
            "Epoch: 01 [12524/20602 ( 61%)], Train Loss: 0.22420\n",
            "Epoch: 01 [12564/20602 ( 61%)], Train Loss: 0.22373\n",
            "Epoch: 01 [12604/20602 ( 61%)], Train Loss: 0.22331\n",
            "Epoch: 01 [12644/20602 ( 61%)], Train Loss: 0.22299\n",
            "Epoch: 01 [12684/20602 ( 62%)], Train Loss: 0.22260\n",
            "Epoch: 01 [12724/20602 ( 62%)], Train Loss: 0.22215\n",
            "Epoch: 01 [12764/20602 ( 62%)], Train Loss: 0.22171\n",
            "Epoch: 01 [12804/20602 ( 62%)], Train Loss: 0.22200\n",
            "Epoch: 01 [12844/20602 ( 62%)], Train Loss: 0.22172\n",
            "Epoch: 01 [12884/20602 ( 63%)], Train Loss: 0.22133\n",
            "Epoch: 01 [12924/20602 ( 63%)], Train Loss: 0.22104\n",
            "Epoch: 01 [12964/20602 ( 63%)], Train Loss: 0.22076\n",
            "Epoch: 01 [13004/20602 ( 63%)], Train Loss: 0.22055\n",
            "Epoch: 01 [13044/20602 ( 63%)], Train Loss: 0.22062\n",
            "Epoch: 01 [13084/20602 ( 64%)], Train Loss: 0.22046\n",
            "Epoch: 01 [13124/20602 ( 64%)], Train Loss: 0.22001\n",
            "Epoch: 01 [13164/20602 ( 64%)], Train Loss: 0.21976\n",
            "Epoch: 01 [13204/20602 ( 64%)], Train Loss: 0.21954\n",
            "Epoch: 01 [13244/20602 ( 64%)], Train Loss: 0.21962\n",
            "Epoch: 01 [13284/20602 ( 64%)], Train Loss: 0.21928\n",
            "Epoch: 01 [13324/20602 ( 65%)], Train Loss: 0.21910\n",
            "Epoch: 01 [13364/20602 ( 65%)], Train Loss: 0.21917\n",
            "Epoch: 01 [13404/20602 ( 65%)], Train Loss: 0.21898\n",
            "Epoch: 01 [13444/20602 ( 65%)], Train Loss: 0.21854\n",
            "Epoch: 01 [13484/20602 ( 65%)], Train Loss: 0.21826\n",
            "Epoch: 01 [13524/20602 ( 66%)], Train Loss: 0.21795\n",
            "Epoch: 01 [13564/20602 ( 66%)], Train Loss: 0.21771\n",
            "Epoch: 01 [13604/20602 ( 66%)], Train Loss: 0.21735\n",
            "Epoch: 01 [13644/20602 ( 66%)], Train Loss: 0.21712\n",
            "Epoch: 01 [13684/20602 ( 66%)], Train Loss: 0.21670\n",
            "Epoch: 01 [13724/20602 ( 67%)], Train Loss: 0.21632\n",
            "Epoch: 01 [13764/20602 ( 67%)], Train Loss: 0.21649\n",
            "Epoch: 01 [13804/20602 ( 67%)], Train Loss: 0.21612\n",
            "Epoch: 01 [13844/20602 ( 67%)], Train Loss: 0.21576\n",
            "Epoch: 01 [13884/20602 ( 67%)], Train Loss: 0.21537\n",
            "Epoch: 01 [13924/20602 ( 68%)], Train Loss: 0.21508\n",
            "Epoch: 01 [13964/20602 ( 68%)], Train Loss: 0.21479\n",
            "Epoch: 01 [14004/20602 ( 68%)], Train Loss: 0.21447\n",
            "Epoch: 01 [14044/20602 ( 68%)], Train Loss: 0.21421\n",
            "Epoch: 01 [14084/20602 ( 68%)], Train Loss: 0.21391\n",
            "Epoch: 01 [14124/20602 ( 69%)], Train Loss: 0.21374\n",
            "Epoch: 01 [14164/20602 ( 69%)], Train Loss: 0.21375\n",
            "Epoch: 01 [14204/20602 ( 69%)], Train Loss: 0.21329\n",
            "Epoch: 01 [14244/20602 ( 69%)], Train Loss: 0.21370\n",
            "Epoch: 01 [14284/20602 ( 69%)], Train Loss: 0.21398\n",
            "Epoch: 01 [14324/20602 ( 70%)], Train Loss: 0.21369\n",
            "Epoch: 01 [14364/20602 ( 70%)], Train Loss: 0.21320\n",
            "Epoch: 01 [14404/20602 ( 70%)], Train Loss: 0.21325\n",
            "Epoch: 01 [14444/20602 ( 70%)], Train Loss: 0.21330\n",
            "Epoch: 01 [14484/20602 ( 70%)], Train Loss: 0.21299\n",
            "Epoch: 01 [14524/20602 ( 70%)], Train Loss: 0.21304\n",
            "Epoch: 01 [14564/20602 ( 71%)], Train Loss: 0.21302\n",
            "Epoch: 01 [14604/20602 ( 71%)], Train Loss: 0.21301\n",
            "Epoch: 01 [14644/20602 ( 71%)], Train Loss: 0.21274\n",
            "Epoch: 01 [14684/20602 ( 71%)], Train Loss: 0.21269\n",
            "Epoch: 01 [14724/20602 ( 71%)], Train Loss: 0.21237\n",
            "Epoch: 01 [14764/20602 ( 72%)], Train Loss: 0.21252\n",
            "Epoch: 01 [14804/20602 ( 72%)], Train Loss: 0.21208\n",
            "Epoch: 01 [14844/20602 ( 72%)], Train Loss: 0.21163\n",
            "Epoch: 01 [14884/20602 ( 72%)], Train Loss: 0.21166\n",
            "Epoch: 01 [14924/20602 ( 72%)], Train Loss: 0.21136\n",
            "Epoch: 01 [14964/20602 ( 73%)], Train Loss: 0.21121\n",
            "Epoch: 01 [15004/20602 ( 73%)], Train Loss: 0.21092\n",
            "Epoch: 01 [15044/20602 ( 73%)], Train Loss: 0.21070\n",
            "Epoch: 01 [15084/20602 ( 73%)], Train Loss: 0.21046\n",
            "Epoch: 01 [15124/20602 ( 73%)], Train Loss: 0.21035\n",
            "Epoch: 01 [15164/20602 ( 74%)], Train Loss: 0.21052\n",
            "Epoch: 01 [15204/20602 ( 74%)], Train Loss: 0.21027\n",
            "Epoch: 01 [15244/20602 ( 74%)], Train Loss: 0.20996\n",
            "Epoch: 01 [15284/20602 ( 74%)], Train Loss: 0.20990\n",
            "Epoch: 01 [15324/20602 ( 74%)], Train Loss: 0.20980\n",
            "Epoch: 01 [15364/20602 ( 75%)], Train Loss: 0.20957\n",
            "Epoch: 01 [15404/20602 ( 75%)], Train Loss: 0.20915\n",
            "Epoch: 01 [15444/20602 ( 75%)], Train Loss: 0.20899\n",
            "Epoch: 01 [15484/20602 ( 75%)], Train Loss: 0.20896\n",
            "Epoch: 01 [15524/20602 ( 75%)], Train Loss: 0.20862\n",
            "Epoch: 01 [15564/20602 ( 76%)], Train Loss: 0.20851\n",
            "Epoch: 01 [15604/20602 ( 76%)], Train Loss: 0.20823\n",
            "Epoch: 01 [15644/20602 ( 76%)], Train Loss: 0.20781\n",
            "Epoch: 01 [15684/20602 ( 76%)], Train Loss: 0.20764\n",
            "Epoch: 01 [15724/20602 ( 76%)], Train Loss: 0.20768\n",
            "Epoch: 01 [15764/20602 ( 77%)], Train Loss: 0.20759\n",
            "Epoch: 01 [15804/20602 ( 77%)], Train Loss: 0.20738\n",
            "Epoch: 01 [15844/20602 ( 77%)], Train Loss: 0.20710\n",
            "Epoch: 01 [15884/20602 ( 77%)], Train Loss: 0.20701\n",
            "Epoch: 01 [15924/20602 ( 77%)], Train Loss: 0.20686\n",
            "Epoch: 01 [15964/20602 ( 77%)], Train Loss: 0.20655\n",
            "Epoch: 01 [16004/20602 ( 78%)], Train Loss: 0.20625\n",
            "Epoch: 01 [16044/20602 ( 78%)], Train Loss: 0.20624\n",
            "Epoch: 01 [16084/20602 ( 78%)], Train Loss: 0.20619\n",
            "Epoch: 01 [16124/20602 ( 78%)], Train Loss: 0.20582\n",
            "Epoch: 01 [16164/20602 ( 78%)], Train Loss: 0.20599\n",
            "Epoch: 01 [16204/20602 ( 79%)], Train Loss: 0.20583\n",
            "Epoch: 01 [16244/20602 ( 79%)], Train Loss: 0.20586\n",
            "Epoch: 01 [16284/20602 ( 79%)], Train Loss: 0.20547\n",
            "Epoch: 01 [16324/20602 ( 79%)], Train Loss: 0.20607\n",
            "Epoch: 01 [16364/20602 ( 79%)], Train Loss: 0.20582\n",
            "Epoch: 01 [16404/20602 ( 80%)], Train Loss: 0.20544\n",
            "Epoch: 01 [16444/20602 ( 80%)], Train Loss: 0.20506\n",
            "Epoch: 01 [16484/20602 ( 80%)], Train Loss: 0.20489\n",
            "Epoch: 01 [16524/20602 ( 80%)], Train Loss: 0.20514\n",
            "Epoch: 01 [16564/20602 ( 80%)], Train Loss: 0.20505\n",
            "Epoch: 01 [16604/20602 ( 81%)], Train Loss: 0.20503\n",
            "Epoch: 01 [16644/20602 ( 81%)], Train Loss: 0.20556\n",
            "Epoch: 01 [16684/20602 ( 81%)], Train Loss: 0.20548\n",
            "Epoch: 01 [16724/20602 ( 81%)], Train Loss: 0.20504\n",
            "Epoch: 01 [16764/20602 ( 81%)], Train Loss: 0.20491\n",
            "Epoch: 01 [16804/20602 ( 82%)], Train Loss: 0.20507\n",
            "Epoch: 01 [16844/20602 ( 82%)], Train Loss: 0.20507\n",
            "Epoch: 01 [16884/20602 ( 82%)], Train Loss: 0.20504\n",
            "Epoch: 01 [16924/20602 ( 82%)], Train Loss: 0.20510\n",
            "Epoch: 01 [16964/20602 ( 82%)], Train Loss: 0.20485\n",
            "Epoch: 01 [17004/20602 ( 83%)], Train Loss: 0.20488\n",
            "Epoch: 01 [17044/20602 ( 83%)], Train Loss: 0.20491\n",
            "Epoch: 01 [17084/20602 ( 83%)], Train Loss: 0.20475\n",
            "Epoch: 01 [17124/20602 ( 83%)], Train Loss: 0.20447\n",
            "Epoch: 01 [17164/20602 ( 83%)], Train Loss: 0.20424\n",
            "Epoch: 01 [17204/20602 ( 84%)], Train Loss: 0.20419\n",
            "Epoch: 01 [17244/20602 ( 84%)], Train Loss: 0.20407\n",
            "Epoch: 01 [17284/20602 ( 84%)], Train Loss: 0.20388\n",
            "Epoch: 01 [17324/20602 ( 84%)], Train Loss: 0.20375\n",
            "Epoch: 01 [17364/20602 ( 84%)], Train Loss: 0.20364\n",
            "Epoch: 01 [17404/20602 ( 84%)], Train Loss: 0.20348\n",
            "Epoch: 01 [17444/20602 ( 85%)], Train Loss: 0.20370\n",
            "Epoch: 01 [17484/20602 ( 85%)], Train Loss: 0.20337\n",
            "Epoch: 01 [17524/20602 ( 85%)], Train Loss: 0.20333\n",
            "Epoch: 01 [17564/20602 ( 85%)], Train Loss: 0.20338\n",
            "Epoch: 01 [17604/20602 ( 85%)], Train Loss: 0.20320\n",
            "Epoch: 01 [17644/20602 ( 86%)], Train Loss: 0.20288\n",
            "Epoch: 01 [17684/20602 ( 86%)], Train Loss: 0.20252\n",
            "Epoch: 01 [17724/20602 ( 86%)], Train Loss: 0.20244\n",
            "Epoch: 01 [17764/20602 ( 86%)], Train Loss: 0.20214\n",
            "Epoch: 01 [17804/20602 ( 86%)], Train Loss: 0.20231\n",
            "Epoch: 01 [17844/20602 ( 87%)], Train Loss: 0.20217\n",
            "Epoch: 01 [17884/20602 ( 87%)], Train Loss: 0.20186\n",
            "Epoch: 01 [17924/20602 ( 87%)], Train Loss: 0.20175\n",
            "Epoch: 01 [17964/20602 ( 87%)], Train Loss: 0.20165\n",
            "Epoch: 01 [18004/20602 ( 87%)], Train Loss: 0.20155\n",
            "Epoch: 01 [18044/20602 ( 88%)], Train Loss: 0.20142\n",
            "Epoch: 01 [18084/20602 ( 88%)], Train Loss: 0.20141\n",
            "Epoch: 01 [18124/20602 ( 88%)], Train Loss: 0.20140\n",
            "Epoch: 01 [18164/20602 ( 88%)], Train Loss: 0.20125\n",
            "Epoch: 01 [18204/20602 ( 88%)], Train Loss: 0.20108\n",
            "Epoch: 01 [18244/20602 ( 89%)], Train Loss: 0.20102\n",
            "Epoch: 01 [18284/20602 ( 89%)], Train Loss: 0.20073\n",
            "Epoch: 01 [18324/20602 ( 89%)], Train Loss: 0.20054\n",
            "Epoch: 01 [18364/20602 ( 89%)], Train Loss: 0.20043\n",
            "Epoch: 01 [18404/20602 ( 89%)], Train Loss: 0.20062\n",
            "Epoch: 01 [18444/20602 ( 90%)], Train Loss: 0.20080\n",
            "Epoch: 01 [18484/20602 ( 90%)], Train Loss: 0.20047\n",
            "Epoch: 01 [18524/20602 ( 90%)], Train Loss: 0.20036\n",
            "Epoch: 01 [18564/20602 ( 90%)], Train Loss: 0.20028\n",
            "Epoch: 01 [18604/20602 ( 90%)], Train Loss: 0.20040\n",
            "Epoch: 01 [18644/20602 ( 90%)], Train Loss: 0.20034\n",
            "Epoch: 01 [18684/20602 ( 91%)], Train Loss: 0.20007\n",
            "Epoch: 01 [18724/20602 ( 91%)], Train Loss: 0.19986\n",
            "Epoch: 01 [18764/20602 ( 91%)], Train Loss: 0.19962\n",
            "Epoch: 01 [18804/20602 ( 91%)], Train Loss: 0.19935\n",
            "Epoch: 01 [18844/20602 ( 91%)], Train Loss: 0.19928\n",
            "Epoch: 01 [18884/20602 ( 92%)], Train Loss: 0.19915\n",
            "Epoch: 01 [18924/20602 ( 92%)], Train Loss: 0.19929\n",
            "Epoch: 01 [18964/20602 ( 92%)], Train Loss: 0.19915\n",
            "Epoch: 01 [19004/20602 ( 92%)], Train Loss: 0.19912\n",
            "Epoch: 01 [19044/20602 ( 92%)], Train Loss: 0.19913\n",
            "Epoch: 01 [19084/20602 ( 93%)], Train Loss: 0.19949\n",
            "Epoch: 01 [19124/20602 ( 93%)], Train Loss: 0.19940\n",
            "Epoch: 01 [19164/20602 ( 93%)], Train Loss: 0.19935\n",
            "Epoch: 01 [19204/20602 ( 93%)], Train Loss: 0.19942\n",
            "Epoch: 01 [19244/20602 ( 93%)], Train Loss: 0.19937\n",
            "Epoch: 01 [19284/20602 ( 94%)], Train Loss: 0.19932\n",
            "Epoch: 01 [19324/20602 ( 94%)], Train Loss: 0.19953\n",
            "Epoch: 01 [19364/20602 ( 94%)], Train Loss: 0.19940\n",
            "Epoch: 01 [19404/20602 ( 94%)], Train Loss: 0.19944\n",
            "Epoch: 01 [19444/20602 ( 94%)], Train Loss: 0.19928\n",
            "Epoch: 01 [19484/20602 ( 95%)], Train Loss: 0.19908\n",
            "Epoch: 01 [19524/20602 ( 95%)], Train Loss: 0.19886\n",
            "Epoch: 01 [19564/20602 ( 95%)], Train Loss: 0.19886\n",
            "Epoch: 01 [19604/20602 ( 95%)], Train Loss: 0.19874\n",
            "Epoch: 01 [19644/20602 ( 95%)], Train Loss: 0.19854\n",
            "Epoch: 01 [19684/20602 ( 96%)], Train Loss: 0.19853\n",
            "Epoch: 01 [19724/20602 ( 96%)], Train Loss: 0.19847\n",
            "Epoch: 01 [19764/20602 ( 96%)], Train Loss: 0.19838\n",
            "Epoch: 01 [19804/20602 ( 96%)], Train Loss: 0.19810\n",
            "Epoch: 01 [19844/20602 ( 96%)], Train Loss: 0.19776\n",
            "Epoch: 01 [19884/20602 ( 97%)], Train Loss: 0.19766\n",
            "Epoch: 01 [19924/20602 ( 97%)], Train Loss: 0.19755\n",
            "Epoch: 01 [19964/20602 ( 97%)], Train Loss: 0.19729\n",
            "Epoch: 01 [20004/20602 ( 97%)], Train Loss: 0.19745\n",
            "Epoch: 01 [20044/20602 ( 97%)], Train Loss: 0.19728\n",
            "Epoch: 01 [20084/20602 ( 97%)], Train Loss: 0.19727\n",
            "Epoch: 01 [20124/20602 ( 98%)], Train Loss: 0.19721\n",
            "Epoch: 01 [20164/20602 ( 98%)], Train Loss: 0.19709\n",
            "Epoch: 01 [20204/20602 ( 98%)], Train Loss: 0.19704\n",
            "Epoch: 01 [20244/20602 ( 98%)], Train Loss: 0.19696\n",
            "Epoch: 01 [20284/20602 ( 98%)], Train Loss: 0.19686\n",
            "Epoch: 01 [20324/20602 ( 99%)], Train Loss: 0.19656\n",
            "Epoch: 01 [20364/20602 ( 99%)], Train Loss: 0.19633\n",
            "Epoch: 01 [20404/20602 ( 99%)], Train Loss: 0.19630\n",
            "Epoch: 01 [20444/20602 ( 99%)], Train Loss: 0.19615\n",
            "Epoch: 01 [20484/20602 ( 99%)], Train Loss: 0.19596\n",
            "Epoch: 01 [20524/20602 (100%)], Train Loss: 0.19570\n",
            "Epoch: 01 [20564/20602 (100%)], Train Loss: 0.19556\n",
            "Epoch: 01 [20602/20602 (100%)], Train Loss: 0.19555\n",
            "----Validation Results Summary----\n",
            "Epoch: [1] Valid Loss: 0.25905\n",
            "\n",
            "Total Training Time: 6054.97665643692secs, Average Training Time per Epoch: 3027.48832821846secs.\n",
            "Total Validation Time: 267.05499386787415secs, Average Validation Time per Epoch: 133.52749693393707secs.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "FOLD: 3\n",
            "--------------------------------------------------\n",
            "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
            "Num examples Train= 20799, Num examples Valid=2797\n",
            "Total Training Steps: 5200, Total Warmup Steps: 520\n",
            "Epoch: 00 [    4/20799 (  0%)], Train Loss: 4.32622\n",
            "Epoch: 00 [   44/20799 (  0%)], Train Loss: 4.17419\n",
            "Epoch: 00 [   84/20799 (  0%)], Train Loss: 4.30797\n",
            "Epoch: 00 [  124/20799 (  1%)], Train Loss: 4.02750\n",
            "Epoch: 00 [  164/20799 (  1%)], Train Loss: 3.75332\n",
            "Epoch: 00 [  204/20799 (  1%)], Train Loss: 3.61822\n",
            "Epoch: 00 [  244/20799 (  1%)], Train Loss: 3.51456\n",
            "Epoch: 00 [  284/20799 (  1%)], Train Loss: 3.33217\n",
            "Epoch: 00 [  324/20799 (  2%)], Train Loss: 3.23138\n",
            "Epoch: 00 [  364/20799 (  2%)], Train Loss: 3.08389\n",
            "Epoch: 00 [  404/20799 (  2%)], Train Loss: 2.94745\n",
            "Epoch: 00 [  444/20799 (  2%)], Train Loss: 2.83743\n",
            "Epoch: 00 [  484/20799 (  2%)], Train Loss: 2.76202\n",
            "Epoch: 00 [  524/20799 (  3%)], Train Loss: 2.65700\n",
            "Epoch: 00 [  564/20799 (  3%)], Train Loss: 2.56085\n",
            "Epoch: 00 [  604/20799 (  3%)], Train Loss: 2.47111\n",
            "Epoch: 00 [  644/20799 (  3%)], Train Loss: 2.37908\n",
            "Epoch: 00 [  684/20799 (  3%)], Train Loss: 2.29060\n",
            "Epoch: 00 [  724/20799 (  3%)], Train Loss: 2.21556\n",
            "Epoch: 00 [  764/20799 (  4%)], Train Loss: 2.14650\n",
            "Epoch: 00 [  804/20799 (  4%)], Train Loss: 2.09828\n",
            "Epoch: 00 [  844/20799 (  4%)], Train Loss: 2.03413\n",
            "Epoch: 00 [  884/20799 (  4%)], Train Loss: 1.97173\n",
            "Epoch: 00 [  924/20799 (  4%)], Train Loss: 1.92724\n",
            "Epoch: 00 [  964/20799 (  5%)], Train Loss: 1.88305\n",
            "Epoch: 00 [ 1004/20799 (  5%)], Train Loss: 1.83649\n",
            "Epoch: 00 [ 1044/20799 (  5%)], Train Loss: 1.78857\n",
            "Epoch: 00 [ 1084/20799 (  5%)], Train Loss: 1.74498\n",
            "Epoch: 00 [ 1124/20799 (  5%)], Train Loss: 1.71134\n",
            "Epoch: 00 [ 1164/20799 (  6%)], Train Loss: 1.66828\n",
            "Epoch: 00 [ 1204/20799 (  6%)], Train Loss: 1.63233\n",
            "Epoch: 00 [ 1244/20799 (  6%)], Train Loss: 1.59871\n",
            "Epoch: 00 [ 1284/20799 (  6%)], Train Loss: 1.56911\n",
            "Epoch: 00 [ 1324/20799 (  6%)], Train Loss: 1.55113\n",
            "Epoch: 00 [ 1364/20799 (  7%)], Train Loss: 1.53584\n",
            "Epoch: 00 [ 1404/20799 (  7%)], Train Loss: 1.51251\n",
            "Epoch: 00 [ 1444/20799 (  7%)], Train Loss: 1.48887\n",
            "Epoch: 00 [ 1484/20799 (  7%)], Train Loss: 1.46698\n",
            "Epoch: 00 [ 1524/20799 (  7%)], Train Loss: 1.44674\n",
            "Epoch: 00 [ 1564/20799 (  8%)], Train Loss: 1.42820\n",
            "Epoch: 00 [ 1604/20799 (  8%)], Train Loss: 1.41514\n",
            "Epoch: 00 [ 1644/20799 (  8%)], Train Loss: 1.39549\n",
            "Epoch: 00 [ 1684/20799 (  8%)], Train Loss: 1.38184\n",
            "Epoch: 00 [ 1724/20799 (  8%)], Train Loss: 1.36406\n",
            "Epoch: 00 [ 1764/20799 (  8%)], Train Loss: 1.33929\n",
            "Epoch: 00 [ 1804/20799 (  9%)], Train Loss: 1.32142\n",
            "Epoch: 00 [ 1844/20799 (  9%)], Train Loss: 1.30305\n",
            "Epoch: 00 [ 1884/20799 (  9%)], Train Loss: 1.29106\n",
            "Epoch: 00 [ 1924/20799 (  9%)], Train Loss: 1.26861\n",
            "Epoch: 00 [ 1964/20799 (  9%)], Train Loss: 1.25658\n",
            "Epoch: 00 [ 2004/20799 ( 10%)], Train Loss: 1.24379\n",
            "Epoch: 00 [ 2044/20799 ( 10%)], Train Loss: 1.22895\n",
            "Epoch: 00 [ 2084/20799 ( 10%)], Train Loss: 1.20973\n",
            "Epoch: 00 [ 2124/20799 ( 10%)], Train Loss: 1.19633\n",
            "Epoch: 00 [ 2164/20799 ( 10%)], Train Loss: 1.18311\n",
            "Epoch: 00 [ 2204/20799 ( 11%)], Train Loss: 1.17398\n",
            "Epoch: 00 [ 2244/20799 ( 11%)], Train Loss: 1.15648\n",
            "Epoch: 00 [ 2284/20799 ( 11%)], Train Loss: 1.14798\n",
            "Epoch: 00 [ 2324/20799 ( 11%)], Train Loss: 1.14061\n",
            "Epoch: 00 [ 2364/20799 ( 11%)], Train Loss: 1.13426\n",
            "Epoch: 00 [ 2404/20799 ( 12%)], Train Loss: 1.12609\n",
            "Epoch: 00 [ 2444/20799 ( 12%)], Train Loss: 1.11972\n",
            "Epoch: 00 [ 2484/20799 ( 12%)], Train Loss: 1.11176\n",
            "Epoch: 00 [ 2524/20799 ( 12%)], Train Loss: 1.10335\n",
            "Epoch: 00 [ 2564/20799 ( 12%)], Train Loss: 1.09546\n",
            "Epoch: 00 [ 2604/20799 ( 13%)], Train Loss: 1.08578\n",
            "Epoch: 00 [ 2644/20799 ( 13%)], Train Loss: 1.07313\n",
            "Epoch: 00 [ 2684/20799 ( 13%)], Train Loss: 1.06581\n",
            "Epoch: 00 [ 2724/20799 ( 13%)], Train Loss: 1.05652\n",
            "Epoch: 00 [ 2764/20799 ( 13%)], Train Loss: 1.04862\n",
            "Epoch: 00 [ 2804/20799 ( 13%)], Train Loss: 1.03873\n",
            "Epoch: 00 [ 2844/20799 ( 14%)], Train Loss: 1.02878\n",
            "Epoch: 00 [ 2884/20799 ( 14%)], Train Loss: 1.01955\n",
            "Epoch: 00 [ 2924/20799 ( 14%)], Train Loss: 1.01421\n",
            "Epoch: 00 [ 2964/20799 ( 14%)], Train Loss: 1.00652\n",
            "Epoch: 00 [ 3004/20799 ( 14%)], Train Loss: 1.00426\n",
            "Epoch: 00 [ 3044/20799 ( 15%)], Train Loss: 0.99703\n",
            "Epoch: 00 [ 3084/20799 ( 15%)], Train Loss: 0.98910\n",
            "Epoch: 00 [ 3124/20799 ( 15%)], Train Loss: 0.98422\n",
            "Epoch: 00 [ 3164/20799 ( 15%)], Train Loss: 0.97907\n",
            "Epoch: 00 [ 3204/20799 ( 15%)], Train Loss: 0.97350\n",
            "Epoch: 00 [ 3244/20799 ( 16%)], Train Loss: 0.97004\n",
            "Epoch: 00 [ 3284/20799 ( 16%)], Train Loss: 0.96764\n",
            "Epoch: 00 [ 3324/20799 ( 16%)], Train Loss: 0.96520\n",
            "Epoch: 00 [ 3364/20799 ( 16%)], Train Loss: 0.96135\n",
            "Epoch: 00 [ 3404/20799 ( 16%)], Train Loss: 0.95592\n",
            "Epoch: 00 [ 3444/20799 ( 17%)], Train Loss: 0.95047\n",
            "Epoch: 00 [ 3484/20799 ( 17%)], Train Loss: 0.94572\n",
            "Epoch: 00 [ 3524/20799 ( 17%)], Train Loss: 0.94114\n",
            "Epoch: 00 [ 3564/20799 ( 17%)], Train Loss: 0.93683\n",
            "Epoch: 00 [ 3604/20799 ( 17%)], Train Loss: 0.93637\n",
            "Epoch: 00 [ 3644/20799 ( 18%)], Train Loss: 0.93058\n",
            "Epoch: 00 [ 3684/20799 ( 18%)], Train Loss: 0.93234\n",
            "Epoch: 00 [ 3724/20799 ( 18%)], Train Loss: 0.92600\n",
            "Epoch: 00 [ 3764/20799 ( 18%)], Train Loss: 0.92305\n",
            "Epoch: 00 [ 3804/20799 ( 18%)], Train Loss: 0.91963\n",
            "Epoch: 00 [ 3844/20799 ( 18%)], Train Loss: 0.91857\n",
            "Epoch: 00 [ 3884/20799 ( 19%)], Train Loss: 0.91507\n",
            "Epoch: 00 [ 3924/20799 ( 19%)], Train Loss: 0.91153\n",
            "Epoch: 00 [ 3964/20799 ( 19%)], Train Loss: 0.90748\n",
            "Epoch: 00 [ 4004/20799 ( 19%)], Train Loss: 0.90600\n",
            "Epoch: 00 [ 4044/20799 ( 19%)], Train Loss: 0.90415\n",
            "Epoch: 00 [ 4084/20799 ( 20%)], Train Loss: 0.90269\n",
            "Epoch: 00 [ 4124/20799 ( 20%)], Train Loss: 0.90188\n",
            "Epoch: 00 [ 4164/20799 ( 20%)], Train Loss: 0.89907\n",
            "Epoch: 00 [ 4204/20799 ( 20%)], Train Loss: 0.89992\n",
            "Epoch: 00 [ 4244/20799 ( 20%)], Train Loss: 0.89630\n",
            "Epoch: 00 [ 4284/20799 ( 21%)], Train Loss: 0.89067\n",
            "Epoch: 00 [ 4324/20799 ( 21%)], Train Loss: 0.88701\n",
            "Epoch: 00 [ 4364/20799 ( 21%)], Train Loss: 0.88340\n",
            "Epoch: 00 [ 4404/20799 ( 21%)], Train Loss: 0.88049\n",
            "Epoch: 00 [ 4444/20799 ( 21%)], Train Loss: 0.87840\n",
            "Epoch: 00 [ 4484/20799 ( 22%)], Train Loss: 0.87651\n",
            "Epoch: 00 [ 4524/20799 ( 22%)], Train Loss: 0.87429\n",
            "Epoch: 00 [ 4564/20799 ( 22%)], Train Loss: 0.86995\n",
            "Epoch: 00 [ 4604/20799 ( 22%)], Train Loss: 0.86626\n",
            "Epoch: 00 [ 4644/20799 ( 22%)], Train Loss: 0.86312\n",
            "Epoch: 00 [ 4684/20799 ( 23%)], Train Loss: 0.86096\n",
            "Epoch: 00 [ 4724/20799 ( 23%)], Train Loss: 0.85682\n",
            "Epoch: 00 [ 4764/20799 ( 23%)], Train Loss: 0.85361\n",
            "Epoch: 00 [ 4804/20799 ( 23%)], Train Loss: 0.85064\n",
            "Epoch: 00 [ 4844/20799 ( 23%)], Train Loss: 0.84759\n",
            "Epoch: 00 [ 4884/20799 ( 23%)], Train Loss: 0.84435\n",
            "Epoch: 00 [ 4924/20799 ( 24%)], Train Loss: 0.84257\n",
            "Epoch: 00 [ 4964/20799 ( 24%)], Train Loss: 0.84019\n",
            "Epoch: 00 [ 5004/20799 ( 24%)], Train Loss: 0.83824\n",
            "Epoch: 00 [ 5044/20799 ( 24%)], Train Loss: 0.83591\n",
            "Epoch: 00 [ 5084/20799 ( 24%)], Train Loss: 0.83169\n",
            "Epoch: 00 [ 5124/20799 ( 25%)], Train Loss: 0.82964\n",
            "Epoch: 00 [ 5164/20799 ( 25%)], Train Loss: 0.82802\n",
            "Epoch: 00 [ 5204/20799 ( 25%)], Train Loss: 0.82649\n",
            "Epoch: 00 [ 5244/20799 ( 25%)], Train Loss: 0.82421\n",
            "Epoch: 00 [ 5284/20799 ( 25%)], Train Loss: 0.82419\n",
            "Epoch: 00 [ 5324/20799 ( 26%)], Train Loss: 0.82269\n",
            "Epoch: 00 [ 5364/20799 ( 26%)], Train Loss: 0.82247\n",
            "Epoch: 00 [ 5404/20799 ( 26%)], Train Loss: 0.81946\n",
            "Epoch: 00 [ 5444/20799 ( 26%)], Train Loss: 0.81731\n",
            "Epoch: 00 [ 5484/20799 ( 26%)], Train Loss: 0.81398\n",
            "Epoch: 00 [ 5524/20799 ( 27%)], Train Loss: 0.81634\n",
            "Epoch: 00 [ 5564/20799 ( 27%)], Train Loss: 0.81525\n",
            "Epoch: 00 [ 5604/20799 ( 27%)], Train Loss: 0.81325\n",
            "Epoch: 00 [ 5644/20799 ( 27%)], Train Loss: 0.81007\n",
            "Epoch: 00 [ 5684/20799 ( 27%)], Train Loss: 0.80714\n",
            "Epoch: 00 [ 5724/20799 ( 28%)], Train Loss: 0.80713\n",
            "Epoch: 00 [ 5764/20799 ( 28%)], Train Loss: 0.80512\n",
            "Epoch: 00 [ 5804/20799 ( 28%)], Train Loss: 0.80240\n",
            "Epoch: 00 [ 5844/20799 ( 28%)], Train Loss: 0.80140\n",
            "Epoch: 00 [ 5884/20799 ( 28%)], Train Loss: 0.80063\n",
            "Epoch: 00 [ 5924/20799 ( 28%)], Train Loss: 0.79912\n",
            "Epoch: 00 [ 5964/20799 ( 29%)], Train Loss: 0.79671\n",
            "Epoch: 00 [ 6004/20799 ( 29%)], Train Loss: 0.79632\n",
            "Epoch: 00 [ 6044/20799 ( 29%)], Train Loss: 0.79394\n",
            "Epoch: 00 [ 6084/20799 ( 29%)], Train Loss: 0.79368\n",
            "Epoch: 00 [ 6124/20799 ( 29%)], Train Loss: 0.79178\n",
            "Epoch: 00 [ 6164/20799 ( 30%)], Train Loss: 0.78885\n",
            "Epoch: 00 [ 6204/20799 ( 30%)], Train Loss: 0.78560\n",
            "Epoch: 00 [ 6244/20799 ( 30%)], Train Loss: 0.78368\n",
            "Epoch: 00 [ 6284/20799 ( 30%)], Train Loss: 0.78201\n",
            "Epoch: 00 [ 6324/20799 ( 30%)], Train Loss: 0.77985\n",
            "Epoch: 00 [ 6364/20799 ( 31%)], Train Loss: 0.77830\n",
            "Epoch: 00 [ 6404/20799 ( 31%)], Train Loss: 0.77635\n",
            "Epoch: 00 [ 6444/20799 ( 31%)], Train Loss: 0.77433\n",
            "Epoch: 00 [ 6484/20799 ( 31%)], Train Loss: 0.77174\n",
            "Epoch: 00 [ 6524/20799 ( 31%)], Train Loss: 0.76975\n",
            "Epoch: 00 [ 6564/20799 ( 32%)], Train Loss: 0.76899\n",
            "Epoch: 00 [ 6604/20799 ( 32%)], Train Loss: 0.76628\n",
            "Epoch: 00 [ 6644/20799 ( 32%)], Train Loss: 0.76527\n",
            "Epoch: 00 [ 6684/20799 ( 32%)], Train Loss: 0.76620\n",
            "Epoch: 00 [ 6724/20799 ( 32%)], Train Loss: 0.76398\n",
            "Epoch: 00 [ 6764/20799 ( 33%)], Train Loss: 0.76438\n",
            "Epoch: 00 [ 6804/20799 ( 33%)], Train Loss: 0.76342\n",
            "Epoch: 00 [ 6844/20799 ( 33%)], Train Loss: 0.76267\n",
            "Epoch: 00 [ 6884/20799 ( 33%)], Train Loss: 0.76118\n",
            "Epoch: 00 [ 6924/20799 ( 33%)], Train Loss: 0.75968\n",
            "Epoch: 00 [ 6964/20799 ( 33%)], Train Loss: 0.75963\n",
            "Epoch: 00 [ 7004/20799 ( 34%)], Train Loss: 0.75827\n",
            "Epoch: 00 [ 7044/20799 ( 34%)], Train Loss: 0.75600\n",
            "Epoch: 00 [ 7084/20799 ( 34%)], Train Loss: 0.75576\n",
            "Epoch: 00 [ 7124/20799 ( 34%)], Train Loss: 0.75553\n",
            "Epoch: 00 [ 7164/20799 ( 34%)], Train Loss: 0.75479\n",
            "Epoch: 00 [ 7204/20799 ( 35%)], Train Loss: 0.75238\n",
            "Epoch: 00 [ 7244/20799 ( 35%)], Train Loss: 0.75197\n",
            "Epoch: 00 [ 7284/20799 ( 35%)], Train Loss: 0.75300\n",
            "Epoch: 00 [ 7324/20799 ( 35%)], Train Loss: 0.75134\n",
            "Epoch: 00 [ 7364/20799 ( 35%)], Train Loss: 0.75053\n",
            "Epoch: 00 [ 7404/20799 ( 36%)], Train Loss: 0.74895\n",
            "Epoch: 00 [ 7444/20799 ( 36%)], Train Loss: 0.74852\n",
            "Epoch: 00 [ 7484/20799 ( 36%)], Train Loss: 0.74683\n",
            "Epoch: 00 [ 7524/20799 ( 36%)], Train Loss: 0.74387\n",
            "Epoch: 00 [ 7564/20799 ( 36%)], Train Loss: 0.74298\n",
            "Epoch: 00 [ 7604/20799 ( 37%)], Train Loss: 0.74043\n",
            "Epoch: 00 [ 7644/20799 ( 37%)], Train Loss: 0.73922\n",
            "Epoch: 00 [ 7684/20799 ( 37%)], Train Loss: 0.73901\n",
            "Epoch: 00 [ 7724/20799 ( 37%)], Train Loss: 0.73750\n",
            "Epoch: 00 [ 7764/20799 ( 37%)], Train Loss: 0.73577\n",
            "Epoch: 00 [ 7804/20799 ( 38%)], Train Loss: 0.73525\n",
            "Epoch: 00 [ 7844/20799 ( 38%)], Train Loss: 0.73508\n",
            "Epoch: 00 [ 7884/20799 ( 38%)], Train Loss: 0.73514\n",
            "Epoch: 00 [ 7924/20799 ( 38%)], Train Loss: 0.73355\n",
            "Epoch: 00 [ 7964/20799 ( 38%)], Train Loss: 0.73162\n",
            "Epoch: 00 [ 8004/20799 ( 38%)], Train Loss: 0.72970\n",
            "Epoch: 00 [ 8044/20799 ( 39%)], Train Loss: 0.72775\n",
            "Epoch: 00 [ 8084/20799 ( 39%)], Train Loss: 0.72555\n",
            "Epoch: 00 [ 8124/20799 ( 39%)], Train Loss: 0.72400\n",
            "Epoch: 00 [ 8164/20799 ( 39%)], Train Loss: 0.72234\n",
            "Epoch: 00 [ 8204/20799 ( 39%)], Train Loss: 0.72211\n",
            "Epoch: 00 [ 8244/20799 ( 40%)], Train Loss: 0.72058\n",
            "Epoch: 00 [ 8284/20799 ( 40%)], Train Loss: 0.71980\n",
            "Epoch: 00 [ 8324/20799 ( 40%)], Train Loss: 0.71881\n",
            "Epoch: 00 [ 8364/20799 ( 40%)], Train Loss: 0.71825\n",
            "Epoch: 00 [ 8404/20799 ( 40%)], Train Loss: 0.71638\n",
            "Epoch: 00 [ 8444/20799 ( 41%)], Train Loss: 0.71607\n",
            "Epoch: 00 [ 8484/20799 ( 41%)], Train Loss: 0.71578\n",
            "Epoch: 00 [ 8524/20799 ( 41%)], Train Loss: 0.71357\n",
            "Epoch: 00 [ 8564/20799 ( 41%)], Train Loss: 0.71228\n",
            "Epoch: 00 [ 8604/20799 ( 41%)], Train Loss: 0.71081\n",
            "Epoch: 00 [ 8644/20799 ( 42%)], Train Loss: 0.71040\n",
            "Epoch: 00 [ 8684/20799 ( 42%)], Train Loss: 0.70881\n",
            "Epoch: 00 [ 8724/20799 ( 42%)], Train Loss: 0.70700\n",
            "Epoch: 00 [ 8764/20799 ( 42%)], Train Loss: 0.70490\n",
            "Epoch: 00 [ 8804/20799 ( 42%)], Train Loss: 0.70406\n",
            "Epoch: 00 [ 8844/20799 ( 43%)], Train Loss: 0.70366\n",
            "Epoch: 00 [ 8884/20799 ( 43%)], Train Loss: 0.70303\n",
            "Epoch: 00 [ 8924/20799 ( 43%)], Train Loss: 0.70115\n",
            "Epoch: 00 [ 8964/20799 ( 43%)], Train Loss: 0.69998\n",
            "Epoch: 00 [ 9004/20799 ( 43%)], Train Loss: 0.69949\n",
            "Epoch: 00 [ 9044/20799 ( 43%)], Train Loss: 0.69821\n",
            "Epoch: 00 [ 9084/20799 ( 44%)], Train Loss: 0.69693\n",
            "Epoch: 00 [ 9124/20799 ( 44%)], Train Loss: 0.69590\n",
            "Epoch: 00 [ 9164/20799 ( 44%)], Train Loss: 0.69511\n",
            "Epoch: 00 [ 9204/20799 ( 44%)], Train Loss: 0.69410\n",
            "Epoch: 00 [ 9244/20799 ( 44%)], Train Loss: 0.69227\n",
            "Epoch: 00 [ 9284/20799 ( 45%)], Train Loss: 0.69175\n",
            "Epoch: 00 [ 9324/20799 ( 45%)], Train Loss: 0.69218\n",
            "Epoch: 00 [ 9364/20799 ( 45%)], Train Loss: 0.69207\n",
            "Epoch: 00 [ 9404/20799 ( 45%)], Train Loss: 0.69173\n",
            "Epoch: 00 [ 9444/20799 ( 45%)], Train Loss: 0.69024\n",
            "Epoch: 00 [ 9484/20799 ( 46%)], Train Loss: 0.68906\n",
            "Epoch: 00 [ 9524/20799 ( 46%)], Train Loss: 0.68773\n",
            "Epoch: 00 [ 9564/20799 ( 46%)], Train Loss: 0.68694\n",
            "Epoch: 00 [ 9604/20799 ( 46%)], Train Loss: 0.68530\n",
            "Epoch: 00 [ 9644/20799 ( 46%)], Train Loss: 0.68465\n",
            "Epoch: 00 [ 9684/20799 ( 47%)], Train Loss: 0.68419\n",
            "Epoch: 00 [ 9724/20799 ( 47%)], Train Loss: 0.68282\n",
            "Epoch: 00 [ 9764/20799 ( 47%)], Train Loss: 0.68170\n",
            "Epoch: 00 [ 9804/20799 ( 47%)], Train Loss: 0.68124\n",
            "Epoch: 00 [ 9844/20799 ( 47%)], Train Loss: 0.68048\n",
            "Epoch: 00 [ 9884/20799 ( 48%)], Train Loss: 0.67969\n",
            "Epoch: 00 [ 9924/20799 ( 48%)], Train Loss: 0.67835\n",
            "Epoch: 00 [ 9964/20799 ( 48%)], Train Loss: 0.67879\n",
            "Epoch: 00 [10004/20799 ( 48%)], Train Loss: 0.67804\n",
            "Epoch: 00 [10044/20799 ( 48%)], Train Loss: 0.67724\n",
            "Epoch: 00 [10084/20799 ( 48%)], Train Loss: 0.67629\n",
            "Epoch: 00 [10124/20799 ( 49%)], Train Loss: 0.67552\n",
            "Epoch: 00 [10164/20799 ( 49%)], Train Loss: 0.67514\n",
            "Epoch: 00 [10204/20799 ( 49%)], Train Loss: 0.67416\n",
            "Epoch: 00 [10244/20799 ( 49%)], Train Loss: 0.67362\n",
            "Epoch: 00 [10284/20799 ( 49%)], Train Loss: 0.67329\n",
            "Epoch: 00 [10324/20799 ( 50%)], Train Loss: 0.67284\n",
            "Epoch: 00 [10364/20799 ( 50%)], Train Loss: 0.67179\n",
            "Epoch: 00 [10404/20799 ( 50%)], Train Loss: 0.67071\n",
            "Epoch: 00 [10444/20799 ( 50%)], Train Loss: 0.66975\n",
            "Epoch: 00 [10484/20799 ( 50%)], Train Loss: 0.66861\n",
            "Epoch: 00 [10524/20799 ( 51%)], Train Loss: 0.66954\n",
            "Epoch: 00 [10564/20799 ( 51%)], Train Loss: 0.66847\n",
            "Epoch: 00 [10604/20799 ( 51%)], Train Loss: 0.66821\n",
            "Epoch: 00 [10644/20799 ( 51%)], Train Loss: 0.66723\n",
            "Epoch: 00 [10684/20799 ( 51%)], Train Loss: 0.66621\n",
            "Epoch: 00 [10724/20799 ( 52%)], Train Loss: 0.66565\n",
            "Epoch: 00 [10764/20799 ( 52%)], Train Loss: 0.66432\n",
            "Epoch: 00 [10804/20799 ( 52%)], Train Loss: 0.66328\n",
            "Epoch: 00 [10844/20799 ( 52%)], Train Loss: 0.66231\n",
            "Epoch: 00 [10884/20799 ( 52%)], Train Loss: 0.66142\n",
            "Epoch: 00 [10924/20799 ( 53%)], Train Loss: 0.66085\n",
            "Epoch: 00 [10964/20799 ( 53%)], Train Loss: 0.65992\n",
            "Epoch: 00 [11004/20799 ( 53%)], Train Loss: 0.66006\n",
            "Epoch: 00 [11044/20799 ( 53%)], Train Loss: 0.65949\n",
            "Epoch: 00 [11084/20799 ( 53%)], Train Loss: 0.65923\n",
            "Epoch: 00 [11124/20799 ( 53%)], Train Loss: 0.65895\n",
            "Epoch: 00 [11164/20799 ( 54%)], Train Loss: 0.65880\n",
            "Epoch: 00 [11204/20799 ( 54%)], Train Loss: 0.65857\n",
            "Epoch: 00 [11244/20799 ( 54%)], Train Loss: 0.65736\n",
            "Epoch: 00 [11284/20799 ( 54%)], Train Loss: 0.65766\n",
            "Epoch: 00 [11324/20799 ( 54%)], Train Loss: 0.65651\n",
            "Epoch: 00 [11364/20799 ( 55%)], Train Loss: 0.65574\n",
            "Epoch: 00 [11404/20799 ( 55%)], Train Loss: 0.65520\n",
            "Epoch: 00 [11444/20799 ( 55%)], Train Loss: 0.65428\n",
            "Epoch: 00 [11484/20799 ( 55%)], Train Loss: 0.65338\n",
            "Epoch: 00 [11524/20799 ( 55%)], Train Loss: 0.65288\n",
            "Epoch: 00 [11564/20799 ( 56%)], Train Loss: 0.65201\n",
            "Epoch: 00 [11604/20799 ( 56%)], Train Loss: 0.65171\n",
            "Epoch: 00 [11644/20799 ( 56%)], Train Loss: 0.65183\n",
            "Epoch: 00 [11684/20799 ( 56%)], Train Loss: 0.65109\n",
            "Epoch: 00 [11724/20799 ( 56%)], Train Loss: 0.65132\n",
            "Epoch: 00 [11764/20799 ( 57%)], Train Loss: 0.65078\n",
            "Epoch: 00 [11804/20799 ( 57%)], Train Loss: 0.65070\n",
            "Epoch: 00 [11844/20799 ( 57%)], Train Loss: 0.65023\n",
            "Epoch: 00 [11884/20799 ( 57%)], Train Loss: 0.64890\n",
            "Epoch: 00 [11924/20799 ( 57%)], Train Loss: 0.64800\n",
            "Epoch: 00 [11964/20799 ( 58%)], Train Loss: 0.64750\n",
            "Epoch: 00 [12004/20799 ( 58%)], Train Loss: 0.64792\n",
            "Epoch: 00 [12044/20799 ( 58%)], Train Loss: 0.64650\n",
            "Epoch: 00 [12084/20799 ( 58%)], Train Loss: 0.64610\n",
            "Epoch: 00 [12124/20799 ( 58%)], Train Loss: 0.64659\n",
            "Epoch: 00 [12164/20799 ( 58%)], Train Loss: 0.64557\n",
            "Epoch: 00 [12204/20799 ( 59%)], Train Loss: 0.64461\n",
            "Epoch: 00 [12244/20799 ( 59%)], Train Loss: 0.64407\n",
            "Epoch: 00 [12284/20799 ( 59%)], Train Loss: 0.64338\n",
            "Epoch: 00 [12324/20799 ( 59%)], Train Loss: 0.64219\n",
            "Epoch: 00 [12364/20799 ( 59%)], Train Loss: 0.64160\n",
            "Epoch: 00 [12404/20799 ( 60%)], Train Loss: 0.64164\n",
            "Epoch: 00 [12444/20799 ( 60%)], Train Loss: 0.64099\n",
            "Epoch: 00 [12484/20799 ( 60%)], Train Loss: 0.64057\n",
            "Epoch: 00 [12524/20799 ( 60%)], Train Loss: 0.64069\n",
            "Epoch: 00 [12564/20799 ( 60%)], Train Loss: 0.64023\n",
            "Epoch: 00 [12604/20799 ( 61%)], Train Loss: 0.63969\n",
            "Epoch: 00 [12644/20799 ( 61%)], Train Loss: 0.63916\n",
            "Epoch: 00 [12684/20799 ( 61%)], Train Loss: 0.63878\n",
            "Epoch: 00 [12724/20799 ( 61%)], Train Loss: 0.63782\n",
            "Epoch: 00 [12764/20799 ( 61%)], Train Loss: 0.63732\n",
            "Epoch: 00 [12804/20799 ( 62%)], Train Loss: 0.63690\n",
            "Epoch: 00 [12844/20799 ( 62%)], Train Loss: 0.63591\n",
            "Epoch: 00 [12884/20799 ( 62%)], Train Loss: 0.63479\n",
            "Epoch: 00 [12924/20799 ( 62%)], Train Loss: 0.63471\n",
            "Epoch: 00 [12964/20799 ( 62%)], Train Loss: 0.63385\n",
            "Epoch: 00 [13004/20799 ( 63%)], Train Loss: 0.63275\n",
            "Epoch: 00 [13044/20799 ( 63%)], Train Loss: 0.63208\n",
            "Epoch: 00 [13084/20799 ( 63%)], Train Loss: 0.63162\n",
            "Epoch: 00 [13124/20799 ( 63%)], Train Loss: 0.63076\n",
            "Epoch: 00 [13164/20799 ( 63%)], Train Loss: 0.63052\n",
            "Epoch: 00 [13204/20799 ( 63%)], Train Loss: 0.63028\n",
            "Epoch: 00 [13244/20799 ( 64%)], Train Loss: 0.62947\n",
            "Epoch: 00 [13284/20799 ( 64%)], Train Loss: 0.62853\n",
            "Epoch: 00 [13324/20799 ( 64%)], Train Loss: 0.62795\n",
            "Epoch: 00 [13364/20799 ( 64%)], Train Loss: 0.62770\n",
            "Epoch: 00 [13404/20799 ( 64%)], Train Loss: 0.62737\n",
            "Epoch: 00 [13444/20799 ( 65%)], Train Loss: 0.62700\n",
            "Epoch: 00 [13484/20799 ( 65%)], Train Loss: 0.62722\n",
            "Epoch: 00 [13524/20799 ( 65%)], Train Loss: 0.62706\n",
            "Epoch: 00 [13564/20799 ( 65%)], Train Loss: 0.62639\n",
            "Epoch: 00 [13604/20799 ( 65%)], Train Loss: 0.62553\n",
            "Epoch: 00 [13644/20799 ( 66%)], Train Loss: 0.62451\n",
            "Epoch: 00 [13684/20799 ( 66%)], Train Loss: 0.62480\n",
            "Epoch: 00 [13724/20799 ( 66%)], Train Loss: 0.62376\n",
            "Epoch: 00 [13764/20799 ( 66%)], Train Loss: 0.62296\n",
            "Epoch: 00 [13804/20799 ( 66%)], Train Loss: 0.62201\n",
            "Epoch: 00 [13844/20799 ( 67%)], Train Loss: 0.62153\n",
            "Epoch: 00 [13884/20799 ( 67%)], Train Loss: 0.62055\n",
            "Epoch: 00 [13924/20799 ( 67%)], Train Loss: 0.62019\n",
            "Epoch: 00 [13964/20799 ( 67%)], Train Loss: 0.62005\n",
            "Epoch: 00 [14004/20799 ( 67%)], Train Loss: 0.61980\n",
            "Epoch: 00 [14044/20799 ( 68%)], Train Loss: 0.61891\n",
            "Epoch: 00 [14084/20799 ( 68%)], Train Loss: 0.61825\n",
            "Epoch: 00 [14124/20799 ( 68%)], Train Loss: 0.61796\n",
            "Epoch: 00 [14164/20799 ( 68%)], Train Loss: 0.61665\n",
            "Epoch: 00 [14204/20799 ( 68%)], Train Loss: 0.61605\n",
            "Epoch: 00 [14244/20799 ( 68%)], Train Loss: 0.61558\n",
            "Epoch: 00 [14284/20799 ( 69%)], Train Loss: 0.61610\n",
            "Epoch: 00 [14324/20799 ( 69%)], Train Loss: 0.61605\n",
            "Epoch: 00 [14364/20799 ( 69%)], Train Loss: 0.61563\n",
            "Epoch: 00 [14404/20799 ( 69%)], Train Loss: 0.61604\n",
            "Epoch: 00 [14444/20799 ( 69%)], Train Loss: 0.61564\n",
            "Epoch: 00 [14484/20799 ( 70%)], Train Loss: 0.61558\n",
            "Epoch: 00 [14524/20799 ( 70%)], Train Loss: 0.61496\n",
            "Epoch: 00 [14564/20799 ( 70%)], Train Loss: 0.61477\n",
            "Epoch: 00 [14604/20799 ( 70%)], Train Loss: 0.61509\n",
            "Epoch: 00 [14644/20799 ( 70%)], Train Loss: 0.61451\n",
            "Epoch: 00 [14684/20799 ( 71%)], Train Loss: 0.61367\n",
            "Epoch: 00 [14724/20799 ( 71%)], Train Loss: 0.61331\n",
            "Epoch: 00 [14764/20799 ( 71%)], Train Loss: 0.61265\n",
            "Epoch: 00 [14804/20799 ( 71%)], Train Loss: 0.61190\n",
            "Epoch: 00 [14844/20799 ( 71%)], Train Loss: 0.61139\n",
            "Epoch: 00 [14884/20799 ( 72%)], Train Loss: 0.61054\n",
            "Epoch: 00 [14924/20799 ( 72%)], Train Loss: 0.61013\n",
            "Epoch: 00 [14964/20799 ( 72%)], Train Loss: 0.60908\n",
            "Epoch: 00 [15004/20799 ( 72%)], Train Loss: 0.60934\n",
            "Epoch: 00 [15044/20799 ( 72%)], Train Loss: 0.60951\n",
            "Epoch: 00 [15084/20799 ( 73%)], Train Loss: 0.60948\n",
            "Epoch: 00 [15124/20799 ( 73%)], Train Loss: 0.60930\n",
            "Epoch: 00 [15164/20799 ( 73%)], Train Loss: 0.60876\n",
            "Epoch: 00 [15204/20799 ( 73%)], Train Loss: 0.60844\n",
            "Epoch: 00 [15244/20799 ( 73%)], Train Loss: 0.60792\n",
            "Epoch: 00 [15284/20799 ( 73%)], Train Loss: 0.60703\n",
            "Epoch: 00 [15324/20799 ( 74%)], Train Loss: 0.60630\n",
            "Epoch: 00 [15364/20799 ( 74%)], Train Loss: 0.60621\n",
            "Epoch: 00 [15404/20799 ( 74%)], Train Loss: 0.60536\n",
            "Epoch: 00 [15444/20799 ( 74%)], Train Loss: 0.60476\n",
            "Epoch: 00 [15484/20799 ( 74%)], Train Loss: 0.60438\n",
            "Epoch: 00 [15524/20799 ( 75%)], Train Loss: 0.60358\n",
            "Epoch: 00 [15564/20799 ( 75%)], Train Loss: 0.60286\n",
            "Epoch: 00 [15604/20799 ( 75%)], Train Loss: 0.60194\n",
            "Epoch: 00 [15644/20799 ( 75%)], Train Loss: 0.60228\n",
            "Epoch: 00 [15684/20799 ( 75%)], Train Loss: 0.60238\n",
            "Epoch: 00 [15724/20799 ( 76%)], Train Loss: 0.60211\n",
            "Epoch: 00 [15764/20799 ( 76%)], Train Loss: 0.60170\n",
            "Epoch: 00 [15804/20799 ( 76%)], Train Loss: 0.60145\n",
            "Epoch: 00 [15844/20799 ( 76%)], Train Loss: 0.60073\n",
            "Epoch: 00 [15884/20799 ( 76%)], Train Loss: 0.60058\n",
            "Epoch: 00 [15924/20799 ( 77%)], Train Loss: 0.60007\n",
            "Epoch: 00 [15964/20799 ( 77%)], Train Loss: 0.60006\n",
            "Epoch: 00 [16004/20799 ( 77%)], Train Loss: 0.60007\n",
            "Epoch: 00 [16044/20799 ( 77%)], Train Loss: 0.59946\n",
            "Epoch: 00 [16084/20799 ( 77%)], Train Loss: 0.59927\n",
            "Epoch: 00 [16124/20799 ( 78%)], Train Loss: 0.59843\n",
            "Epoch: 00 [16164/20799 ( 78%)], Train Loss: 0.59830\n",
            "Epoch: 00 [16204/20799 ( 78%)], Train Loss: 0.59748\n",
            "Epoch: 00 [16244/20799 ( 78%)], Train Loss: 0.59727\n",
            "Epoch: 00 [16284/20799 ( 78%)], Train Loss: 0.59697\n",
            "Epoch: 00 [16324/20799 ( 78%)], Train Loss: 0.59696\n",
            "Epoch: 00 [16364/20799 ( 79%)], Train Loss: 0.59617\n",
            "Epoch: 00 [16404/20799 ( 79%)], Train Loss: 0.59576\n",
            "Epoch: 00 [16444/20799 ( 79%)], Train Loss: 0.59547\n",
            "Epoch: 00 [16484/20799 ( 79%)], Train Loss: 0.59538\n",
            "Epoch: 00 [16524/20799 ( 79%)], Train Loss: 0.59491\n",
            "Epoch: 00 [16564/20799 ( 80%)], Train Loss: 0.59373\n",
            "Epoch: 00 [16604/20799 ( 80%)], Train Loss: 0.59311\n",
            "Epoch: 00 [16644/20799 ( 80%)], Train Loss: 0.59312\n",
            "Epoch: 00 [16684/20799 ( 80%)], Train Loss: 0.59237\n",
            "Epoch: 00 [16724/20799 ( 80%)], Train Loss: 0.59216\n",
            "Epoch: 00 [16764/20799 ( 81%)], Train Loss: 0.59171\n",
            "Epoch: 00 [16804/20799 ( 81%)], Train Loss: 0.59156\n",
            "Epoch: 00 [16844/20799 ( 81%)], Train Loss: 0.59144\n",
            "Epoch: 00 [16884/20799 ( 81%)], Train Loss: 0.59098\n",
            "Epoch: 00 [16924/20799 ( 81%)], Train Loss: 0.59133\n",
            "Epoch: 00 [16964/20799 ( 82%)], Train Loss: 0.59088\n",
            "Epoch: 00 [17004/20799 ( 82%)], Train Loss: 0.59074\n",
            "Epoch: 00 [17044/20799 ( 82%)], Train Loss: 0.59049\n",
            "Epoch: 00 [17084/20799 ( 82%)], Train Loss: 0.59049\n",
            "Epoch: 00 [17124/20799 ( 82%)], Train Loss: 0.59030\n",
            "Epoch: 00 [17164/20799 ( 83%)], Train Loss: 0.59008\n",
            "Epoch: 00 [17204/20799 ( 83%)], Train Loss: 0.59004\n",
            "Epoch: 00 [17244/20799 ( 83%)], Train Loss: 0.58942\n",
            "Epoch: 00 [17284/20799 ( 83%)], Train Loss: 0.58942\n",
            "Epoch: 00 [17324/20799 ( 83%)], Train Loss: 0.58924\n",
            "Epoch: 00 [17364/20799 ( 83%)], Train Loss: 0.58875\n",
            "Epoch: 00 [17404/20799 ( 84%)], Train Loss: 0.58858\n",
            "Epoch: 00 [17444/20799 ( 84%)], Train Loss: 0.58837\n",
            "Epoch: 00 [17484/20799 ( 84%)], Train Loss: 0.58820\n",
            "Epoch: 00 [17524/20799 ( 84%)], Train Loss: 0.58872\n",
            "Epoch: 00 [17564/20799 ( 84%)], Train Loss: 0.58814\n",
            "Epoch: 00 [17604/20799 ( 85%)], Train Loss: 0.58780\n",
            "Epoch: 00 [17644/20799 ( 85%)], Train Loss: 0.58754\n",
            "Epoch: 00 [17684/20799 ( 85%)], Train Loss: 0.58740\n",
            "Epoch: 00 [17724/20799 ( 85%)], Train Loss: 0.58689\n",
            "Epoch: 00 [17764/20799 ( 85%)], Train Loss: 0.58637\n",
            "Epoch: 00 [17804/20799 ( 86%)], Train Loss: 0.58573\n",
            "Epoch: 00 [17844/20799 ( 86%)], Train Loss: 0.58567\n",
            "Epoch: 00 [17884/20799 ( 86%)], Train Loss: 0.58583\n",
            "Epoch: 00 [17924/20799 ( 86%)], Train Loss: 0.58571\n",
            "Epoch: 00 [17964/20799 ( 86%)], Train Loss: 0.58539\n",
            "Epoch: 00 [18004/20799 ( 87%)], Train Loss: 0.58469\n",
            "Epoch: 00 [18044/20799 ( 87%)], Train Loss: 0.58410\n",
            "Epoch: 00 [18084/20799 ( 87%)], Train Loss: 0.58390\n",
            "Epoch: 00 [18124/20799 ( 87%)], Train Loss: 0.58326\n",
            "Epoch: 00 [18164/20799 ( 87%)], Train Loss: 0.58274\n",
            "Epoch: 00 [18204/20799 ( 88%)], Train Loss: 0.58221\n",
            "Epoch: 00 [18244/20799 ( 88%)], Train Loss: 0.58149\n",
            "Epoch: 00 [18284/20799 ( 88%)], Train Loss: 0.58107\n",
            "Epoch: 00 [18324/20799 ( 88%)], Train Loss: 0.58066\n",
            "Epoch: 00 [18364/20799 ( 88%)], Train Loss: 0.58049\n",
            "Epoch: 00 [18404/20799 ( 88%)], Train Loss: 0.57990\n",
            "Epoch: 00 [18444/20799 ( 89%)], Train Loss: 0.57956\n",
            "Epoch: 00 [18484/20799 ( 89%)], Train Loss: 0.57883\n",
            "Epoch: 00 [18524/20799 ( 89%)], Train Loss: 0.57869\n",
            "Epoch: 00 [18564/20799 ( 89%)], Train Loss: 0.57840\n",
            "Epoch: 00 [18604/20799 ( 89%)], Train Loss: 0.57796\n",
            "Epoch: 00 [18644/20799 ( 90%)], Train Loss: 0.57776\n",
            "Epoch: 00 [18684/20799 ( 90%)], Train Loss: 0.57767\n",
            "Epoch: 00 [18724/20799 ( 90%)], Train Loss: 0.57728\n",
            "Epoch: 00 [18764/20799 ( 90%)], Train Loss: 0.57654\n",
            "Epoch: 00 [18804/20799 ( 90%)], Train Loss: 0.57629\n",
            "Epoch: 00 [18844/20799 ( 91%)], Train Loss: 0.57618\n",
            "Epoch: 00 [18884/20799 ( 91%)], Train Loss: 0.57612\n",
            "Epoch: 00 [18924/20799 ( 91%)], Train Loss: 0.57568\n",
            "Epoch: 00 [18964/20799 ( 91%)], Train Loss: 0.57543\n",
            "Epoch: 00 [19004/20799 ( 91%)], Train Loss: 0.57515\n",
            "Epoch: 00 [19044/20799 ( 92%)], Train Loss: 0.57553\n",
            "Epoch: 00 [19084/20799 ( 92%)], Train Loss: 0.57536\n",
            "Epoch: 00 [19124/20799 ( 92%)], Train Loss: 0.57487\n",
            "Epoch: 00 [19164/20799 ( 92%)], Train Loss: 0.57503\n",
            "Epoch: 00 [19204/20799 ( 92%)], Train Loss: 0.57477\n",
            "Epoch: 00 [19244/20799 ( 93%)], Train Loss: 0.57482\n",
            "Epoch: 00 [19284/20799 ( 93%)], Train Loss: 0.57446\n",
            "Epoch: 00 [19324/20799 ( 93%)], Train Loss: 0.57407\n",
            "Epoch: 00 [19364/20799 ( 93%)], Train Loss: 0.57387\n",
            "Epoch: 00 [19404/20799 ( 93%)], Train Loss: 0.57323\n",
            "Epoch: 00 [19444/20799 ( 93%)], Train Loss: 0.57311\n",
            "Epoch: 00 [19484/20799 ( 94%)], Train Loss: 0.57269\n",
            "Epoch: 00 [19524/20799 ( 94%)], Train Loss: 0.57244\n",
            "Epoch: 00 [19564/20799 ( 94%)], Train Loss: 0.57273\n",
            "Epoch: 00 [19604/20799 ( 94%)], Train Loss: 0.57226\n",
            "Epoch: 00 [19644/20799 ( 94%)], Train Loss: 0.57202\n",
            "Epoch: 00 [19684/20799 ( 95%)], Train Loss: 0.57134\n",
            "Epoch: 00 [19724/20799 ( 95%)], Train Loss: 0.57112\n",
            "Epoch: 00 [19764/20799 ( 95%)], Train Loss: 0.57076\n",
            "Epoch: 00 [19804/20799 ( 95%)], Train Loss: 0.57052\n",
            "Epoch: 00 [19844/20799 ( 95%)], Train Loss: 0.57065\n",
            "Epoch: 00 [19884/20799 ( 96%)], Train Loss: 0.57096\n",
            "Epoch: 00 [19924/20799 ( 96%)], Train Loss: 0.57044\n",
            "Epoch: 00 [19964/20799 ( 96%)], Train Loss: 0.57011\n",
            "Epoch: 00 [20004/20799 ( 96%)], Train Loss: 0.56935\n",
            "Epoch: 00 [20044/20799 ( 96%)], Train Loss: 0.56914\n",
            "Epoch: 00 [20084/20799 ( 97%)], Train Loss: 0.56890\n",
            "Epoch: 00 [20124/20799 ( 97%)], Train Loss: 0.56894\n",
            "Epoch: 00 [20164/20799 ( 97%)], Train Loss: 0.56839\n",
            "Epoch: 00 [20204/20799 ( 97%)], Train Loss: 0.56815\n",
            "Epoch: 00 [20244/20799 ( 97%)], Train Loss: 0.56786\n",
            "Epoch: 00 [20284/20799 ( 98%)], Train Loss: 0.56741\n",
            "Epoch: 00 [20324/20799 ( 98%)], Train Loss: 0.56710\n",
            "Epoch: 00 [20364/20799 ( 98%)], Train Loss: 0.56667\n",
            "Epoch: 00 [20404/20799 ( 98%)], Train Loss: 0.56601\n",
            "Epoch: 00 [20444/20799 ( 98%)], Train Loss: 0.56586\n",
            "Epoch: 00 [20484/20799 ( 98%)], Train Loss: 0.56598\n",
            "Epoch: 00 [20524/20799 ( 99%)], Train Loss: 0.56558\n",
            "Epoch: 00 [20564/20799 ( 99%)], Train Loss: 0.56565\n",
            "Epoch: 00 [20604/20799 ( 99%)], Train Loss: 0.56525\n",
            "Epoch: 00 [20644/20799 ( 99%)], Train Loss: 0.56512\n",
            "Epoch: 00 [20684/20799 ( 99%)], Train Loss: 0.56464\n",
            "Epoch: 00 [20724/20799 (100%)], Train Loss: 0.56466\n",
            "Epoch: 00 [20764/20799 (100%)], Train Loss: 0.56508\n",
            "Epoch: 00 [20799/20799 (100%)], Train Loss: 0.56485\n",
            "----Validation Results Summary----\n",
            "Epoch: [0] Valid Loss: 0.27430\n",
            "0 Epoch, Best epoch was updated! Valid Loss: 0.27430\n",
            "Saving model checkpoint to output/checkpoint-fold-3.\n",
            "\n",
            "Epoch: 01 [    4/20799 (  0%)], Train Loss: 0.18193\n",
            "Epoch: 01 [   44/20799 (  0%)], Train Loss: 0.67509\n",
            "Epoch: 01 [   84/20799 (  0%)], Train Loss: 0.59745\n",
            "Epoch: 01 [  124/20799 (  1%)], Train Loss: 0.52414\n",
            "Epoch: 01 [  164/20799 (  1%)], Train Loss: 0.44112\n",
            "Epoch: 01 [  204/20799 (  1%)], Train Loss: 0.44722\n",
            "Epoch: 01 [  244/20799 (  1%)], Train Loss: 0.45097\n",
            "Epoch: 01 [  284/20799 (  1%)], Train Loss: 0.43048\n",
            "Epoch: 01 [  324/20799 (  2%)], Train Loss: 0.46879\n",
            "Epoch: 01 [  364/20799 (  2%)], Train Loss: 0.47889\n",
            "Epoch: 01 [  404/20799 (  2%)], Train Loss: 0.47291\n",
            "Epoch: 01 [  444/20799 (  2%)], Train Loss: 0.47486\n",
            "Epoch: 01 [  484/20799 (  2%)], Train Loss: 0.48203\n",
            "Epoch: 01 [  524/20799 (  3%)], Train Loss: 0.48056\n",
            "Epoch: 01 [  564/20799 (  3%)], Train Loss: 0.48307\n",
            "Epoch: 01 [  604/20799 (  3%)], Train Loss: 0.49220\n",
            "Epoch: 01 [  644/20799 (  3%)], Train Loss: 0.49417\n",
            "Epoch: 01 [  684/20799 (  3%)], Train Loss: 0.48222\n",
            "Epoch: 01 [  724/20799 (  3%)], Train Loss: 0.47627\n",
            "Epoch: 01 [  764/20799 (  4%)], Train Loss: 0.47742\n",
            "Epoch: 01 [  804/20799 (  4%)], Train Loss: 0.47718\n",
            "Epoch: 01 [  844/20799 (  4%)], Train Loss: 0.46638\n",
            "Epoch: 01 [  884/20799 (  4%)], Train Loss: 0.45983\n",
            "Epoch: 01 [  924/20799 (  4%)], Train Loss: 0.45403\n",
            "Epoch: 01 [  964/20799 (  5%)], Train Loss: 0.45473\n",
            "Epoch: 01 [ 1004/20799 (  5%)], Train Loss: 0.44891\n",
            "Epoch: 01 [ 1044/20799 (  5%)], Train Loss: 0.44061\n",
            "Epoch: 01 [ 1084/20799 (  5%)], Train Loss: 0.43728\n",
            "Epoch: 01 [ 1124/20799 (  5%)], Train Loss: 0.43627\n",
            "Epoch: 01 [ 1164/20799 (  6%)], Train Loss: 0.42620\n",
            "Epoch: 01 [ 1204/20799 (  6%)], Train Loss: 0.41954\n",
            "Epoch: 01 [ 1244/20799 (  6%)], Train Loss: 0.41718\n",
            "Epoch: 01 [ 1284/20799 (  6%)], Train Loss: 0.41305\n",
            "Epoch: 01 [ 1324/20799 (  6%)], Train Loss: 0.41653\n",
            "Epoch: 01 [ 1364/20799 (  7%)], Train Loss: 0.42054\n",
            "Epoch: 01 [ 1404/20799 (  7%)], Train Loss: 0.41829\n",
            "Epoch: 01 [ 1444/20799 (  7%)], Train Loss: 0.41332\n",
            "Epoch: 01 [ 1484/20799 (  7%)], Train Loss: 0.41062\n",
            "Epoch: 01 [ 1524/20799 (  7%)], Train Loss: 0.40885\n",
            "Epoch: 01 [ 1564/20799 (  8%)], Train Loss: 0.40875\n",
            "Epoch: 01 [ 1604/20799 (  8%)], Train Loss: 0.41146\n",
            "Epoch: 01 [ 1644/20799 (  8%)], Train Loss: 0.41043\n",
            "Epoch: 01 [ 1684/20799 (  8%)], Train Loss: 0.41061\n",
            "Epoch: 01 [ 1724/20799 (  8%)], Train Loss: 0.40984\n",
            "Epoch: 01 [ 1764/20799 (  8%)], Train Loss: 0.40253\n",
            "Epoch: 01 [ 1804/20799 (  9%)], Train Loss: 0.39768\n",
            "Epoch: 01 [ 1844/20799 (  9%)], Train Loss: 0.39441\n",
            "Epoch: 01 [ 1884/20799 (  9%)], Train Loss: 0.39453\n",
            "Epoch: 01 [ 1924/20799 (  9%)], Train Loss: 0.38792\n",
            "Epoch: 01 [ 1964/20799 (  9%)], Train Loss: 0.38638\n",
            "Epoch: 01 [ 2004/20799 ( 10%)], Train Loss: 0.38524\n",
            "Epoch: 01 [ 2044/20799 ( 10%)], Train Loss: 0.38185\n",
            "Epoch: 01 [ 2084/20799 ( 10%)], Train Loss: 0.37715\n",
            "Epoch: 01 [ 2124/20799 ( 10%)], Train Loss: 0.37361\n",
            "Epoch: 01 [ 2164/20799 ( 10%)], Train Loss: 0.37028\n",
            "Epoch: 01 [ 2204/20799 ( 11%)], Train Loss: 0.36730\n",
            "Epoch: 01 [ 2244/20799 ( 11%)], Train Loss: 0.36154\n",
            "Epoch: 01 [ 2284/20799 ( 11%)], Train Loss: 0.36103\n",
            "Epoch: 01 [ 2324/20799 ( 11%)], Train Loss: 0.36096\n",
            "Epoch: 01 [ 2364/20799 ( 11%)], Train Loss: 0.35844\n",
            "Epoch: 01 [ 2404/20799 ( 12%)], Train Loss: 0.35646\n",
            "Epoch: 01 [ 2444/20799 ( 12%)], Train Loss: 0.35832\n",
            "Epoch: 01 [ 2484/20799 ( 12%)], Train Loss: 0.35804\n",
            "Epoch: 01 [ 2524/20799 ( 12%)], Train Loss: 0.35728\n",
            "Epoch: 01 [ 2564/20799 ( 12%)], Train Loss: 0.35619\n",
            "Epoch: 01 [ 2604/20799 ( 13%)], Train Loss: 0.35311\n",
            "Epoch: 01 [ 2644/20799 ( 13%)], Train Loss: 0.34890\n",
            "Epoch: 01 [ 2684/20799 ( 13%)], Train Loss: 0.34710\n",
            "Epoch: 01 [ 2724/20799 ( 13%)], Train Loss: 0.34462\n",
            "Epoch: 01 [ 2764/20799 ( 13%)], Train Loss: 0.34174\n",
            "Epoch: 01 [ 2804/20799 ( 13%)], Train Loss: 0.33856\n",
            "Epoch: 01 [ 2844/20799 ( 14%)], Train Loss: 0.33544\n",
            "Epoch: 01 [ 2884/20799 ( 14%)], Train Loss: 0.33455\n",
            "Epoch: 01 [ 2924/20799 ( 14%)], Train Loss: 0.33255\n",
            "Epoch: 01 [ 2964/20799 ( 14%)], Train Loss: 0.32979\n",
            "Epoch: 01 [ 3004/20799 ( 14%)], Train Loss: 0.32876\n",
            "Epoch: 01 [ 3044/20799 ( 15%)], Train Loss: 0.32693\n",
            "Epoch: 01 [ 3084/20799 ( 15%)], Train Loss: 0.32396\n",
            "Epoch: 01 [ 3124/20799 ( 15%)], Train Loss: 0.32235\n",
            "Epoch: 01 [ 3164/20799 ( 15%)], Train Loss: 0.32220\n",
            "Epoch: 01 [ 3204/20799 ( 15%)], Train Loss: 0.32079\n",
            "Epoch: 01 [ 3244/20799 ( 16%)], Train Loss: 0.31994\n",
            "Epoch: 01 [ 3284/20799 ( 16%)], Train Loss: 0.32131\n",
            "Epoch: 01 [ 3324/20799 ( 16%)], Train Loss: 0.32139\n",
            "Epoch: 01 [ 3364/20799 ( 16%)], Train Loss: 0.32090\n",
            "Epoch: 01 [ 3404/20799 ( 16%)], Train Loss: 0.32011\n",
            "Epoch: 01 [ 3444/20799 ( 17%)], Train Loss: 0.31944\n",
            "Epoch: 01 [ 3484/20799 ( 17%)], Train Loss: 0.31884\n",
            "Epoch: 01 [ 3524/20799 ( 17%)], Train Loss: 0.31679\n",
            "Epoch: 01 [ 3564/20799 ( 17%)], Train Loss: 0.31652\n",
            "Epoch: 01 [ 3604/20799 ( 17%)], Train Loss: 0.31808\n",
            "Epoch: 01 [ 3644/20799 ( 18%)], Train Loss: 0.31639\n",
            "Epoch: 01 [ 3684/20799 ( 18%)], Train Loss: 0.31710\n",
            "Epoch: 01 [ 3724/20799 ( 18%)], Train Loss: 0.31469\n",
            "Epoch: 01 [ 3764/20799 ( 18%)], Train Loss: 0.31397\n",
            "Epoch: 01 [ 3804/20799 ( 18%)], Train Loss: 0.31305\n",
            "Epoch: 01 [ 3844/20799 ( 18%)], Train Loss: 0.31288\n",
            "Epoch: 01 [ 3884/20799 ( 19%)], Train Loss: 0.31124\n",
            "Epoch: 01 [ 3924/20799 ( 19%)], Train Loss: 0.30948\n",
            "Epoch: 01 [ 3964/20799 ( 19%)], Train Loss: 0.30789\n",
            "Epoch: 01 [ 4004/20799 ( 19%)], Train Loss: 0.30871\n",
            "Epoch: 01 [ 4044/20799 ( 19%)], Train Loss: 0.30927\n",
            "Epoch: 01 [ 4084/20799 ( 20%)], Train Loss: 0.30786\n",
            "Epoch: 01 [ 4124/20799 ( 20%)], Train Loss: 0.30849\n",
            "Epoch: 01 [ 4164/20799 ( 20%)], Train Loss: 0.30706\n",
            "Epoch: 01 [ 4204/20799 ( 20%)], Train Loss: 0.30705\n",
            "Epoch: 01 [ 4244/20799 ( 20%)], Train Loss: 0.30527\n",
            "Epoch: 01 [ 4284/20799 ( 21%)], Train Loss: 0.30375\n",
            "Epoch: 01 [ 4324/20799 ( 21%)], Train Loss: 0.30240\n",
            "Epoch: 01 [ 4364/20799 ( 21%)], Train Loss: 0.30104\n",
            "Epoch: 01 [ 4404/20799 ( 21%)], Train Loss: 0.29957\n",
            "Epoch: 01 [ 4444/20799 ( 21%)], Train Loss: 0.29867\n",
            "Epoch: 01 [ 4484/20799 ( 22%)], Train Loss: 0.29842\n",
            "Epoch: 01 [ 4524/20799 ( 22%)], Train Loss: 0.29794\n",
            "Epoch: 01 [ 4564/20799 ( 22%)], Train Loss: 0.29601\n",
            "Epoch: 01 [ 4604/20799 ( 22%)], Train Loss: 0.29486\n",
            "Epoch: 01 [ 4644/20799 ( 22%)], Train Loss: 0.29376\n",
            "Epoch: 01 [ 4684/20799 ( 23%)], Train Loss: 0.29302\n",
            "Epoch: 01 [ 4724/20799 ( 23%)], Train Loss: 0.29171\n",
            "Epoch: 01 [ 4764/20799 ( 23%)], Train Loss: 0.29158\n",
            "Epoch: 01 [ 4804/20799 ( 23%)], Train Loss: 0.29081\n",
            "Epoch: 01 [ 4844/20799 ( 23%)], Train Loss: 0.28944\n",
            "Epoch: 01 [ 4884/20799 ( 23%)], Train Loss: 0.28777\n",
            "Epoch: 01 [ 4924/20799 ( 24%)], Train Loss: 0.28681\n",
            "Epoch: 01 [ 4964/20799 ( 24%)], Train Loss: 0.28638\n",
            "Epoch: 01 [ 5004/20799 ( 24%)], Train Loss: 0.28580\n",
            "Epoch: 01 [ 5044/20799 ( 24%)], Train Loss: 0.28551\n",
            "Epoch: 01 [ 5084/20799 ( 24%)], Train Loss: 0.28411\n",
            "Epoch: 01 [ 5124/20799 ( 25%)], Train Loss: 0.28303\n",
            "Epoch: 01 [ 5164/20799 ( 25%)], Train Loss: 0.28290\n",
            "Epoch: 01 [ 5204/20799 ( 25%)], Train Loss: 0.28207\n",
            "Epoch: 01 [ 5244/20799 ( 25%)], Train Loss: 0.28084\n",
            "Epoch: 01 [ 5284/20799 ( 25%)], Train Loss: 0.28092\n",
            "Epoch: 01 [ 5324/20799 ( 26%)], Train Loss: 0.28116\n",
            "Epoch: 01 [ 5364/20799 ( 26%)], Train Loss: 0.28204\n",
            "Epoch: 01 [ 5404/20799 ( 26%)], Train Loss: 0.28080\n",
            "Epoch: 01 [ 5444/20799 ( 26%)], Train Loss: 0.27924\n",
            "Epoch: 01 [ 5484/20799 ( 26%)], Train Loss: 0.27785\n",
            "Epoch: 01 [ 5524/20799 ( 27%)], Train Loss: 0.27845\n",
            "Epoch: 01 [ 5564/20799 ( 27%)], Train Loss: 0.27847\n",
            "Epoch: 01 [ 5604/20799 ( 27%)], Train Loss: 0.27717\n",
            "Epoch: 01 [ 5644/20799 ( 27%)], Train Loss: 0.27606\n",
            "Epoch: 01 [ 5684/20799 ( 27%)], Train Loss: 0.27513\n",
            "Epoch: 01 [ 5724/20799 ( 28%)], Train Loss: 0.27562\n",
            "Epoch: 01 [ 5764/20799 ( 28%)], Train Loss: 0.27473\n",
            "Epoch: 01 [ 5804/20799 ( 28%)], Train Loss: 0.27318\n",
            "Epoch: 01 [ 5844/20799 ( 28%)], Train Loss: 0.27250\n",
            "Epoch: 01 [ 5884/20799 ( 28%)], Train Loss: 0.27219\n",
            "Epoch: 01 [ 5924/20799 ( 28%)], Train Loss: 0.27171\n",
            "Epoch: 01 [ 5964/20799 ( 29%)], Train Loss: 0.27094\n",
            "Epoch: 01 [ 6004/20799 ( 29%)], Train Loss: 0.27124\n",
            "Epoch: 01 [ 6044/20799 ( 29%)], Train Loss: 0.27074\n",
            "Epoch: 01 [ 6084/20799 ( 29%)], Train Loss: 0.27073\n",
            "Epoch: 01 [ 6124/20799 ( 29%)], Train Loss: 0.26974\n",
            "Epoch: 01 [ 6164/20799 ( 30%)], Train Loss: 0.26840\n",
            "Epoch: 01 [ 6204/20799 ( 30%)], Train Loss: 0.26715\n",
            "Epoch: 01 [ 6244/20799 ( 30%)], Train Loss: 0.26671\n",
            "Epoch: 01 [ 6284/20799 ( 30%)], Train Loss: 0.26576\n",
            "Epoch: 01 [ 6324/20799 ( 30%)], Train Loss: 0.26504\n",
            "Epoch: 01 [ 6364/20799 ( 31%)], Train Loss: 0.26469\n",
            "Epoch: 01 [ 6404/20799 ( 31%)], Train Loss: 0.26383\n",
            "Epoch: 01 [ 6444/20799 ( 31%)], Train Loss: 0.26283\n",
            "Epoch: 01 [ 6484/20799 ( 31%)], Train Loss: 0.26181\n",
            "Epoch: 01 [ 6524/20799 ( 31%)], Train Loss: 0.26085\n",
            "Epoch: 01 [ 6564/20799 ( 32%)], Train Loss: 0.26062\n",
            "Epoch: 01 [ 6604/20799 ( 32%)], Train Loss: 0.25957\n",
            "Epoch: 01 [ 6644/20799 ( 32%)], Train Loss: 0.25959\n",
            "Epoch: 01 [ 6684/20799 ( 32%)], Train Loss: 0.26021\n",
            "Epoch: 01 [ 6724/20799 ( 32%)], Train Loss: 0.25932\n",
            "Epoch: 01 [ 6764/20799 ( 33%)], Train Loss: 0.25970\n",
            "Epoch: 01 [ 6804/20799 ( 33%)], Train Loss: 0.25973\n",
            "Epoch: 01 [ 6844/20799 ( 33%)], Train Loss: 0.25897\n",
            "Epoch: 01 [ 6884/20799 ( 33%)], Train Loss: 0.25880\n",
            "Epoch: 01 [ 6924/20799 ( 33%)], Train Loss: 0.25845\n",
            "Epoch: 01 [ 6964/20799 ( 33%)], Train Loss: 0.25808\n",
            "Epoch: 01 [ 7004/20799 ( 34%)], Train Loss: 0.25774\n",
            "Epoch: 01 [ 7044/20799 ( 34%)], Train Loss: 0.25680\n",
            "Epoch: 01 [ 7084/20799 ( 34%)], Train Loss: 0.25683\n",
            "Epoch: 01 [ 7124/20799 ( 34%)], Train Loss: 0.25733\n",
            "Epoch: 01 [ 7164/20799 ( 34%)], Train Loss: 0.25681\n",
            "Epoch: 01 [ 7204/20799 ( 35%)], Train Loss: 0.25621\n",
            "Epoch: 01 [ 7244/20799 ( 35%)], Train Loss: 0.25659\n",
            "Epoch: 01 [ 7284/20799 ( 35%)], Train Loss: 0.25740\n",
            "Epoch: 01 [ 7324/20799 ( 35%)], Train Loss: 0.25718\n",
            "Epoch: 01 [ 7364/20799 ( 35%)], Train Loss: 0.25760\n",
            "Epoch: 01 [ 7404/20799 ( 36%)], Train Loss: 0.25687\n",
            "Epoch: 01 [ 7444/20799 ( 36%)], Train Loss: 0.25740\n",
            "Epoch: 01 [ 7484/20799 ( 36%)], Train Loss: 0.25724\n",
            "Epoch: 01 [ 7524/20799 ( 36%)], Train Loss: 0.25611\n",
            "Epoch: 01 [ 7564/20799 ( 36%)], Train Loss: 0.25545\n",
            "Epoch: 01 [ 7604/20799 ( 37%)], Train Loss: 0.25453\n",
            "Epoch: 01 [ 7644/20799 ( 37%)], Train Loss: 0.25381\n",
            "Epoch: 01 [ 7684/20799 ( 37%)], Train Loss: 0.25376\n",
            "Epoch: 01 [ 7724/20799 ( 37%)], Train Loss: 0.25323\n",
            "Epoch: 01 [ 7764/20799 ( 37%)], Train Loss: 0.25253\n",
            "Epoch: 01 [ 7804/20799 ( 38%)], Train Loss: 0.25207\n",
            "Epoch: 01 [ 7844/20799 ( 38%)], Train Loss: 0.25208\n",
            "Epoch: 01 [ 7884/20799 ( 38%)], Train Loss: 0.25221\n",
            "Epoch: 01 [ 7924/20799 ( 38%)], Train Loss: 0.25197\n",
            "Epoch: 01 [ 7964/20799 ( 38%)], Train Loss: 0.25105\n",
            "Epoch: 01 [ 8004/20799 ( 38%)], Train Loss: 0.25058\n",
            "Epoch: 01 [ 8044/20799 ( 39%)], Train Loss: 0.24966\n",
            "Epoch: 01 [ 8084/20799 ( 39%)], Train Loss: 0.24916\n",
            "Epoch: 01 [ 8124/20799 ( 39%)], Train Loss: 0.24844\n",
            "Epoch: 01 [ 8164/20799 ( 39%)], Train Loss: 0.24794\n",
            "Epoch: 01 [ 8204/20799 ( 39%)], Train Loss: 0.24743\n",
            "Epoch: 01 [ 8244/20799 ( 40%)], Train Loss: 0.24660\n",
            "Epoch: 01 [ 8284/20799 ( 40%)], Train Loss: 0.24596\n",
            "Epoch: 01 [ 8324/20799 ( 40%)], Train Loss: 0.24609\n",
            "Epoch: 01 [ 8364/20799 ( 40%)], Train Loss: 0.24614\n",
            "Epoch: 01 [ 8404/20799 ( 40%)], Train Loss: 0.24514\n",
            "Epoch: 01 [ 8444/20799 ( 41%)], Train Loss: 0.24529\n",
            "Epoch: 01 [ 8484/20799 ( 41%)], Train Loss: 0.24568\n",
            "Epoch: 01 [ 8524/20799 ( 41%)], Train Loss: 0.24496\n",
            "Epoch: 01 [ 8564/20799 ( 41%)], Train Loss: 0.24460\n",
            "Epoch: 01 [ 8604/20799 ( 41%)], Train Loss: 0.24398\n",
            "Epoch: 01 [ 8644/20799 ( 42%)], Train Loss: 0.24357\n",
            "Epoch: 01 [ 8684/20799 ( 42%)], Train Loss: 0.24292\n",
            "Epoch: 01 [ 8724/20799 ( 42%)], Train Loss: 0.24218\n",
            "Epoch: 01 [ 8764/20799 ( 42%)], Train Loss: 0.24116\n",
            "Epoch: 01 [ 8804/20799 ( 42%)], Train Loss: 0.24116\n",
            "Epoch: 01 [ 8844/20799 ( 43%)], Train Loss: 0.24091\n",
            "Epoch: 01 [ 8884/20799 ( 43%)], Train Loss: 0.24082\n",
            "Epoch: 01 [ 8924/20799 ( 43%)], Train Loss: 0.23996\n",
            "Epoch: 01 [ 8964/20799 ( 43%)], Train Loss: 0.23938\n",
            "Epoch: 01 [ 9004/20799 ( 43%)], Train Loss: 0.23891\n",
            "Epoch: 01 [ 9044/20799 ( 43%)], Train Loss: 0.23826\n",
            "Epoch: 01 [ 9084/20799 ( 44%)], Train Loss: 0.23816\n",
            "Epoch: 01 [ 9124/20799 ( 44%)], Train Loss: 0.23784\n",
            "Epoch: 01 [ 9164/20799 ( 44%)], Train Loss: 0.23760\n",
            "Epoch: 01 [ 9204/20799 ( 44%)], Train Loss: 0.23730\n",
            "Epoch: 01 [ 9244/20799 ( 44%)], Train Loss: 0.23669\n",
            "Epoch: 01 [ 9284/20799 ( 45%)], Train Loss: 0.23706\n",
            "Epoch: 01 [ 9324/20799 ( 45%)], Train Loss: 0.23778\n",
            "Epoch: 01 [ 9364/20799 ( 45%)], Train Loss: 0.23766\n",
            "Epoch: 01 [ 9404/20799 ( 45%)], Train Loss: 0.23769\n",
            "Epoch: 01 [ 9444/20799 ( 45%)], Train Loss: 0.23690\n",
            "Epoch: 01 [ 9484/20799 ( 46%)], Train Loss: 0.23654\n",
            "Epoch: 01 [ 9524/20799 ( 46%)], Train Loss: 0.23610\n",
            "Epoch: 01 [ 9564/20799 ( 46%)], Train Loss: 0.23558\n",
            "Epoch: 01 [ 9604/20799 ( 46%)], Train Loss: 0.23487\n",
            "Epoch: 01 [ 9644/20799 ( 46%)], Train Loss: 0.23478\n",
            "Epoch: 01 [ 9684/20799 ( 47%)], Train Loss: 0.23440\n",
            "Epoch: 01 [ 9724/20799 ( 47%)], Train Loss: 0.23364\n",
            "Epoch: 01 [ 9764/20799 ( 47%)], Train Loss: 0.23302\n",
            "Epoch: 01 [ 9804/20799 ( 47%)], Train Loss: 0.23271\n",
            "Epoch: 01 [ 9844/20799 ( 47%)], Train Loss: 0.23254\n",
            "Epoch: 01 [ 9884/20799 ( 48%)], Train Loss: 0.23214\n",
            "Epoch: 01 [ 9924/20799 ( 48%)], Train Loss: 0.23152\n",
            "Epoch: 01 [ 9964/20799 ( 48%)], Train Loss: 0.23151\n",
            "Epoch: 01 [10004/20799 ( 48%)], Train Loss: 0.23130\n",
            "Epoch: 01 [10044/20799 ( 48%)], Train Loss: 0.23109\n",
            "Epoch: 01 [10084/20799 ( 48%)], Train Loss: 0.23086\n",
            "Epoch: 01 [10124/20799 ( 49%)], Train Loss: 0.23043\n",
            "Epoch: 01 [10164/20799 ( 49%)], Train Loss: 0.23015\n",
            "Epoch: 01 [10204/20799 ( 49%)], Train Loss: 0.22961\n",
            "Epoch: 01 [10244/20799 ( 49%)], Train Loss: 0.22971\n",
            "Epoch: 01 [10284/20799 ( 49%)], Train Loss: 0.22996\n",
            "Epoch: 01 [10324/20799 ( 50%)], Train Loss: 0.22984\n",
            "Epoch: 01 [10364/20799 ( 50%)], Train Loss: 0.22933\n",
            "Epoch: 01 [10404/20799 ( 50%)], Train Loss: 0.22877\n",
            "Epoch: 01 [10444/20799 ( 50%)], Train Loss: 0.22849\n",
            "Epoch: 01 [10484/20799 ( 50%)], Train Loss: 0.22794\n",
            "Epoch: 01 [10524/20799 ( 51%)], Train Loss: 0.22879\n",
            "Epoch: 01 [10564/20799 ( 51%)], Train Loss: 0.22821\n",
            "Epoch: 01 [10604/20799 ( 51%)], Train Loss: 0.22824\n",
            "Epoch: 01 [10644/20799 ( 51%)], Train Loss: 0.22800\n",
            "Epoch: 01 [10684/20799 ( 51%)], Train Loss: 0.22751\n",
            "Epoch: 01 [10724/20799 ( 52%)], Train Loss: 0.22743\n",
            "Epoch: 01 [10764/20799 ( 52%)], Train Loss: 0.22691\n",
            "Epoch: 01 [10804/20799 ( 52%)], Train Loss: 0.22654\n",
            "Epoch: 01 [10844/20799 ( 52%)], Train Loss: 0.22607\n",
            "Epoch: 01 [10884/20799 ( 52%)], Train Loss: 0.22591\n",
            "Epoch: 01 [10924/20799 ( 53%)], Train Loss: 0.22608\n",
            "Epoch: 01 [10964/20799 ( 53%)], Train Loss: 0.22577\n",
            "Epoch: 01 [11004/20799 ( 53%)], Train Loss: 0.22581\n",
            "Epoch: 01 [11044/20799 ( 53%)], Train Loss: 0.22550\n",
            "Epoch: 01 [11084/20799 ( 53%)], Train Loss: 0.22541\n",
            "Epoch: 01 [11124/20799 ( 53%)], Train Loss: 0.22521\n",
            "Epoch: 01 [11164/20799 ( 54%)], Train Loss: 0.22538\n",
            "Epoch: 01 [11204/20799 ( 54%)], Train Loss: 0.22517\n",
            "Epoch: 01 [11244/20799 ( 54%)], Train Loss: 0.22478\n",
            "Epoch: 01 [11284/20799 ( 54%)], Train Loss: 0.22495\n",
            "Epoch: 01 [11324/20799 ( 54%)], Train Loss: 0.22456\n",
            "Epoch: 01 [11364/20799 ( 55%)], Train Loss: 0.22447\n",
            "Epoch: 01 [11404/20799 ( 55%)], Train Loss: 0.22433\n",
            "Epoch: 01 [11444/20799 ( 55%)], Train Loss: 0.22399\n",
            "Epoch: 01 [11484/20799 ( 55%)], Train Loss: 0.22363\n",
            "Epoch: 01 [11524/20799 ( 55%)], Train Loss: 0.22364\n",
            "Epoch: 01 [11564/20799 ( 56%)], Train Loss: 0.22324\n",
            "Epoch: 01 [11604/20799 ( 56%)], Train Loss: 0.22302\n",
            "Epoch: 01 [11644/20799 ( 56%)], Train Loss: 0.22288\n",
            "Epoch: 01 [11684/20799 ( 56%)], Train Loss: 0.22267\n",
            "Epoch: 01 [11724/20799 ( 56%)], Train Loss: 0.22315\n",
            "Epoch: 01 [11764/20799 ( 57%)], Train Loss: 0.22324\n",
            "Epoch: 01 [11804/20799 ( 57%)], Train Loss: 0.22317\n",
            "Epoch: 01 [11844/20799 ( 57%)], Train Loss: 0.22290\n",
            "Epoch: 01 [11884/20799 ( 57%)], Train Loss: 0.22269\n",
            "Epoch: 01 [11924/20799 ( 57%)], Train Loss: 0.22224\n",
            "Epoch: 01 [11964/20799 ( 58%)], Train Loss: 0.22186\n",
            "Epoch: 01 [12004/20799 ( 58%)], Train Loss: 0.22217\n",
            "Epoch: 01 [12044/20799 ( 58%)], Train Loss: 0.22164\n",
            "Epoch: 01 [12084/20799 ( 58%)], Train Loss: 0.22160\n",
            "Epoch: 01 [12124/20799 ( 58%)], Train Loss: 0.22227\n",
            "Epoch: 01 [12164/20799 ( 58%)], Train Loss: 0.22210\n",
            "Epoch: 01 [12204/20799 ( 59%)], Train Loss: 0.22174\n",
            "Epoch: 01 [12244/20799 ( 59%)], Train Loss: 0.22152\n",
            "Epoch: 01 [12284/20799 ( 59%)], Train Loss: 0.22134\n",
            "Epoch: 01 [12324/20799 ( 59%)], Train Loss: 0.22092\n",
            "Epoch: 01 [12364/20799 ( 59%)], Train Loss: 0.22064\n",
            "Epoch: 01 [12404/20799 ( 60%)], Train Loss: 0.22080\n",
            "Epoch: 01 [12444/20799 ( 60%)], Train Loss: 0.22065\n",
            "Epoch: 01 [12484/20799 ( 60%)], Train Loss: 0.22048\n",
            "Epoch: 01 [12524/20799 ( 60%)], Train Loss: 0.22023\n",
            "Epoch: 01 [12564/20799 ( 60%)], Train Loss: 0.22006\n",
            "Epoch: 01 [12604/20799 ( 61%)], Train Loss: 0.21981\n",
            "Epoch: 01 [12644/20799 ( 61%)], Train Loss: 0.21950\n",
            "Epoch: 01 [12684/20799 ( 61%)], Train Loss: 0.21926\n",
            "Epoch: 01 [12724/20799 ( 61%)], Train Loss: 0.21896\n",
            "Epoch: 01 [12764/20799 ( 61%)], Train Loss: 0.21865\n",
            "Epoch: 01 [12804/20799 ( 62%)], Train Loss: 0.21853\n",
            "Epoch: 01 [12844/20799 ( 62%)], Train Loss: 0.21826\n",
            "Epoch: 01 [12884/20799 ( 62%)], Train Loss: 0.21773\n",
            "Epoch: 01 [12924/20799 ( 62%)], Train Loss: 0.21751\n",
            "Epoch: 01 [12964/20799 ( 62%)], Train Loss: 0.21697\n",
            "Epoch: 01 [13004/20799 ( 63%)], Train Loss: 0.21668\n",
            "Epoch: 01 [13044/20799 ( 63%)], Train Loss: 0.21630\n",
            "Epoch: 01 [13084/20799 ( 63%)], Train Loss: 0.21604\n",
            "Epoch: 01 [13124/20799 ( 63%)], Train Loss: 0.21567\n",
            "Epoch: 01 [13164/20799 ( 63%)], Train Loss: 0.21529\n",
            "Epoch: 01 [13204/20799 ( 63%)], Train Loss: 0.21525\n",
            "Epoch: 01 [13244/20799 ( 64%)], Train Loss: 0.21489\n",
            "Epoch: 01 [13284/20799 ( 64%)], Train Loss: 0.21441\n",
            "Epoch: 01 [13324/20799 ( 64%)], Train Loss: 0.21423\n",
            "Epoch: 01 [13364/20799 ( 64%)], Train Loss: 0.21432\n",
            "Epoch: 01 [13404/20799 ( 64%)], Train Loss: 0.21443\n",
            "Epoch: 01 [13444/20799 ( 65%)], Train Loss: 0.21429\n",
            "Epoch: 01 [13484/20799 ( 65%)], Train Loss: 0.21425\n",
            "Epoch: 01 [13524/20799 ( 65%)], Train Loss: 0.21444\n",
            "Epoch: 01 [13564/20799 ( 65%)], Train Loss: 0.21438\n",
            "Epoch: 01 [13604/20799 ( 65%)], Train Loss: 0.21401\n",
            "Epoch: 01 [13644/20799 ( 66%)], Train Loss: 0.21369\n",
            "Epoch: 01 [13684/20799 ( 66%)], Train Loss: 0.21367\n",
            "Epoch: 01 [13724/20799 ( 66%)], Train Loss: 0.21343\n",
            "Epoch: 01 [13764/20799 ( 66%)], Train Loss: 0.21305\n",
            "Epoch: 01 [13804/20799 ( 66%)], Train Loss: 0.21277\n",
            "Epoch: 01 [13844/20799 ( 67%)], Train Loss: 0.21249\n",
            "Epoch: 01 [13884/20799 ( 67%)], Train Loss: 0.21214\n",
            "Epoch: 01 [13924/20799 ( 67%)], Train Loss: 0.21204\n",
            "Epoch: 01 [13964/20799 ( 67%)], Train Loss: 0.21174\n",
            "Epoch: 01 [14004/20799 ( 67%)], Train Loss: 0.21174\n",
            "Epoch: 01 [14044/20799 ( 68%)], Train Loss: 0.21122\n",
            "Epoch: 01 [14084/20799 ( 68%)], Train Loss: 0.21084\n",
            "Epoch: 01 [14124/20799 ( 68%)], Train Loss: 0.21063\n",
            "Epoch: 01 [14164/20799 ( 68%)], Train Loss: 0.21015\n",
            "Epoch: 01 [14204/20799 ( 68%)], Train Loss: 0.20980\n",
            "Epoch: 01 [14244/20799 ( 68%)], Train Loss: 0.20975\n",
            "Epoch: 01 [14284/20799 ( 69%)], Train Loss: 0.20967\n",
            "Epoch: 01 [14324/20799 ( 69%)], Train Loss: 0.20975\n",
            "Epoch: 01 [14364/20799 ( 69%)], Train Loss: 0.20969\n",
            "Epoch: 01 [14404/20799 ( 69%)], Train Loss: 0.21018\n",
            "Epoch: 01 [14444/20799 ( 69%)], Train Loss: 0.20997\n",
            "Epoch: 01 [14484/20799 ( 70%)], Train Loss: 0.21012\n",
            "Epoch: 01 [14524/20799 ( 70%)], Train Loss: 0.20977\n",
            "Epoch: 01 [14564/20799 ( 70%)], Train Loss: 0.20948\n",
            "Epoch: 01 [14604/20799 ( 70%)], Train Loss: 0.20961\n",
            "Epoch: 01 [14644/20799 ( 70%)], Train Loss: 0.20931\n",
            "Epoch: 01 [14684/20799 ( 71%)], Train Loss: 0.20930\n",
            "Epoch: 01 [14724/20799 ( 71%)], Train Loss: 0.20900\n",
            "Epoch: 01 [14764/20799 ( 71%)], Train Loss: 0.20877\n",
            "Epoch: 01 [14804/20799 ( 71%)], Train Loss: 0.20854\n",
            "Epoch: 01 [14844/20799 ( 71%)], Train Loss: 0.20836\n",
            "Epoch: 01 [14884/20799 ( 72%)], Train Loss: 0.20816\n",
            "Epoch: 01 [14924/20799 ( 72%)], Train Loss: 0.20813\n",
            "Epoch: 01 [14964/20799 ( 72%)], Train Loss: 0.20781\n",
            "Epoch: 01 [15004/20799 ( 72%)], Train Loss: 0.20787\n",
            "Epoch: 01 [15044/20799 ( 72%)], Train Loss: 0.20796\n",
            "Epoch: 01 [15084/20799 ( 73%)], Train Loss: 0.20808\n",
            "Epoch: 01 [15124/20799 ( 73%)], Train Loss: 0.20791\n",
            "Epoch: 01 [15164/20799 ( 73%)], Train Loss: 0.20774\n",
            "Epoch: 01 [15204/20799 ( 73%)], Train Loss: 0.20781\n",
            "Epoch: 01 [15244/20799 ( 73%)], Train Loss: 0.20753\n",
            "Epoch: 01 [15284/20799 ( 73%)], Train Loss: 0.20710\n",
            "Epoch: 01 [15324/20799 ( 74%)], Train Loss: 0.20694\n",
            "Epoch: 01 [15364/20799 ( 74%)], Train Loss: 0.20687\n",
            "Epoch: 01 [15404/20799 ( 74%)], Train Loss: 0.20667\n",
            "Epoch: 01 [15444/20799 ( 74%)], Train Loss: 0.20655\n",
            "Epoch: 01 [15484/20799 ( 74%)], Train Loss: 0.20646\n",
            "Epoch: 01 [15524/20799 ( 75%)], Train Loss: 0.20619\n",
            "Epoch: 01 [15564/20799 ( 75%)], Train Loss: 0.20592\n",
            "Epoch: 01 [15604/20799 ( 75%)], Train Loss: 0.20555\n",
            "Epoch: 01 [15644/20799 ( 75%)], Train Loss: 0.20586\n",
            "Epoch: 01 [15684/20799 ( 75%)], Train Loss: 0.20578\n",
            "Epoch: 01 [15724/20799 ( 76%)], Train Loss: 0.20585\n",
            "Epoch: 01 [15764/20799 ( 76%)], Train Loss: 0.20583\n",
            "Epoch: 01 [15804/20799 ( 76%)], Train Loss: 0.20590\n",
            "Epoch: 01 [15844/20799 ( 76%)], Train Loss: 0.20566\n",
            "Epoch: 01 [15884/20799 ( 76%)], Train Loss: 0.20563\n",
            "Epoch: 01 [15924/20799 ( 77%)], Train Loss: 0.20552\n",
            "Epoch: 01 [15964/20799 ( 77%)], Train Loss: 0.20537\n",
            "Epoch: 01 [16004/20799 ( 77%)], Train Loss: 0.20548\n",
            "Epoch: 01 [16044/20799 ( 77%)], Train Loss: 0.20521\n",
            "Epoch: 01 [16084/20799 ( 77%)], Train Loss: 0.20526\n",
            "Epoch: 01 [16124/20799 ( 78%)], Train Loss: 0.20490\n",
            "Epoch: 01 [16164/20799 ( 78%)], Train Loss: 0.20495\n",
            "Epoch: 01 [16204/20799 ( 78%)], Train Loss: 0.20470\n",
            "Epoch: 01 [16244/20799 ( 78%)], Train Loss: 0.20462\n",
            "Epoch: 01 [16284/20799 ( 78%)], Train Loss: 0.20444\n",
            "Epoch: 01 [16324/20799 ( 78%)], Train Loss: 0.20436\n",
            "Epoch: 01 [16364/20799 ( 79%)], Train Loss: 0.20398\n",
            "Epoch: 01 [16404/20799 ( 79%)], Train Loss: 0.20372\n",
            "Epoch: 01 [16444/20799 ( 79%)], Train Loss: 0.20355\n",
            "Epoch: 01 [16484/20799 ( 79%)], Train Loss: 0.20358\n",
            "Epoch: 01 [16524/20799 ( 79%)], Train Loss: 0.20337\n",
            "Epoch: 01 [16564/20799 ( 80%)], Train Loss: 0.20293\n",
            "Epoch: 01 [16604/20799 ( 80%)], Train Loss: 0.20263\n",
            "Epoch: 01 [16644/20799 ( 80%)], Train Loss: 0.20264\n",
            "Epoch: 01 [16684/20799 ( 80%)], Train Loss: 0.20239\n",
            "Epoch: 01 [16724/20799 ( 80%)], Train Loss: 0.20231\n",
            "Epoch: 01 [16764/20799 ( 81%)], Train Loss: 0.20219\n",
            "Epoch: 01 [16804/20799 ( 81%)], Train Loss: 0.20196\n",
            "Epoch: 01 [16844/20799 ( 81%)], Train Loss: 0.20194\n",
            "Epoch: 01 [16884/20799 ( 81%)], Train Loss: 0.20173\n",
            "Epoch: 01 [16924/20799 ( 81%)], Train Loss: 0.20197\n",
            "Epoch: 01 [16964/20799 ( 82%)], Train Loss: 0.20181\n",
            "Epoch: 01 [17004/20799 ( 82%)], Train Loss: 0.20170\n",
            "Epoch: 01 [17044/20799 ( 82%)], Train Loss: 0.20156\n",
            "Epoch: 01 [17084/20799 ( 82%)], Train Loss: 0.20150\n",
            "Epoch: 01 [17124/20799 ( 82%)], Train Loss: 0.20164\n",
            "Epoch: 01 [17164/20799 ( 83%)], Train Loss: 0.20155\n",
            "Epoch: 01 [17204/20799 ( 83%)], Train Loss: 0.20162\n",
            "Epoch: 01 [17244/20799 ( 83%)], Train Loss: 0.20136\n",
            "Epoch: 01 [17284/20799 ( 83%)], Train Loss: 0.20131\n",
            "Epoch: 01 [17324/20799 ( 83%)], Train Loss: 0.20118\n",
            "Epoch: 01 [17364/20799 ( 83%)], Train Loss: 0.20099\n",
            "Epoch: 01 [17404/20799 ( 84%)], Train Loss: 0.20091\n",
            "Epoch: 01 [17444/20799 ( 84%)], Train Loss: 0.20091\n",
            "Epoch: 01 [17484/20799 ( 84%)], Train Loss: 0.20074\n",
            "Epoch: 01 [17524/20799 ( 84%)], Train Loss: 0.20114\n",
            "Epoch: 01 [17564/20799 ( 84%)], Train Loss: 0.20085\n",
            "Epoch: 01 [17604/20799 ( 85%)], Train Loss: 0.20080\n",
            "Epoch: 01 [17644/20799 ( 85%)], Train Loss: 0.20064\n",
            "Epoch: 01 [17684/20799 ( 85%)], Train Loss: 0.20056\n",
            "Epoch: 01 [17724/20799 ( 85%)], Train Loss: 0.20039\n",
            "Epoch: 01 [17764/20799 ( 85%)], Train Loss: 0.20032\n",
            "Epoch: 01 [17804/20799 ( 86%)], Train Loss: 0.19999\n",
            "Epoch: 01 [17844/20799 ( 86%)], Train Loss: 0.20008\n",
            "Epoch: 01 [17884/20799 ( 86%)], Train Loss: 0.20018\n",
            "Epoch: 01 [17924/20799 ( 86%)], Train Loss: 0.20031\n",
            "Epoch: 01 [17964/20799 ( 86%)], Train Loss: 0.20036\n",
            "Epoch: 01 [18004/20799 ( 87%)], Train Loss: 0.20012\n",
            "Epoch: 01 [18044/20799 ( 87%)], Train Loss: 0.19992\n",
            "Epoch: 01 [18084/20799 ( 87%)], Train Loss: 0.19987\n",
            "Epoch: 01 [18124/20799 ( 87%)], Train Loss: 0.19962\n",
            "Epoch: 01 [18164/20799 ( 87%)], Train Loss: 0.19938\n",
            "Epoch: 01 [18204/20799 ( 88%)], Train Loss: 0.19907\n",
            "Epoch: 01 [18244/20799 ( 88%)], Train Loss: 0.19873\n",
            "Epoch: 01 [18284/20799 ( 88%)], Train Loss: 0.19860\n",
            "Epoch: 01 [18324/20799 ( 88%)], Train Loss: 0.19843\n",
            "Epoch: 01 [18364/20799 ( 88%)], Train Loss: 0.19817\n",
            "Epoch: 01 [18404/20799 ( 88%)], Train Loss: 0.19792\n",
            "Epoch: 01 [18444/20799 ( 89%)], Train Loss: 0.19776\n",
            "Epoch: 01 [18484/20799 ( 89%)], Train Loss: 0.19764\n",
            "Epoch: 01 [18524/20799 ( 89%)], Train Loss: 0.19746\n",
            "Epoch: 01 [18564/20799 ( 89%)], Train Loss: 0.19749\n",
            "Epoch: 01 [18604/20799 ( 89%)], Train Loss: 0.19737\n",
            "Epoch: 01 [18644/20799 ( 90%)], Train Loss: 0.19740\n",
            "Epoch: 01 [18684/20799 ( 90%)], Train Loss: 0.19739\n",
            "Epoch: 01 [18724/20799 ( 90%)], Train Loss: 0.19744\n",
            "Epoch: 01 [18764/20799 ( 90%)], Train Loss: 0.19717\n",
            "Epoch: 01 [18804/20799 ( 90%)], Train Loss: 0.19710\n",
            "Epoch: 01 [18844/20799 ( 91%)], Train Loss: 0.19728\n",
            "Epoch: 01 [18884/20799 ( 91%)], Train Loss: 0.19722\n",
            "Epoch: 01 [18924/20799 ( 91%)], Train Loss: 0.19698\n",
            "Epoch: 01 [18964/20799 ( 91%)], Train Loss: 0.19678\n",
            "Epoch: 01 [19004/20799 ( 91%)], Train Loss: 0.19664\n",
            "Epoch: 01 [19044/20799 ( 92%)], Train Loss: 0.19673\n",
            "Epoch: 01 [19084/20799 ( 92%)], Train Loss: 0.19666\n",
            "Epoch: 01 [19124/20799 ( 92%)], Train Loss: 0.19651\n",
            "Epoch: 01 [19164/20799 ( 92%)], Train Loss: 0.19686\n",
            "Epoch: 01 [19204/20799 ( 92%)], Train Loss: 0.19676\n",
            "Epoch: 01 [19244/20799 ( 93%)], Train Loss: 0.19688\n",
            "Epoch: 01 [19284/20799 ( 93%)], Train Loss: 0.19685\n",
            "Epoch: 01 [19324/20799 ( 93%)], Train Loss: 0.19661\n",
            "Epoch: 01 [19364/20799 ( 93%)], Train Loss: 0.19668\n",
            "Epoch: 01 [19404/20799 ( 93%)], Train Loss: 0.19650\n",
            "Epoch: 01 [19444/20799 ( 93%)], Train Loss: 0.19654\n",
            "Epoch: 01 [19484/20799 ( 94%)], Train Loss: 0.19624\n",
            "Epoch: 01 [19524/20799 ( 94%)], Train Loss: 0.19609\n",
            "Epoch: 01 [19564/20799 ( 94%)], Train Loss: 0.19619\n",
            "Epoch: 01 [19604/20799 ( 94%)], Train Loss: 0.19614\n",
            "Epoch: 01 [19644/20799 ( 94%)], Train Loss: 0.19616\n",
            "Epoch: 01 [19684/20799 ( 95%)], Train Loss: 0.19588\n",
            "Epoch: 01 [19724/20799 ( 95%)], Train Loss: 0.19565\n",
            "Epoch: 01 [19764/20799 ( 95%)], Train Loss: 0.19545\n",
            "Epoch: 01 [19804/20799 ( 95%)], Train Loss: 0.19517\n",
            "Epoch: 01 [19844/20799 ( 95%)], Train Loss: 0.19517\n",
            "Epoch: 01 [19884/20799 ( 96%)], Train Loss: 0.19513\n",
            "Epoch: 01 [19924/20799 ( 96%)], Train Loss: 0.19494\n",
            "Epoch: 01 [19964/20799 ( 96%)], Train Loss: 0.19488\n",
            "Epoch: 01 [20004/20799 ( 96%)], Train Loss: 0.19466\n",
            "Epoch: 01 [20044/20799 ( 96%)], Train Loss: 0.19460\n",
            "Epoch: 01 [20084/20799 ( 97%)], Train Loss: 0.19444\n",
            "Epoch: 01 [20124/20799 ( 97%)], Train Loss: 0.19441\n",
            "Epoch: 01 [20164/20799 ( 97%)], Train Loss: 0.19418\n",
            "Epoch: 01 [20204/20799 ( 97%)], Train Loss: 0.19405\n",
            "Epoch: 01 [20244/20799 ( 97%)], Train Loss: 0.19401\n",
            "Epoch: 01 [20284/20799 ( 98%)], Train Loss: 0.19381\n",
            "Epoch: 01 [20324/20799 ( 98%)], Train Loss: 0.19384\n",
            "Epoch: 01 [20364/20799 ( 98%)], Train Loss: 0.19371\n",
            "Epoch: 01 [20404/20799 ( 98%)], Train Loss: 0.19350\n",
            "Epoch: 01 [20444/20799 ( 98%)], Train Loss: 0.19355\n",
            "Epoch: 01 [20484/20799 ( 98%)], Train Loss: 0.19388\n",
            "Epoch: 01 [20524/20799 ( 99%)], Train Loss: 0.19375\n",
            "Epoch: 01 [20564/20799 ( 99%)], Train Loss: 0.19394\n",
            "Epoch: 01 [20604/20799 ( 99%)], Train Loss: 0.19390\n",
            "Epoch: 01 [20644/20799 ( 99%)], Train Loss: 0.19382\n",
            "Epoch: 01 [20684/20799 ( 99%)], Train Loss: 0.19372\n",
            "Epoch: 01 [20724/20799 (100%)], Train Loss: 0.19378\n",
            "Epoch: 01 [20764/20799 (100%)], Train Loss: 0.19384\n",
            "Epoch: 01 [20799/20799 (100%)], Train Loss: 0.19379\n",
            "----Validation Results Summary----\n",
            "Epoch: [1] Valid Loss: 0.33778\n",
            "\n",
            "Total Training Time: 6109.529581546783secs, Average Training Time per Epoch: 3054.7647907733917secs.\n",
            "Total Validation Time: 249.47518754005432secs, Average Validation Time per Epoch: 124.73759377002716secs.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "FOLD: 4\n",
            "--------------------------------------------------\n",
            "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
            "Num examples Train= 20839, Num examples Valid=2757\n",
            "Total Training Steps: 5210, Total Warmup Steps: 521\n",
            "Epoch: 00 [    4/20839 (  0%)], Train Loss: 4.47503\n",
            "Epoch: 00 [   44/20839 (  0%)], Train Loss: 4.42105\n",
            "Epoch: 00 [   84/20839 (  0%)], Train Loss: 4.19536\n",
            "Epoch: 00 [  124/20839 (  1%)], Train Loss: 4.00320\n",
            "Epoch: 00 [  164/20839 (  1%)], Train Loss: 3.87595\n",
            "Epoch: 00 [  204/20839 (  1%)], Train Loss: 3.72880\n",
            "Epoch: 00 [  244/20839 (  1%)], Train Loss: 3.62490\n",
            "Epoch: 00 [  284/20839 (  1%)], Train Loss: 3.45700\n",
            "Epoch: 00 [  324/20839 (  2%)], Train Loss: 3.29618\n",
            "Epoch: 00 [  364/20839 (  2%)], Train Loss: 3.13758\n",
            "Epoch: 00 [  404/20839 (  2%)], Train Loss: 2.98047\n",
            "Epoch: 00 [  444/20839 (  2%)], Train Loss: 2.90133\n",
            "Epoch: 00 [  484/20839 (  2%)], Train Loss: 2.78203\n",
            "Epoch: 00 [  524/20839 (  3%)], Train Loss: 2.65820\n",
            "Epoch: 00 [  564/20839 (  3%)], Train Loss: 2.57492\n",
            "Epoch: 00 [  604/20839 (  3%)], Train Loss: 2.47615\n",
            "Epoch: 00 [  644/20839 (  3%)], Train Loss: 2.36704\n",
            "Epoch: 00 [  684/20839 (  3%)], Train Loss: 2.28768\n",
            "Epoch: 00 [  724/20839 (  3%)], Train Loss: 2.22038\n",
            "Epoch: 00 [  764/20839 (  4%)], Train Loss: 2.16480\n",
            "Epoch: 00 [  804/20839 (  4%)], Train Loss: 2.10973\n",
            "Epoch: 00 [  844/20839 (  4%)], Train Loss: 2.03837\n",
            "Epoch: 00 [  884/20839 (  4%)], Train Loss: 1.96719\n",
            "Epoch: 00 [  924/20839 (  4%)], Train Loss: 1.90969\n",
            "Epoch: 00 [  964/20839 (  5%)], Train Loss: 1.86453\n",
            "Epoch: 00 [ 1004/20839 (  5%)], Train Loss: 1.81781\n",
            "Epoch: 00 [ 1044/20839 (  5%)], Train Loss: 1.76272\n",
            "Epoch: 00 [ 1084/20839 (  5%)], Train Loss: 1.72066\n",
            "Epoch: 00 [ 1124/20839 (  5%)], Train Loss: 1.69150\n",
            "Epoch: 00 [ 1164/20839 (  6%)], Train Loss: 1.66535\n",
            "Epoch: 00 [ 1204/20839 (  6%)], Train Loss: 1.62888\n",
            "Epoch: 00 [ 1244/20839 (  6%)], Train Loss: 1.58675\n",
            "Epoch: 00 [ 1284/20839 (  6%)], Train Loss: 1.56679\n",
            "Epoch: 00 [ 1324/20839 (  6%)], Train Loss: 1.53705\n",
            "Epoch: 00 [ 1364/20839 (  7%)], Train Loss: 1.51647\n",
            "Epoch: 00 [ 1404/20839 (  7%)], Train Loss: 1.49597\n",
            "Epoch: 00 [ 1444/20839 (  7%)], Train Loss: 1.47307\n",
            "Epoch: 00 [ 1484/20839 (  7%)], Train Loss: 1.44897\n",
            "Epoch: 00 [ 1524/20839 (  7%)], Train Loss: 1.42535\n",
            "Epoch: 00 [ 1564/20839 (  8%)], Train Loss: 1.41452\n",
            "Epoch: 00 [ 1604/20839 (  8%)], Train Loss: 1.40944\n",
            "Epoch: 00 [ 1644/20839 (  8%)], Train Loss: 1.39222\n",
            "Epoch: 00 [ 1684/20839 (  8%)], Train Loss: 1.37277\n",
            "Epoch: 00 [ 1724/20839 (  8%)], Train Loss: 1.35721\n",
            "Epoch: 00 [ 1764/20839 (  8%)], Train Loss: 1.33154\n",
            "Epoch: 00 [ 1804/20839 (  9%)], Train Loss: 1.31166\n",
            "Epoch: 00 [ 1844/20839 (  9%)], Train Loss: 1.29311\n",
            "Epoch: 00 [ 1884/20839 (  9%)], Train Loss: 1.28254\n",
            "Epoch: 00 [ 1924/20839 (  9%)], Train Loss: 1.26536\n",
            "Epoch: 00 [ 1964/20839 (  9%)], Train Loss: 1.25239\n",
            "Epoch: 00 [ 2004/20839 ( 10%)], Train Loss: 1.24058\n",
            "Epoch: 00 [ 2044/20839 ( 10%)], Train Loss: 1.22787\n",
            "Epoch: 00 [ 2084/20839 ( 10%)], Train Loss: 1.21855\n",
            "Epoch: 00 [ 2124/20839 ( 10%)], Train Loss: 1.20632\n",
            "Epoch: 00 [ 2164/20839 ( 10%)], Train Loss: 1.19469\n",
            "Epoch: 00 [ 2204/20839 ( 11%)], Train Loss: 1.18448\n",
            "Epoch: 00 [ 2244/20839 ( 11%)], Train Loss: 1.17719\n",
            "Epoch: 00 [ 2284/20839 ( 11%)], Train Loss: 1.17021\n",
            "Epoch: 00 [ 2324/20839 ( 11%)], Train Loss: 1.15494\n",
            "Epoch: 00 [ 2364/20839 ( 11%)], Train Loss: 1.14280\n",
            "Epoch: 00 [ 2404/20839 ( 12%)], Train Loss: 1.13102\n",
            "Epoch: 00 [ 2444/20839 ( 12%)], Train Loss: 1.12241\n",
            "Epoch: 00 [ 2484/20839 ( 12%)], Train Loss: 1.11335\n",
            "Epoch: 00 [ 2524/20839 ( 12%)], Train Loss: 1.10576\n",
            "Epoch: 00 [ 2564/20839 ( 12%)], Train Loss: 1.10368\n",
            "Epoch: 00 [ 2604/20839 ( 12%)], Train Loss: 1.09759\n",
            "Epoch: 00 [ 2644/20839 ( 13%)], Train Loss: 1.08750\n",
            "Epoch: 00 [ 2684/20839 ( 13%)], Train Loss: 1.07995\n",
            "Epoch: 00 [ 2724/20839 ( 13%)], Train Loss: 1.07444\n",
            "Epoch: 00 [ 2764/20839 ( 13%)], Train Loss: 1.06575\n",
            "Epoch: 00 [ 2804/20839 ( 13%)], Train Loss: 1.05867\n",
            "Epoch: 00 [ 2844/20839 ( 14%)], Train Loss: 1.04958\n",
            "Epoch: 00 [ 2884/20839 ( 14%)], Train Loss: 1.04534\n",
            "Epoch: 00 [ 2924/20839 ( 14%)], Train Loss: 1.04232\n",
            "Epoch: 00 [ 2964/20839 ( 14%)], Train Loss: 1.03822\n",
            "Epoch: 00 [ 3004/20839 ( 14%)], Train Loss: 1.03032\n",
            "Epoch: 00 [ 3044/20839 ( 15%)], Train Loss: 1.02302\n",
            "Epoch: 00 [ 3084/20839 ( 15%)], Train Loss: 1.01507\n",
            "Epoch: 00 [ 3124/20839 ( 15%)], Train Loss: 1.01209\n",
            "Epoch: 00 [ 3164/20839 ( 15%)], Train Loss: 1.00687\n",
            "Epoch: 00 [ 3204/20839 ( 15%)], Train Loss: 1.00149\n",
            "Epoch: 00 [ 3244/20839 ( 16%)], Train Loss: 0.99679\n",
            "Epoch: 00 [ 3284/20839 ( 16%)], Train Loss: 0.99384\n",
            "Epoch: 00 [ 3324/20839 ( 16%)], Train Loss: 0.98722\n",
            "Epoch: 00 [ 3364/20839 ( 16%)], Train Loss: 0.97987\n",
            "Epoch: 00 [ 3404/20839 ( 16%)], Train Loss: 0.97242\n",
            "Epoch: 00 [ 3444/20839 ( 17%)], Train Loss: 0.96811\n",
            "Epoch: 00 [ 3484/20839 ( 17%)], Train Loss: 0.96346\n",
            "Epoch: 00 [ 3524/20839 ( 17%)], Train Loss: 0.95560\n",
            "Epoch: 00 [ 3564/20839 ( 17%)], Train Loss: 0.95195\n",
            "Epoch: 00 [ 3604/20839 ( 17%)], Train Loss: 0.94583\n",
            "Epoch: 00 [ 3644/20839 ( 17%)], Train Loss: 0.93974\n",
            "Epoch: 00 [ 3684/20839 ( 18%)], Train Loss: 0.93508\n",
            "Epoch: 00 [ 3724/20839 ( 18%)], Train Loss: 0.92986\n",
            "Epoch: 00 [ 3764/20839 ( 18%)], Train Loss: 0.92386\n",
            "Epoch: 00 [ 3804/20839 ( 18%)], Train Loss: 0.91776\n",
            "Epoch: 00 [ 3844/20839 ( 18%)], Train Loss: 0.91419\n",
            "Epoch: 00 [ 3884/20839 ( 19%)], Train Loss: 0.91029\n",
            "Epoch: 00 [ 3924/20839 ( 19%)], Train Loss: 0.90701\n",
            "Epoch: 00 [ 3964/20839 ( 19%)], Train Loss: 0.90426\n",
            "Epoch: 00 [ 4004/20839 ( 19%)], Train Loss: 0.90088\n",
            "Epoch: 00 [ 4044/20839 ( 19%)], Train Loss: 0.89887\n",
            "Epoch: 00 [ 4084/20839 ( 20%)], Train Loss: 0.89773\n",
            "Epoch: 00 [ 4124/20839 ( 20%)], Train Loss: 0.89348\n",
            "Epoch: 00 [ 4164/20839 ( 20%)], Train Loss: 0.88982\n",
            "Epoch: 00 [ 4204/20839 ( 20%)], Train Loss: 0.88513\n",
            "Epoch: 00 [ 4244/20839 ( 20%)], Train Loss: 0.88278\n",
            "Epoch: 00 [ 4284/20839 ( 21%)], Train Loss: 0.88025\n",
            "Epoch: 00 [ 4324/20839 ( 21%)], Train Loss: 0.87560\n",
            "Epoch: 00 [ 4364/20839 ( 21%)], Train Loss: 0.87248\n",
            "Epoch: 00 [ 4404/20839 ( 21%)], Train Loss: 0.86899\n",
            "Epoch: 00 [ 4444/20839 ( 21%)], Train Loss: 0.86574\n",
            "Epoch: 00 [ 4484/20839 ( 22%)], Train Loss: 0.86355\n",
            "Epoch: 00 [ 4524/20839 ( 22%)], Train Loss: 0.85868\n",
            "Epoch: 00 [ 4564/20839 ( 22%)], Train Loss: 0.85601\n",
            "Epoch: 00 [ 4604/20839 ( 22%)], Train Loss: 0.85206\n",
            "Epoch: 00 [ 4644/20839 ( 22%)], Train Loss: 0.85151\n",
            "Epoch: 00 [ 4684/20839 ( 22%)], Train Loss: 0.84883\n",
            "Epoch: 00 [ 4724/20839 ( 23%)], Train Loss: 0.84783\n",
            "Epoch: 00 [ 4764/20839 ( 23%)], Train Loss: 0.84616\n",
            "Epoch: 00 [ 4804/20839 ( 23%)], Train Loss: 0.84384\n",
            "Epoch: 00 [ 4844/20839 ( 23%)], Train Loss: 0.84203\n",
            "Epoch: 00 [ 4884/20839 ( 23%)], Train Loss: 0.83834\n",
            "Epoch: 00 [ 4924/20839 ( 24%)], Train Loss: 0.83509\n",
            "Epoch: 00 [ 4964/20839 ( 24%)], Train Loss: 0.83236\n",
            "Epoch: 00 [ 5004/20839 ( 24%)], Train Loss: 0.82890\n",
            "Epoch: 00 [ 5044/20839 ( 24%)], Train Loss: 0.82788\n",
            "Epoch: 00 [ 5084/20839 ( 24%)], Train Loss: 0.82539\n",
            "Epoch: 00 [ 5124/20839 ( 25%)], Train Loss: 0.82492\n",
            "Epoch: 00 [ 5164/20839 ( 25%)], Train Loss: 0.82486\n",
            "Epoch: 00 [ 5204/20839 ( 25%)], Train Loss: 0.82363\n",
            "Epoch: 00 [ 5244/20839 ( 25%)], Train Loss: 0.82181\n",
            "Epoch: 00 [ 5284/20839 ( 25%)], Train Loss: 0.82059\n",
            "Epoch: 00 [ 5324/20839 ( 26%)], Train Loss: 0.81721\n",
            "Epoch: 00 [ 5364/20839 ( 26%)], Train Loss: 0.81586\n",
            "Epoch: 00 [ 5404/20839 ( 26%)], Train Loss: 0.81338\n",
            "Epoch: 00 [ 5444/20839 ( 26%)], Train Loss: 0.81162\n",
            "Epoch: 00 [ 5484/20839 ( 26%)], Train Loss: 0.80934\n",
            "Epoch: 00 [ 5524/20839 ( 27%)], Train Loss: 0.80885\n",
            "Epoch: 00 [ 5564/20839 ( 27%)], Train Loss: 0.80638\n",
            "Epoch: 00 [ 5604/20839 ( 27%)], Train Loss: 0.80407\n",
            "Epoch: 00 [ 5644/20839 ( 27%)], Train Loss: 0.80168\n",
            "Epoch: 00 [ 5684/20839 ( 27%)], Train Loss: 0.79926\n",
            "Epoch: 00 [ 5724/20839 ( 27%)], Train Loss: 0.79692\n",
            "Epoch: 00 [ 5764/20839 ( 28%)], Train Loss: 0.79365\n",
            "Epoch: 00 [ 5804/20839 ( 28%)], Train Loss: 0.79199\n",
            "Epoch: 00 [ 5844/20839 ( 28%)], Train Loss: 0.78991\n",
            "Epoch: 00 [ 5884/20839 ( 28%)], Train Loss: 0.78713\n",
            "Epoch: 00 [ 5924/20839 ( 28%)], Train Loss: 0.78536\n",
            "Epoch: 00 [ 5964/20839 ( 29%)], Train Loss: 0.78264\n",
            "Epoch: 00 [ 6004/20839 ( 29%)], Train Loss: 0.78226\n",
            "Epoch: 00 [ 6044/20839 ( 29%)], Train Loss: 0.78020\n",
            "Epoch: 00 [ 6084/20839 ( 29%)], Train Loss: 0.77638\n",
            "Epoch: 00 [ 6124/20839 ( 29%)], Train Loss: 0.77510\n",
            "Epoch: 00 [ 6164/20839 ( 30%)], Train Loss: 0.77247\n",
            "Epoch: 00 [ 6204/20839 ( 30%)], Train Loss: 0.77145\n",
            "Epoch: 00 [ 6244/20839 ( 30%)], Train Loss: 0.76817\n",
            "Epoch: 00 [ 6284/20839 ( 30%)], Train Loss: 0.76664\n",
            "Epoch: 00 [ 6324/20839 ( 30%)], Train Loss: 0.76545\n",
            "Epoch: 00 [ 6364/20839 ( 31%)], Train Loss: 0.76433\n",
            "Epoch: 00 [ 6404/20839 ( 31%)], Train Loss: 0.76385\n",
            "Epoch: 00 [ 6444/20839 ( 31%)], Train Loss: 0.76336\n",
            "Epoch: 00 [ 6484/20839 ( 31%)], Train Loss: 0.76181\n",
            "Epoch: 00 [ 6524/20839 ( 31%)], Train Loss: 0.76138\n",
            "Epoch: 00 [ 6564/20839 ( 31%)], Train Loss: 0.76112\n",
            "Epoch: 00 [ 6604/20839 ( 32%)], Train Loss: 0.75845\n",
            "Epoch: 00 [ 6644/20839 ( 32%)], Train Loss: 0.75754\n",
            "Epoch: 00 [ 6684/20839 ( 32%)], Train Loss: 0.75599\n",
            "Epoch: 00 [ 6724/20839 ( 32%)], Train Loss: 0.75599\n",
            "Epoch: 00 [ 6764/20839 ( 32%)], Train Loss: 0.75405\n",
            "Epoch: 00 [ 6804/20839 ( 33%)], Train Loss: 0.75116\n",
            "Epoch: 00 [ 6844/20839 ( 33%)], Train Loss: 0.75091\n",
            "Epoch: 00 [ 6884/20839 ( 33%)], Train Loss: 0.74966\n",
            "Epoch: 00 [ 6924/20839 ( 33%)], Train Loss: 0.74779\n",
            "Epoch: 00 [ 6964/20839 ( 33%)], Train Loss: 0.74646\n",
            "Epoch: 00 [ 7004/20839 ( 34%)], Train Loss: 0.74332\n",
            "Epoch: 00 [ 7044/20839 ( 34%)], Train Loss: 0.74235\n",
            "Epoch: 00 [ 7084/20839 ( 34%)], Train Loss: 0.74160\n",
            "Epoch: 00 [ 7124/20839 ( 34%)], Train Loss: 0.73912\n",
            "Epoch: 00 [ 7164/20839 ( 34%)], Train Loss: 0.73850\n",
            "Epoch: 00 [ 7204/20839 ( 35%)], Train Loss: 0.73769\n",
            "Epoch: 00 [ 7244/20839 ( 35%)], Train Loss: 0.73536\n",
            "Epoch: 00 [ 7284/20839 ( 35%)], Train Loss: 0.73495\n",
            "Epoch: 00 [ 7324/20839 ( 35%)], Train Loss: 0.73583\n",
            "Epoch: 00 [ 7364/20839 ( 35%)], Train Loss: 0.73499\n",
            "Epoch: 00 [ 7404/20839 ( 36%)], Train Loss: 0.73265\n",
            "Epoch: 00 [ 7444/20839 ( 36%)], Train Loss: 0.73144\n",
            "Epoch: 00 [ 7484/20839 ( 36%)], Train Loss: 0.73029\n",
            "Epoch: 00 [ 7524/20839 ( 36%)], Train Loss: 0.72876\n",
            "Epoch: 00 [ 7564/20839 ( 36%)], Train Loss: 0.72676\n",
            "Epoch: 00 [ 7604/20839 ( 36%)], Train Loss: 0.72510\n",
            "Epoch: 00 [ 7644/20839 ( 37%)], Train Loss: 0.72339\n",
            "Epoch: 00 [ 7684/20839 ( 37%)], Train Loss: 0.72220\n",
            "Epoch: 00 [ 7724/20839 ( 37%)], Train Loss: 0.72069\n",
            "Epoch: 00 [ 7764/20839 ( 37%)], Train Loss: 0.71857\n",
            "Epoch: 00 [ 7804/20839 ( 37%)], Train Loss: 0.71702\n",
            "Epoch: 00 [ 7844/20839 ( 38%)], Train Loss: 0.71596\n",
            "Epoch: 00 [ 7884/20839 ( 38%)], Train Loss: 0.71414\n",
            "Epoch: 00 [ 7924/20839 ( 38%)], Train Loss: 0.71314\n",
            "Epoch: 00 [ 7964/20839 ( 38%)], Train Loss: 0.71233\n",
            "Epoch: 00 [ 8004/20839 ( 38%)], Train Loss: 0.71123\n",
            "Epoch: 00 [ 8044/20839 ( 39%)], Train Loss: 0.71080\n",
            "Epoch: 00 [ 8084/20839 ( 39%)], Train Loss: 0.71095\n",
            "Epoch: 00 [ 8124/20839 ( 39%)], Train Loss: 0.70991\n",
            "Epoch: 00 [ 8164/20839 ( 39%)], Train Loss: 0.70896\n",
            "Epoch: 00 [ 8204/20839 ( 39%)], Train Loss: 0.70673\n",
            "Epoch: 00 [ 8244/20839 ( 40%)], Train Loss: 0.70604\n",
            "Epoch: 00 [ 8284/20839 ( 40%)], Train Loss: 0.70453\n",
            "Epoch: 00 [ 8324/20839 ( 40%)], Train Loss: 0.70426\n",
            "Epoch: 00 [ 8364/20839 ( 40%)], Train Loss: 0.70334\n",
            "Epoch: 00 [ 8404/20839 ( 40%)], Train Loss: 0.70173\n",
            "Epoch: 00 [ 8444/20839 ( 41%)], Train Loss: 0.70157\n",
            "Epoch: 00 [ 8484/20839 ( 41%)], Train Loss: 0.70029\n",
            "Epoch: 00 [ 8524/20839 ( 41%)], Train Loss: 0.69845\n",
            "Epoch: 00 [ 8564/20839 ( 41%)], Train Loss: 0.69755\n",
            "Epoch: 00 [ 8604/20839 ( 41%)], Train Loss: 0.69781\n",
            "Epoch: 00 [ 8644/20839 ( 41%)], Train Loss: 0.69589\n",
            "Epoch: 00 [ 8684/20839 ( 42%)], Train Loss: 0.69493\n",
            "Epoch: 00 [ 8724/20839 ( 42%)], Train Loss: 0.69330\n",
            "Epoch: 00 [ 8764/20839 ( 42%)], Train Loss: 0.69376\n",
            "Epoch: 00 [ 8804/20839 ( 42%)], Train Loss: 0.69240\n",
            "Epoch: 00 [ 8844/20839 ( 42%)], Train Loss: 0.69236\n",
            "Epoch: 00 [ 8884/20839 ( 43%)], Train Loss: 0.69228\n",
            "Epoch: 00 [ 8924/20839 ( 43%)], Train Loss: 0.69088\n",
            "Epoch: 00 [ 8964/20839 ( 43%)], Train Loss: 0.68972\n",
            "Epoch: 00 [ 9004/20839 ( 43%)], Train Loss: 0.68989\n",
            "Epoch: 00 [ 9044/20839 ( 43%)], Train Loss: 0.68980\n",
            "Epoch: 00 [ 9084/20839 ( 44%)], Train Loss: 0.68916\n",
            "Epoch: 00 [ 9124/20839 ( 44%)], Train Loss: 0.68812\n",
            "Epoch: 00 [ 9164/20839 ( 44%)], Train Loss: 0.68676\n",
            "Epoch: 00 [ 9204/20839 ( 44%)], Train Loss: 0.68557\n",
            "Epoch: 00 [ 9244/20839 ( 44%)], Train Loss: 0.68501\n",
            "Epoch: 00 [ 9284/20839 ( 45%)], Train Loss: 0.68400\n",
            "Epoch: 00 [ 9324/20839 ( 45%)], Train Loss: 0.68347\n",
            "Epoch: 00 [ 9364/20839 ( 45%)], Train Loss: 0.68165\n",
            "Epoch: 00 [ 9404/20839 ( 45%)], Train Loss: 0.68021\n",
            "Epoch: 00 [ 9444/20839 ( 45%)], Train Loss: 0.67936\n",
            "Epoch: 00 [ 9484/20839 ( 46%)], Train Loss: 0.67833\n",
            "Epoch: 00 [ 9524/20839 ( 46%)], Train Loss: 0.67620\n",
            "Epoch: 00 [ 9564/20839 ( 46%)], Train Loss: 0.67476\n",
            "Epoch: 00 [ 9604/20839 ( 46%)], Train Loss: 0.67346\n",
            "Epoch: 00 [ 9644/20839 ( 46%)], Train Loss: 0.67336\n",
            "Epoch: 00 [ 9684/20839 ( 46%)], Train Loss: 0.67241\n",
            "Epoch: 00 [ 9724/20839 ( 47%)], Train Loss: 0.67155\n",
            "Epoch: 00 [ 9764/20839 ( 47%)], Train Loss: 0.67125\n",
            "Epoch: 00 [ 9804/20839 ( 47%)], Train Loss: 0.66986\n",
            "Epoch: 00 [ 9844/20839 ( 47%)], Train Loss: 0.66965\n",
            "Epoch: 00 [ 9884/20839 ( 47%)], Train Loss: 0.66934\n",
            "Epoch: 00 [ 9924/20839 ( 48%)], Train Loss: 0.66886\n",
            "Epoch: 00 [ 9964/20839 ( 48%)], Train Loss: 0.66815\n",
            "Epoch: 00 [10004/20839 ( 48%)], Train Loss: 0.66724\n",
            "Epoch: 00 [10044/20839 ( 48%)], Train Loss: 0.66661\n",
            "Epoch: 00 [10084/20839 ( 48%)], Train Loss: 0.66536\n",
            "Epoch: 00 [10124/20839 ( 49%)], Train Loss: 0.66509\n",
            "Epoch: 00 [10164/20839 ( 49%)], Train Loss: 0.66438\n",
            "Epoch: 00 [10204/20839 ( 49%)], Train Loss: 0.66315\n",
            "Epoch: 00 [10244/20839 ( 49%)], Train Loss: 0.66273\n",
            "Epoch: 00 [10284/20839 ( 49%)], Train Loss: 0.66230\n",
            "Epoch: 00 [10324/20839 ( 50%)], Train Loss: 0.66104\n",
            "Epoch: 00 [10364/20839 ( 50%)], Train Loss: 0.66036\n",
            "Epoch: 00 [10404/20839 ( 50%)], Train Loss: 0.65994\n",
            "Epoch: 00 [10444/20839 ( 50%)], Train Loss: 0.65979\n",
            "Epoch: 00 [10484/20839 ( 50%)], Train Loss: 0.65896\n",
            "Epoch: 00 [10524/20839 ( 51%)], Train Loss: 0.65806\n",
            "Epoch: 00 [10564/20839 ( 51%)], Train Loss: 0.65785\n",
            "Epoch: 00 [10604/20839 ( 51%)], Train Loss: 0.65692\n",
            "Epoch: 00 [10644/20839 ( 51%)], Train Loss: 0.65732\n",
            "Epoch: 00 [10684/20839 ( 51%)], Train Loss: 0.65654\n",
            "Epoch: 00 [10724/20839 ( 51%)], Train Loss: 0.65574\n",
            "Epoch: 00 [10764/20839 ( 52%)], Train Loss: 0.65545\n",
            "Epoch: 00 [10804/20839 ( 52%)], Train Loss: 0.65454\n",
            "Epoch: 00 [10844/20839 ( 52%)], Train Loss: 0.65336\n",
            "Epoch: 00 [10884/20839 ( 52%)], Train Loss: 0.65304\n",
            "Epoch: 00 [10924/20839 ( 52%)], Train Loss: 0.65253\n",
            "Epoch: 00 [10964/20839 ( 53%)], Train Loss: 0.65166\n",
            "Epoch: 00 [11004/20839 ( 53%)], Train Loss: 0.65152\n",
            "Epoch: 00 [11044/20839 ( 53%)], Train Loss: 0.65152\n",
            "Epoch: 00 [11084/20839 ( 53%)], Train Loss: 0.65007\n",
            "Epoch: 00 [11124/20839 ( 53%)], Train Loss: 0.64981\n",
            "Epoch: 00 [11164/20839 ( 54%)], Train Loss: 0.64922\n",
            "Epoch: 00 [11204/20839 ( 54%)], Train Loss: 0.64822\n",
            "Epoch: 00 [11244/20839 ( 54%)], Train Loss: 0.64863\n",
            "Epoch: 00 [11284/20839 ( 54%)], Train Loss: 0.64759\n",
            "Epoch: 00 [11324/20839 ( 54%)], Train Loss: 0.64696\n",
            "Epoch: 00 [11364/20839 ( 55%)], Train Loss: 0.64603\n",
            "Epoch: 00 [11404/20839 ( 55%)], Train Loss: 0.64723\n",
            "Epoch: 00 [11444/20839 ( 55%)], Train Loss: 0.64744\n",
            "Epoch: 00 [11484/20839 ( 55%)], Train Loss: 0.64702\n",
            "Epoch: 00 [11524/20839 ( 55%)], Train Loss: 0.64636\n",
            "Epoch: 00 [11564/20839 ( 55%)], Train Loss: 0.64606\n",
            "Epoch: 00 [11604/20839 ( 56%)], Train Loss: 0.64488\n",
            "Epoch: 00 [11644/20839 ( 56%)], Train Loss: 0.64406\n",
            "Epoch: 00 [11684/20839 ( 56%)], Train Loss: 0.64350\n",
            "Epoch: 00 [11724/20839 ( 56%)], Train Loss: 0.64280\n",
            "Epoch: 00 [11764/20839 ( 56%)], Train Loss: 0.64190\n",
            "Epoch: 00 [11804/20839 ( 57%)], Train Loss: 0.64067\n",
            "Epoch: 00 [11844/20839 ( 57%)], Train Loss: 0.63969\n",
            "Epoch: 00 [11884/20839 ( 57%)], Train Loss: 0.63914\n",
            "Epoch: 00 [11924/20839 ( 57%)], Train Loss: 0.63814\n",
            "Epoch: 00 [11964/20839 ( 57%)], Train Loss: 0.63715\n",
            "Epoch: 00 [12004/20839 ( 58%)], Train Loss: 0.63747\n",
            "Epoch: 00 [12044/20839 ( 58%)], Train Loss: 0.63725\n",
            "Epoch: 00 [12084/20839 ( 58%)], Train Loss: 0.63687\n",
            "Epoch: 00 [12124/20839 ( 58%)], Train Loss: 0.63624\n",
            "Epoch: 00 [12164/20839 ( 58%)], Train Loss: 0.63572\n",
            "Epoch: 00 [12204/20839 ( 59%)], Train Loss: 0.63582\n",
            "Epoch: 00 [12244/20839 ( 59%)], Train Loss: 0.63583\n",
            "Epoch: 00 [12284/20839 ( 59%)], Train Loss: 0.63568\n",
            "Epoch: 00 [12324/20839 ( 59%)], Train Loss: 0.63525\n",
            "Epoch: 00 [12364/20839 ( 59%)], Train Loss: 0.63503\n",
            "Epoch: 00 [12404/20839 ( 60%)], Train Loss: 0.63518\n",
            "Epoch: 00 [12444/20839 ( 60%)], Train Loss: 0.63519\n",
            "Epoch: 00 [12484/20839 ( 60%)], Train Loss: 0.63460\n",
            "Epoch: 00 [12524/20839 ( 60%)], Train Loss: 0.63495\n",
            "Epoch: 00 [12564/20839 ( 60%)], Train Loss: 0.63553\n",
            "Epoch: 00 [12604/20839 ( 60%)], Train Loss: 0.63568\n",
            "Epoch: 00 [12644/20839 ( 61%)], Train Loss: 0.63455\n",
            "Epoch: 00 [12684/20839 ( 61%)], Train Loss: 0.63393\n",
            "Epoch: 00 [12724/20839 ( 61%)], Train Loss: 0.63359\n",
            "Epoch: 00 [12764/20839 ( 61%)], Train Loss: 0.63278\n",
            "Epoch: 00 [12804/20839 ( 61%)], Train Loss: 0.63190\n",
            "Epoch: 00 [12844/20839 ( 62%)], Train Loss: 0.63087\n",
            "Epoch: 00 [12884/20839 ( 62%)], Train Loss: 0.63034\n",
            "Epoch: 00 [12924/20839 ( 62%)], Train Loss: 0.62999\n",
            "Epoch: 00 [12964/20839 ( 62%)], Train Loss: 0.62902\n",
            "Epoch: 00 [13004/20839 ( 62%)], Train Loss: 0.62805\n",
            "Epoch: 00 [13044/20839 ( 63%)], Train Loss: 0.62761\n",
            "Epoch: 00 [13084/20839 ( 63%)], Train Loss: 0.62653\n",
            "Epoch: 00 [13124/20839 ( 63%)], Train Loss: 0.62521\n",
            "Epoch: 00 [13164/20839 ( 63%)], Train Loss: 0.62534\n",
            "Epoch: 00 [13204/20839 ( 63%)], Train Loss: 0.62470\n",
            "Epoch: 00 [13244/20839 ( 64%)], Train Loss: 0.62473\n",
            "Epoch: 00 [13284/20839 ( 64%)], Train Loss: 0.62385\n",
            "Epoch: 00 [13324/20839 ( 64%)], Train Loss: 0.62319\n",
            "Epoch: 00 [13364/20839 ( 64%)], Train Loss: 0.62266\n",
            "Epoch: 00 [13404/20839 ( 64%)], Train Loss: 0.62214\n",
            "Epoch: 00 [13444/20839 ( 65%)], Train Loss: 0.62148\n",
            "Epoch: 00 [13484/20839 ( 65%)], Train Loss: 0.62052\n",
            "Epoch: 00 [13524/20839 ( 65%)], Train Loss: 0.61945\n",
            "Epoch: 00 [13564/20839 ( 65%)], Train Loss: 0.61894\n",
            "Epoch: 00 [13604/20839 ( 65%)], Train Loss: 0.61892\n",
            "Epoch: 00 [13644/20839 ( 65%)], Train Loss: 0.61804\n",
            "Epoch: 00 [13684/20839 ( 66%)], Train Loss: 0.61772\n",
            "Epoch: 00 [13724/20839 ( 66%)], Train Loss: 0.61731\n",
            "Epoch: 00 [13764/20839 ( 66%)], Train Loss: 0.61768\n",
            "Epoch: 00 [13804/20839 ( 66%)], Train Loss: 0.61763\n",
            "Epoch: 00 [13844/20839 ( 66%)], Train Loss: 0.61708\n",
            "Epoch: 00 [13884/20839 ( 67%)], Train Loss: 0.61715\n",
            "Epoch: 00 [13924/20839 ( 67%)], Train Loss: 0.61662\n",
            "Epoch: 00 [13964/20839 ( 67%)], Train Loss: 0.61576\n",
            "Epoch: 00 [14004/20839 ( 67%)], Train Loss: 0.61557\n",
            "Epoch: 00 [14044/20839 ( 67%)], Train Loss: 0.61551\n",
            "Epoch: 00 [14084/20839 ( 68%)], Train Loss: 0.61481\n",
            "Epoch: 00 [14124/20839 ( 68%)], Train Loss: 0.61402\n",
            "Epoch: 00 [14164/20839 ( 68%)], Train Loss: 0.61366\n",
            "Epoch: 00 [14204/20839 ( 68%)], Train Loss: 0.61361\n",
            "Epoch: 00 [14244/20839 ( 68%)], Train Loss: 0.61298\n",
            "Epoch: 00 [14284/20839 ( 69%)], Train Loss: 0.61256\n",
            "Epoch: 00 [14324/20839 ( 69%)], Train Loss: 0.61182\n",
            "Epoch: 00 [14364/20839 ( 69%)], Train Loss: 0.61187\n",
            "Epoch: 00 [14404/20839 ( 69%)], Train Loss: 0.61167\n",
            "Epoch: 00 [14444/20839 ( 69%)], Train Loss: 0.61220\n",
            "Epoch: 00 [14484/20839 ( 70%)], Train Loss: 0.61187\n",
            "Epoch: 00 [14524/20839 ( 70%)], Train Loss: 0.61165\n",
            "Epoch: 00 [14564/20839 ( 70%)], Train Loss: 0.61181\n",
            "Epoch: 00 [14604/20839 ( 70%)], Train Loss: 0.61122\n",
            "Epoch: 00 [14644/20839 ( 70%)], Train Loss: 0.61104\n",
            "Epoch: 00 [14684/20839 ( 70%)], Train Loss: 0.61046\n",
            "Epoch: 00 [14724/20839 ( 71%)], Train Loss: 0.60981\n",
            "Epoch: 00 [14764/20839 ( 71%)], Train Loss: 0.60917\n",
            "Epoch: 00 [14804/20839 ( 71%)], Train Loss: 0.60907\n",
            "Epoch: 00 [14844/20839 ( 71%)], Train Loss: 0.60880\n",
            "Epoch: 00 [14884/20839 ( 71%)], Train Loss: 0.60913\n",
            "Epoch: 00 [14924/20839 ( 72%)], Train Loss: 0.60889\n",
            "Epoch: 00 [14964/20839 ( 72%)], Train Loss: 0.60875\n",
            "Epoch: 00 [15004/20839 ( 72%)], Train Loss: 0.60862\n",
            "Epoch: 00 [15044/20839 ( 72%)], Train Loss: 0.60846\n",
            "Epoch: 00 [15084/20839 ( 72%)], Train Loss: 0.60798\n",
            "Epoch: 00 [15124/20839 ( 73%)], Train Loss: 0.60743\n",
            "Epoch: 00 [15164/20839 ( 73%)], Train Loss: 0.60706\n",
            "Epoch: 00 [15204/20839 ( 73%)], Train Loss: 0.60711\n",
            "Epoch: 00 [15244/20839 ( 73%)], Train Loss: 0.60615\n",
            "Epoch: 00 [15284/20839 ( 73%)], Train Loss: 0.60581\n",
            "Epoch: 00 [15324/20839 ( 74%)], Train Loss: 0.60506\n",
            "Epoch: 00 [15364/20839 ( 74%)], Train Loss: 0.60445\n",
            "Epoch: 00 [15404/20839 ( 74%)], Train Loss: 0.60445\n",
            "Epoch: 00 [15444/20839 ( 74%)], Train Loss: 0.60490\n",
            "Epoch: 00 [15484/20839 ( 74%)], Train Loss: 0.60421\n",
            "Epoch: 00 [15524/20839 ( 74%)], Train Loss: 0.60406\n",
            "Epoch: 00 [15564/20839 ( 75%)], Train Loss: 0.60403\n",
            "Epoch: 00 [15604/20839 ( 75%)], Train Loss: 0.60351\n",
            "Epoch: 00 [15644/20839 ( 75%)], Train Loss: 0.60290\n",
            "Epoch: 00 [15684/20839 ( 75%)], Train Loss: 0.60273\n",
            "Epoch: 00 [15724/20839 ( 75%)], Train Loss: 0.60299\n",
            "Epoch: 00 [15764/20839 ( 76%)], Train Loss: 0.60229\n",
            "Epoch: 00 [15804/20839 ( 76%)], Train Loss: 0.60231\n",
            "Epoch: 00 [15844/20839 ( 76%)], Train Loss: 0.60215\n",
            "Epoch: 00 [15884/20839 ( 76%)], Train Loss: 0.60164\n",
            "Epoch: 00 [15924/20839 ( 76%)], Train Loss: 0.60084\n",
            "Epoch: 00 [15964/20839 ( 77%)], Train Loss: 0.60072\n",
            "Epoch: 00 [16004/20839 ( 77%)], Train Loss: 0.60092\n",
            "Epoch: 00 [16044/20839 ( 77%)], Train Loss: 0.60054\n",
            "Epoch: 00 [16084/20839 ( 77%)], Train Loss: 0.60028\n",
            "Epoch: 00 [16124/20839 ( 77%)], Train Loss: 0.59986\n",
            "Epoch: 00 [16164/20839 ( 78%)], Train Loss: 0.59982\n",
            "Epoch: 00 [16204/20839 ( 78%)], Train Loss: 0.59971\n",
            "Epoch: 00 [16244/20839 ( 78%)], Train Loss: 0.59903\n",
            "Epoch: 00 [16284/20839 ( 78%)], Train Loss: 0.59847\n",
            "Epoch: 00 [16324/20839 ( 78%)], Train Loss: 0.59831\n",
            "Epoch: 00 [16364/20839 ( 79%)], Train Loss: 0.59771\n",
            "Epoch: 00 [16404/20839 ( 79%)], Train Loss: 0.59707\n",
            "Epoch: 00 [16444/20839 ( 79%)], Train Loss: 0.59649\n",
            "Epoch: 00 [16484/20839 ( 79%)], Train Loss: 0.59637\n",
            "Epoch: 00 [16524/20839 ( 79%)], Train Loss: 0.59559\n",
            "Epoch: 00 [16564/20839 ( 79%)], Train Loss: 0.59471\n",
            "Epoch: 00 [16604/20839 ( 80%)], Train Loss: 0.59436\n",
            "Epoch: 00 [16644/20839 ( 80%)], Train Loss: 0.59343\n",
            "Epoch: 00 [16684/20839 ( 80%)], Train Loss: 0.59326\n",
            "Epoch: 00 [16724/20839 ( 80%)], Train Loss: 0.59267\n",
            "Epoch: 00 [16764/20839 ( 80%)], Train Loss: 0.59235\n",
            "Epoch: 00 [16804/20839 ( 81%)], Train Loss: 0.59179\n",
            "Epoch: 00 [16844/20839 ( 81%)], Train Loss: 0.59162\n",
            "Epoch: 00 [16884/20839 ( 81%)], Train Loss: 0.59094\n",
            "Epoch: 00 [16924/20839 ( 81%)], Train Loss: 0.59147\n",
            "Epoch: 00 [16964/20839 ( 81%)], Train Loss: 0.59129\n",
            "Epoch: 00 [17004/20839 ( 82%)], Train Loss: 0.59178\n",
            "Epoch: 00 [17044/20839 ( 82%)], Train Loss: 0.59169\n",
            "Epoch: 00 [17084/20839 ( 82%)], Train Loss: 0.59194\n",
            "Epoch: 00 [17124/20839 ( 82%)], Train Loss: 0.59181\n",
            "Epoch: 00 [17164/20839 ( 82%)], Train Loss: 0.59198\n",
            "Epoch: 00 [17204/20839 ( 83%)], Train Loss: 0.59168\n",
            "Epoch: 00 [17244/20839 ( 83%)], Train Loss: 0.59162\n",
            "Epoch: 00 [17284/20839 ( 83%)], Train Loss: 0.59134\n",
            "Epoch: 00 [17324/20839 ( 83%)], Train Loss: 0.59124\n",
            "Epoch: 00 [17364/20839 ( 83%)], Train Loss: 0.59101\n",
            "Epoch: 00 [17404/20839 ( 84%)], Train Loss: 0.59041\n",
            "Epoch: 00 [17444/20839 ( 84%)], Train Loss: 0.58969\n",
            "Epoch: 00 [17484/20839 ( 84%)], Train Loss: 0.58937\n",
            "Epoch: 00 [17524/20839 ( 84%)], Train Loss: 0.58877\n",
            "Epoch: 00 [17564/20839 ( 84%)], Train Loss: 0.58836\n",
            "Epoch: 00 [17604/20839 ( 84%)], Train Loss: 0.58832\n",
            "Epoch: 00 [17644/20839 ( 85%)], Train Loss: 0.58867\n",
            "Epoch: 00 [17684/20839 ( 85%)], Train Loss: 0.58847\n",
            "Epoch: 00 [17724/20839 ( 85%)], Train Loss: 0.58782\n",
            "Epoch: 00 [17764/20839 ( 85%)], Train Loss: 0.58787\n",
            "Epoch: 00 [17804/20839 ( 85%)], Train Loss: 0.58727\n",
            "Epoch: 00 [17844/20839 ( 86%)], Train Loss: 0.58628\n",
            "Epoch: 00 [17884/20839 ( 86%)], Train Loss: 0.58547\n",
            "Epoch: 00 [17924/20839 ( 86%)], Train Loss: 0.58491\n",
            "Epoch: 00 [17964/20839 ( 86%)], Train Loss: 0.58429\n",
            "Epoch: 00 [18004/20839 ( 86%)], Train Loss: 0.58433\n",
            "Epoch: 00 [18044/20839 ( 87%)], Train Loss: 0.58381\n",
            "Epoch: 00 [18084/20839 ( 87%)], Train Loss: 0.58321\n",
            "Epoch: 00 [18124/20839 ( 87%)], Train Loss: 0.58257\n",
            "Epoch: 00 [18164/20839 ( 87%)], Train Loss: 0.58205\n",
            "Epoch: 00 [18204/20839 ( 87%)], Train Loss: 0.58178\n",
            "Epoch: 00 [18244/20839 ( 88%)], Train Loss: 0.58136\n",
            "Epoch: 00 [18284/20839 ( 88%)], Train Loss: 0.58118\n",
            "Epoch: 00 [18324/20839 ( 88%)], Train Loss: 0.58061\n",
            "Epoch: 00 [18364/20839 ( 88%)], Train Loss: 0.58003\n",
            "Epoch: 00 [18404/20839 ( 88%)], Train Loss: 0.58009\n",
            "Epoch: 00 [18444/20839 ( 89%)], Train Loss: 0.57984\n",
            "Epoch: 00 [18484/20839 ( 89%)], Train Loss: 0.57941\n",
            "Epoch: 00 [18524/20839 ( 89%)], Train Loss: 0.57864\n",
            "Epoch: 00 [18564/20839 ( 89%)], Train Loss: 0.57814\n",
            "Epoch: 00 [18604/20839 ( 89%)], Train Loss: 0.57796\n",
            "Epoch: 00 [18644/20839 ( 89%)], Train Loss: 0.57746\n",
            "Epoch: 00 [18684/20839 ( 90%)], Train Loss: 0.57691\n",
            "Epoch: 00 [18724/20839 ( 90%)], Train Loss: 0.57660\n",
            "Epoch: 00 [18764/20839 ( 90%)], Train Loss: 0.57649\n",
            "Epoch: 00 [18804/20839 ( 90%)], Train Loss: 0.57603\n",
            "Epoch: 00 [18844/20839 ( 90%)], Train Loss: 0.57587\n",
            "Epoch: 00 [18884/20839 ( 91%)], Train Loss: 0.57576\n",
            "Epoch: 00 [18924/20839 ( 91%)], Train Loss: 0.57537\n",
            "Epoch: 00 [18964/20839 ( 91%)], Train Loss: 0.57492\n",
            "Epoch: 00 [19004/20839 ( 91%)], Train Loss: 0.57489\n",
            "Epoch: 00 [19044/20839 ( 91%)], Train Loss: 0.57468\n",
            "Epoch: 00 [19084/20839 ( 92%)], Train Loss: 0.57416\n",
            "Epoch: 00 [19124/20839 ( 92%)], Train Loss: 0.57403\n",
            "Epoch: 00 [19164/20839 ( 92%)], Train Loss: 0.57370\n",
            "Epoch: 00 [19204/20839 ( 92%)], Train Loss: 0.57395\n",
            "Epoch: 00 [19244/20839 ( 92%)], Train Loss: 0.57380\n",
            "Epoch: 00 [19284/20839 ( 93%)], Train Loss: 0.57345\n",
            "Epoch: 00 [19324/20839 ( 93%)], Train Loss: 0.57304\n",
            "Epoch: 00 [19364/20839 ( 93%)], Train Loss: 0.57274\n",
            "Epoch: 00 [19404/20839 ( 93%)], Train Loss: 0.57273\n",
            "Epoch: 00 [19444/20839 ( 93%)], Train Loss: 0.57295\n",
            "Epoch: 00 [19484/20839 ( 93%)], Train Loss: 0.57325\n",
            "Epoch: 00 [19524/20839 ( 94%)], Train Loss: 0.57316\n",
            "Epoch: 00 [19564/20839 ( 94%)], Train Loss: 0.57304\n",
            "Epoch: 00 [19604/20839 ( 94%)], Train Loss: 0.57222\n",
            "Epoch: 00 [19644/20839 ( 94%)], Train Loss: 0.57161\n",
            "Epoch: 00 [19684/20839 ( 94%)], Train Loss: 0.57158\n",
            "Epoch: 00 [19724/20839 ( 95%)], Train Loss: 0.57123\n",
            "Epoch: 00 [19764/20839 ( 95%)], Train Loss: 0.57095\n",
            "Epoch: 00 [19804/20839 ( 95%)], Train Loss: 0.57089\n",
            "Epoch: 00 [19844/20839 ( 95%)], Train Loss: 0.57073\n",
            "Epoch: 00 [19884/20839 ( 95%)], Train Loss: 0.57037\n",
            "Epoch: 00 [19924/20839 ( 96%)], Train Loss: 0.57025\n",
            "Epoch: 00 [19964/20839 ( 96%)], Train Loss: 0.57022\n",
            "Epoch: 00 [20004/20839 ( 96%)], Train Loss: 0.57040\n",
            "Epoch: 00 [20044/20839 ( 96%)], Train Loss: 0.57001\n",
            "Epoch: 00 [20084/20839 ( 96%)], Train Loss: 0.56976\n",
            "Epoch: 00 [20124/20839 ( 97%)], Train Loss: 0.56951\n",
            "Epoch: 00 [20164/20839 ( 97%)], Train Loss: 0.56912\n",
            "Epoch: 00 [20204/20839 ( 97%)], Train Loss: 0.56855\n",
            "Epoch: 00 [20244/20839 ( 97%)], Train Loss: 0.56839\n",
            "Epoch: 00 [20284/20839 ( 97%)], Train Loss: 0.56803\n",
            "Epoch: 00 [20324/20839 ( 98%)], Train Loss: 0.56773\n",
            "Epoch: 00 [20364/20839 ( 98%)], Train Loss: 0.56706\n",
            "Epoch: 00 [20404/20839 ( 98%)], Train Loss: 0.56715\n",
            "Epoch: 00 [20444/20839 ( 98%)], Train Loss: 0.56704\n",
            "Epoch: 00 [20484/20839 ( 98%)], Train Loss: 0.56657\n",
            "Epoch: 00 [20524/20839 ( 98%)], Train Loss: 0.56667\n",
            "Epoch: 00 [20564/20839 ( 99%)], Train Loss: 0.56622\n",
            "Epoch: 00 [20604/20839 ( 99%)], Train Loss: 0.56636\n",
            "Epoch: 00 [20644/20839 ( 99%)], Train Loss: 0.56615\n",
            "Epoch: 00 [20684/20839 ( 99%)], Train Loss: 0.56588\n",
            "Epoch: 00 [20724/20839 ( 99%)], Train Loss: 0.56605\n",
            "Epoch: 00 [20764/20839 (100%)], Train Loss: 0.56591\n",
            "Epoch: 00 [20804/20839 (100%)], Train Loss: 0.56580\n",
            "Epoch: 00 [20839/20839 (100%)], Train Loss: 0.56558\n",
            "----Validation Results Summary----\n",
            "Epoch: [0] Valid Loss: 0.20651\n",
            "0 Epoch, Best epoch was updated! Valid Loss: 0.20651\n",
            "Saving model checkpoint to output/checkpoint-fold-4.\n",
            "\n",
            "Epoch: 01 [    4/20839 (  0%)], Train Loss: 0.66287\n",
            "Epoch: 01 [   44/20839 (  0%)], Train Loss: 0.84659\n",
            "Epoch: 01 [   84/20839 (  0%)], Train Loss: 0.68580\n",
            "Epoch: 01 [  124/20839 (  1%)], Train Loss: 0.61614\n",
            "Epoch: 01 [  164/20839 (  1%)], Train Loss: 0.56957\n",
            "Epoch: 01 [  204/20839 (  1%)], Train Loss: 0.53394\n",
            "Epoch: 01 [  244/20839 (  1%)], Train Loss: 0.55138\n",
            "Epoch: 01 [  284/20839 (  1%)], Train Loss: 0.52114\n",
            "Epoch: 01 [  324/20839 (  2%)], Train Loss: 0.49407\n",
            "Epoch: 01 [  364/20839 (  2%)], Train Loss: 0.47702\n",
            "Epoch: 01 [  404/20839 (  2%)], Train Loss: 0.47990\n",
            "Epoch: 01 [  444/20839 (  2%)], Train Loss: 0.48388\n",
            "Epoch: 01 [  484/20839 (  2%)], Train Loss: 0.47438\n",
            "Epoch: 01 [  524/20839 (  3%)], Train Loss: 0.46744\n",
            "Epoch: 01 [  564/20839 (  3%)], Train Loss: 0.47771\n",
            "Epoch: 01 [  604/20839 (  3%)], Train Loss: 0.46984\n",
            "Epoch: 01 [  644/20839 (  3%)], Train Loss: 0.45266\n",
            "Epoch: 01 [  684/20839 (  3%)], Train Loss: 0.45401\n",
            "Epoch: 01 [  724/20839 (  3%)], Train Loss: 0.45302\n",
            "Epoch: 01 [  764/20839 (  4%)], Train Loss: 0.44941\n",
            "Epoch: 01 [  804/20839 (  4%)], Train Loss: 0.45894\n",
            "Epoch: 01 [  844/20839 (  4%)], Train Loss: 0.45157\n",
            "Epoch: 01 [  884/20839 (  4%)], Train Loss: 0.43989\n",
            "Epoch: 01 [  924/20839 (  4%)], Train Loss: 0.43946\n",
            "Epoch: 01 [  964/20839 (  5%)], Train Loss: 0.44358\n",
            "Epoch: 01 [ 1004/20839 (  5%)], Train Loss: 0.44230\n",
            "Epoch: 01 [ 1044/20839 (  5%)], Train Loss: 0.43448\n",
            "Epoch: 01 [ 1084/20839 (  5%)], Train Loss: 0.42989\n",
            "Epoch: 01 [ 1124/20839 (  5%)], Train Loss: 0.42765\n",
            "Epoch: 01 [ 1164/20839 (  6%)], Train Loss: 0.42637\n",
            "Epoch: 01 [ 1204/20839 (  6%)], Train Loss: 0.42347\n",
            "Epoch: 01 [ 1244/20839 (  6%)], Train Loss: 0.41291\n",
            "Epoch: 01 [ 1284/20839 (  6%)], Train Loss: 0.41490\n",
            "Epoch: 01 [ 1324/20839 (  6%)], Train Loss: 0.41169\n",
            "Epoch: 01 [ 1364/20839 (  7%)], Train Loss: 0.41197\n",
            "Epoch: 01 [ 1404/20839 (  7%)], Train Loss: 0.41092\n",
            "Epoch: 01 [ 1444/20839 (  7%)], Train Loss: 0.40906\n",
            "Epoch: 01 [ 1484/20839 (  7%)], Train Loss: 0.40542\n",
            "Epoch: 01 [ 1524/20839 (  7%)], Train Loss: 0.40223\n",
            "Epoch: 01 [ 1564/20839 (  8%)], Train Loss: 0.40399\n",
            "Epoch: 01 [ 1604/20839 (  8%)], Train Loss: 0.41087\n",
            "Epoch: 01 [ 1644/20839 (  8%)], Train Loss: 0.41024\n",
            "Epoch: 01 [ 1684/20839 (  8%)], Train Loss: 0.40766\n",
            "Epoch: 01 [ 1724/20839 (  8%)], Train Loss: 0.40765\n",
            "Epoch: 01 [ 1764/20839 (  8%)], Train Loss: 0.40205\n",
            "Epoch: 01 [ 1804/20839 (  9%)], Train Loss: 0.39768\n",
            "Epoch: 01 [ 1844/20839 (  9%)], Train Loss: 0.39414\n",
            "Epoch: 01 [ 1884/20839 (  9%)], Train Loss: 0.39491\n",
            "Epoch: 01 [ 1924/20839 (  9%)], Train Loss: 0.39115\n",
            "Epoch: 01 [ 1964/20839 (  9%)], Train Loss: 0.38881\n",
            "Epoch: 01 [ 2004/20839 ( 10%)], Train Loss: 0.38663\n",
            "Epoch: 01 [ 2044/20839 ( 10%)], Train Loss: 0.38592\n",
            "Epoch: 01 [ 2084/20839 ( 10%)], Train Loss: 0.38628\n",
            "Epoch: 01 [ 2124/20839 ( 10%)], Train Loss: 0.38269\n",
            "Epoch: 01 [ 2164/20839 ( 10%)], Train Loss: 0.37996\n",
            "Epoch: 01 [ 2204/20839 ( 11%)], Train Loss: 0.37847\n",
            "Epoch: 01 [ 2244/20839 ( 11%)], Train Loss: 0.37740\n",
            "Epoch: 01 [ 2284/20839 ( 11%)], Train Loss: 0.38117\n",
            "Epoch: 01 [ 2324/20839 ( 11%)], Train Loss: 0.37667\n",
            "Epoch: 01 [ 2364/20839 ( 11%)], Train Loss: 0.37382\n",
            "Epoch: 01 [ 2404/20839 ( 12%)], Train Loss: 0.36989\n",
            "Epoch: 01 [ 2444/20839 ( 12%)], Train Loss: 0.36778\n",
            "Epoch: 01 [ 2484/20839 ( 12%)], Train Loss: 0.36707\n",
            "Epoch: 01 [ 2524/20839 ( 12%)], Train Loss: 0.36513\n",
            "Epoch: 01 [ 2564/20839 ( 12%)], Train Loss: 0.36758\n",
            "Epoch: 01 [ 2604/20839 ( 12%)], Train Loss: 0.36740\n",
            "Epoch: 01 [ 2644/20839 ( 13%)], Train Loss: 0.36554\n",
            "Epoch: 01 [ 2684/20839 ( 13%)], Train Loss: 0.36454\n",
            "Epoch: 01 [ 2724/20839 ( 13%)], Train Loss: 0.36384\n",
            "Epoch: 01 [ 2764/20839 ( 13%)], Train Loss: 0.36137\n",
            "Epoch: 01 [ 2804/20839 ( 13%)], Train Loss: 0.36012\n",
            "Epoch: 01 [ 2844/20839 ( 14%)], Train Loss: 0.35796\n",
            "Epoch: 01 [ 2884/20839 ( 14%)], Train Loss: 0.35785\n",
            "Epoch: 01 [ 2924/20839 ( 14%)], Train Loss: 0.35691\n",
            "Epoch: 01 [ 2964/20839 ( 14%)], Train Loss: 0.35710\n",
            "Epoch: 01 [ 3004/20839 ( 14%)], Train Loss: 0.35426\n",
            "Epoch: 01 [ 3044/20839 ( 15%)], Train Loss: 0.35252\n",
            "Epoch: 01 [ 3084/20839 ( 15%)], Train Loss: 0.34970\n",
            "Epoch: 01 [ 3124/20839 ( 15%)], Train Loss: 0.35043\n",
            "Epoch: 01 [ 3164/20839 ( 15%)], Train Loss: 0.34977\n",
            "Epoch: 01 [ 3204/20839 ( 15%)], Train Loss: 0.34837\n",
            "Epoch: 01 [ 3244/20839 ( 16%)], Train Loss: 0.34747\n",
            "Epoch: 01 [ 3284/20839 ( 16%)], Train Loss: 0.34689\n",
            "Epoch: 01 [ 3324/20839 ( 16%)], Train Loss: 0.34461\n",
            "Epoch: 01 [ 3364/20839 ( 16%)], Train Loss: 0.34209\n",
            "Epoch: 01 [ 3404/20839 ( 16%)], Train Loss: 0.33893\n",
            "Epoch: 01 [ 3444/20839 ( 17%)], Train Loss: 0.33689\n",
            "Epoch: 01 [ 3484/20839 ( 17%)], Train Loss: 0.33583\n",
            "Epoch: 01 [ 3524/20839 ( 17%)], Train Loss: 0.33326\n",
            "Epoch: 01 [ 3564/20839 ( 17%)], Train Loss: 0.33154\n",
            "Epoch: 01 [ 3604/20839 ( 17%)], Train Loss: 0.32965\n",
            "Epoch: 01 [ 3644/20839 ( 17%)], Train Loss: 0.32827\n",
            "Epoch: 01 [ 3684/20839 ( 18%)], Train Loss: 0.32672\n",
            "Epoch: 01 [ 3724/20839 ( 18%)], Train Loss: 0.32474\n",
            "Epoch: 01 [ 3764/20839 ( 18%)], Train Loss: 0.32207\n",
            "Epoch: 01 [ 3804/20839 ( 18%)], Train Loss: 0.31909\n",
            "Epoch: 01 [ 3844/20839 ( 18%)], Train Loss: 0.31799\n",
            "Epoch: 01 [ 3884/20839 ( 19%)], Train Loss: 0.31648\n",
            "Epoch: 01 [ 3924/20839 ( 19%)], Train Loss: 0.31601\n",
            "Epoch: 01 [ 3964/20839 ( 19%)], Train Loss: 0.31498\n",
            "Epoch: 01 [ 4004/20839 ( 19%)], Train Loss: 0.31377\n",
            "Epoch: 01 [ 4044/20839 ( 19%)], Train Loss: 0.31443\n",
            "Epoch: 01 [ 4084/20839 ( 20%)], Train Loss: 0.31459\n",
            "Epoch: 01 [ 4124/20839 ( 20%)], Train Loss: 0.31307\n",
            "Epoch: 01 [ 4164/20839 ( 20%)], Train Loss: 0.31198\n",
            "Epoch: 01 [ 4204/20839 ( 20%)], Train Loss: 0.30993\n",
            "Epoch: 01 [ 4244/20839 ( 20%)], Train Loss: 0.30934\n",
            "Epoch: 01 [ 4284/20839 ( 21%)], Train Loss: 0.30896\n",
            "Epoch: 01 [ 4324/20839 ( 21%)], Train Loss: 0.30683\n",
            "Epoch: 01 [ 4364/20839 ( 21%)], Train Loss: 0.30594\n",
            "Epoch: 01 [ 4404/20839 ( 21%)], Train Loss: 0.30416\n",
            "Epoch: 01 [ 4444/20839 ( 21%)], Train Loss: 0.30297\n",
            "Epoch: 01 [ 4484/20839 ( 22%)], Train Loss: 0.30254\n",
            "Epoch: 01 [ 4524/20839 ( 22%)], Train Loss: 0.30028\n",
            "Epoch: 01 [ 4564/20839 ( 22%)], Train Loss: 0.29910\n",
            "Epoch: 01 [ 4604/20839 ( 22%)], Train Loss: 0.29752\n",
            "Epoch: 01 [ 4644/20839 ( 22%)], Train Loss: 0.29782\n",
            "Epoch: 01 [ 4684/20839 ( 22%)], Train Loss: 0.29658\n",
            "Epoch: 01 [ 4724/20839 ( 23%)], Train Loss: 0.29631\n",
            "Epoch: 01 [ 4764/20839 ( 23%)], Train Loss: 0.29567\n",
            "Epoch: 01 [ 4804/20839 ( 23%)], Train Loss: 0.29519\n",
            "Epoch: 01 [ 4844/20839 ( 23%)], Train Loss: 0.29397\n",
            "Epoch: 01 [ 4884/20839 ( 23%)], Train Loss: 0.29221\n",
            "Epoch: 01 [ 4924/20839 ( 24%)], Train Loss: 0.29090\n",
            "Epoch: 01 [ 4964/20839 ( 24%)], Train Loss: 0.28953\n",
            "Epoch: 01 [ 5004/20839 ( 24%)], Train Loss: 0.28819\n",
            "Epoch: 01 [ 5044/20839 ( 24%)], Train Loss: 0.28818\n",
            "Epoch: 01 [ 5084/20839 ( 24%)], Train Loss: 0.28711\n",
            "Epoch: 01 [ 5124/20839 ( 25%)], Train Loss: 0.28651\n",
            "Epoch: 01 [ 5164/20839 ( 25%)], Train Loss: 0.28729\n",
            "Epoch: 01 [ 5204/20839 ( 25%)], Train Loss: 0.28650\n",
            "Epoch: 01 [ 5244/20839 ( 25%)], Train Loss: 0.28582\n",
            "Epoch: 01 [ 5284/20839 ( 25%)], Train Loss: 0.28500\n",
            "Epoch: 01 [ 5324/20839 ( 26%)], Train Loss: 0.28395\n",
            "Epoch: 01 [ 5364/20839 ( 26%)], Train Loss: 0.28363\n",
            "Epoch: 01 [ 5404/20839 ( 26%)], Train Loss: 0.28261\n",
            "Epoch: 01 [ 5444/20839 ( 26%)], Train Loss: 0.28258\n",
            "Epoch: 01 [ 5484/20839 ( 26%)], Train Loss: 0.28155\n",
            "Epoch: 01 [ 5524/20839 ( 27%)], Train Loss: 0.28165\n",
            "Epoch: 01 [ 5564/20839 ( 27%)], Train Loss: 0.28053\n",
            "Epoch: 01 [ 5604/20839 ( 27%)], Train Loss: 0.27953\n",
            "Epoch: 01 [ 5644/20839 ( 27%)], Train Loss: 0.27874\n",
            "Epoch: 01 [ 5684/20839 ( 27%)], Train Loss: 0.27778\n",
            "Epoch: 01 [ 5724/20839 ( 27%)], Train Loss: 0.27698\n",
            "Epoch: 01 [ 5764/20839 ( 28%)], Train Loss: 0.27570\n",
            "Epoch: 01 [ 5804/20839 ( 28%)], Train Loss: 0.27465\n",
            "Epoch: 01 [ 5844/20839 ( 28%)], Train Loss: 0.27395\n",
            "Epoch: 01 [ 5884/20839 ( 28%)], Train Loss: 0.27297\n",
            "Epoch: 01 [ 5924/20839 ( 28%)], Train Loss: 0.27223\n",
            "Epoch: 01 [ 5964/20839 ( 29%)], Train Loss: 0.27106\n",
            "Epoch: 01 [ 6004/20839 ( 29%)], Train Loss: 0.27100\n",
            "Epoch: 01 [ 6044/20839 ( 29%)], Train Loss: 0.26985\n",
            "Epoch: 01 [ 6084/20839 ( 29%)], Train Loss: 0.26845\n",
            "Epoch: 01 [ 6124/20839 ( 29%)], Train Loss: 0.26772\n",
            "Epoch: 01 [ 6164/20839 ( 30%)], Train Loss: 0.26720\n",
            "Epoch: 01 [ 6204/20839 ( 30%)], Train Loss: 0.26675\n",
            "Epoch: 01 [ 6244/20839 ( 30%)], Train Loss: 0.26538\n",
            "Epoch: 01 [ 6284/20839 ( 30%)], Train Loss: 0.26484\n",
            "Epoch: 01 [ 6324/20839 ( 30%)], Train Loss: 0.26447\n",
            "Epoch: 01 [ 6364/20839 ( 31%)], Train Loss: 0.26412\n",
            "Epoch: 01 [ 6404/20839 ( 31%)], Train Loss: 0.26332\n",
            "Epoch: 01 [ 6444/20839 ( 31%)], Train Loss: 0.26280\n",
            "Epoch: 01 [ 6484/20839 ( 31%)], Train Loss: 0.26179\n",
            "Epoch: 01 [ 6524/20839 ( 31%)], Train Loss: 0.26160\n",
            "Epoch: 01 [ 6564/20839 ( 31%)], Train Loss: 0.26204\n",
            "Epoch: 01 [ 6604/20839 ( 32%)], Train Loss: 0.26094\n",
            "Epoch: 01 [ 6644/20839 ( 32%)], Train Loss: 0.26045\n",
            "Epoch: 01 [ 6684/20839 ( 32%)], Train Loss: 0.26025\n",
            "Epoch: 01 [ 6724/20839 ( 32%)], Train Loss: 0.26018\n",
            "Epoch: 01 [ 6764/20839 ( 32%)], Train Loss: 0.25970\n",
            "Epoch: 01 [ 6804/20839 ( 33%)], Train Loss: 0.25847\n",
            "Epoch: 01 [ 6844/20839 ( 33%)], Train Loss: 0.25798\n",
            "Epoch: 01 [ 6884/20839 ( 33%)], Train Loss: 0.25735\n",
            "Epoch: 01 [ 6924/20839 ( 33%)], Train Loss: 0.25642\n",
            "Epoch: 01 [ 6964/20839 ( 33%)], Train Loss: 0.25613\n",
            "Epoch: 01 [ 7004/20839 ( 34%)], Train Loss: 0.25499\n",
            "Epoch: 01 [ 7044/20839 ( 34%)], Train Loss: 0.25446\n",
            "Epoch: 01 [ 7084/20839 ( 34%)], Train Loss: 0.25366\n",
            "Epoch: 01 [ 7124/20839 ( 34%)], Train Loss: 0.25250\n",
            "Epoch: 01 [ 7164/20839 ( 34%)], Train Loss: 0.25192\n",
            "Epoch: 01 [ 7204/20839 ( 35%)], Train Loss: 0.25100\n",
            "Epoch: 01 [ 7244/20839 ( 35%)], Train Loss: 0.25015\n",
            "Epoch: 01 [ 7284/20839 ( 35%)], Train Loss: 0.25047\n",
            "Epoch: 01 [ 7324/20839 ( 35%)], Train Loss: 0.25125\n",
            "Epoch: 01 [ 7364/20839 ( 35%)], Train Loss: 0.25078\n",
            "Epoch: 01 [ 7404/20839 ( 36%)], Train Loss: 0.24982\n",
            "Epoch: 01 [ 7444/20839 ( 36%)], Train Loss: 0.24966\n",
            "Epoch: 01 [ 7484/20839 ( 36%)], Train Loss: 0.24932\n",
            "Epoch: 01 [ 7524/20839 ( 36%)], Train Loss: 0.24885\n",
            "Epoch: 01 [ 7564/20839 ( 36%)], Train Loss: 0.24790\n",
            "Epoch: 01 [ 7604/20839 ( 36%)], Train Loss: 0.24728\n",
            "Epoch: 01 [ 7644/20839 ( 37%)], Train Loss: 0.24653\n",
            "Epoch: 01 [ 7684/20839 ( 37%)], Train Loss: 0.24598\n",
            "Epoch: 01 [ 7724/20839 ( 37%)], Train Loss: 0.24544\n",
            "Epoch: 01 [ 7764/20839 ( 37%)], Train Loss: 0.24487\n",
            "Epoch: 01 [ 7804/20839 ( 37%)], Train Loss: 0.24434\n",
            "Epoch: 01 [ 7844/20839 ( 38%)], Train Loss: 0.24366\n",
            "Epoch: 01 [ 7884/20839 ( 38%)], Train Loss: 0.24297\n",
            "Epoch: 01 [ 7924/20839 ( 38%)], Train Loss: 0.24248\n",
            "Epoch: 01 [ 7964/20839 ( 38%)], Train Loss: 0.24208\n",
            "Epoch: 01 [ 8004/20839 ( 38%)], Train Loss: 0.24137\n",
            "Epoch: 01 [ 8044/20839 ( 39%)], Train Loss: 0.24122\n",
            "Epoch: 01 [ 8084/20839 ( 39%)], Train Loss: 0.24119\n",
            "Epoch: 01 [ 8124/20839 ( 39%)], Train Loss: 0.24115\n",
            "Epoch: 01 [ 8164/20839 ( 39%)], Train Loss: 0.24068\n",
            "Epoch: 01 [ 8204/20839 ( 39%)], Train Loss: 0.23986\n",
            "Epoch: 01 [ 8244/20839 ( 40%)], Train Loss: 0.23939\n",
            "Epoch: 01 [ 8284/20839 ( 40%)], Train Loss: 0.23873\n",
            "Epoch: 01 [ 8324/20839 ( 40%)], Train Loss: 0.23862\n",
            "Epoch: 01 [ 8364/20839 ( 40%)], Train Loss: 0.23795\n",
            "Epoch: 01 [ 8404/20839 ( 40%)], Train Loss: 0.23732\n",
            "Epoch: 01 [ 8444/20839 ( 41%)], Train Loss: 0.23760\n",
            "Epoch: 01 [ 8484/20839 ( 41%)], Train Loss: 0.23720\n",
            "Epoch: 01 [ 8524/20839 ( 41%)], Train Loss: 0.23636\n",
            "Epoch: 01 [ 8564/20839 ( 41%)], Train Loss: 0.23615\n",
            "Epoch: 01 [ 8604/20839 ( 41%)], Train Loss: 0.23625\n",
            "Epoch: 01 [ 8644/20839 ( 41%)], Train Loss: 0.23571\n",
            "Epoch: 01 [ 8684/20839 ( 42%)], Train Loss: 0.23538\n",
            "Epoch: 01 [ 8724/20839 ( 42%)], Train Loss: 0.23489\n",
            "Epoch: 01 [ 8764/20839 ( 42%)], Train Loss: 0.23479\n",
            "Epoch: 01 [ 8804/20839 ( 42%)], Train Loss: 0.23452\n",
            "Epoch: 01 [ 8844/20839 ( 42%)], Train Loss: 0.23425\n",
            "Epoch: 01 [ 8884/20839 ( 43%)], Train Loss: 0.23429\n",
            "Epoch: 01 [ 8924/20839 ( 43%)], Train Loss: 0.23352\n",
            "Epoch: 01 [ 8964/20839 ( 43%)], Train Loss: 0.23305\n",
            "Epoch: 01 [ 9004/20839 ( 43%)], Train Loss: 0.23352\n",
            "Epoch: 01 [ 9044/20839 ( 43%)], Train Loss: 0.23359\n",
            "Epoch: 01 [ 9084/20839 ( 44%)], Train Loss: 0.23349\n",
            "Epoch: 01 [ 9124/20839 ( 44%)], Train Loss: 0.23308\n",
            "Epoch: 01 [ 9164/20839 ( 44%)], Train Loss: 0.23256\n",
            "Epoch: 01 [ 9204/20839 ( 44%)], Train Loss: 0.23214\n",
            "Epoch: 01 [ 9244/20839 ( 44%)], Train Loss: 0.23183\n",
            "Epoch: 01 [ 9284/20839 ( 45%)], Train Loss: 0.23127\n",
            "Epoch: 01 [ 9324/20839 ( 45%)], Train Loss: 0.23058\n",
            "Epoch: 01 [ 9364/20839 ( 45%)], Train Loss: 0.22994\n",
            "Epoch: 01 [ 9404/20839 ( 45%)], Train Loss: 0.22946\n",
            "Epoch: 01 [ 9444/20839 ( 45%)], Train Loss: 0.22916\n",
            "Epoch: 01 [ 9484/20839 ( 46%)], Train Loss: 0.22871\n",
            "Epoch: 01 [ 9524/20839 ( 46%)], Train Loss: 0.22800\n",
            "Epoch: 01 [ 9564/20839 ( 46%)], Train Loss: 0.22746\n",
            "Epoch: 01 [ 9604/20839 ( 46%)], Train Loss: 0.22708\n",
            "Epoch: 01 [ 9644/20839 ( 46%)], Train Loss: 0.22738\n",
            "Epoch: 01 [ 9684/20839 ( 46%)], Train Loss: 0.22745\n",
            "Epoch: 01 [ 9724/20839 ( 47%)], Train Loss: 0.22692\n",
            "Epoch: 01 [ 9764/20839 ( 47%)], Train Loss: 0.22671\n",
            "Epoch: 01 [ 9804/20839 ( 47%)], Train Loss: 0.22625\n",
            "Epoch: 01 [ 9844/20839 ( 47%)], Train Loss: 0.22618\n",
            "Epoch: 01 [ 9884/20839 ( 47%)], Train Loss: 0.22629\n",
            "Epoch: 01 [ 9924/20839 ( 48%)], Train Loss: 0.22616\n",
            "Epoch: 01 [ 9964/20839 ( 48%)], Train Loss: 0.22599\n",
            "Epoch: 01 [10004/20839 ( 48%)], Train Loss: 0.22576\n",
            "Epoch: 01 [10044/20839 ( 48%)], Train Loss: 0.22579\n",
            "Epoch: 01 [10084/20839 ( 48%)], Train Loss: 0.22559\n",
            "Epoch: 01 [10124/20839 ( 49%)], Train Loss: 0.22536\n",
            "Epoch: 01 [10164/20839 ( 49%)], Train Loss: 0.22511\n",
            "Epoch: 01 [10204/20839 ( 49%)], Train Loss: 0.22461\n",
            "Epoch: 01 [10244/20839 ( 49%)], Train Loss: 0.22445\n",
            "Epoch: 01 [10284/20839 ( 49%)], Train Loss: 0.22423\n",
            "Epoch: 01 [10324/20839 ( 50%)], Train Loss: 0.22375\n",
            "Epoch: 01 [10364/20839 ( 50%)], Train Loss: 0.22356\n",
            "Epoch: 01 [10404/20839 ( 50%)], Train Loss: 0.22325\n",
            "Epoch: 01 [10444/20839 ( 50%)], Train Loss: 0.22332\n",
            "Epoch: 01 [10484/20839 ( 50%)], Train Loss: 0.22305\n",
            "Epoch: 01 [10524/20839 ( 51%)], Train Loss: 0.22256\n",
            "Epoch: 01 [10564/20839 ( 51%)], Train Loss: 0.22254\n",
            "Epoch: 01 [10604/20839 ( 51%)], Train Loss: 0.22217\n",
            "Epoch: 01 [10644/20839 ( 51%)], Train Loss: 0.22213\n",
            "Epoch: 01 [10684/20839 ( 51%)], Train Loss: 0.22197\n",
            "Epoch: 01 [10724/20839 ( 51%)], Train Loss: 0.22149\n",
            "Epoch: 01 [10764/20839 ( 52%)], Train Loss: 0.22159\n",
            "Epoch: 01 [10804/20839 ( 52%)], Train Loss: 0.22144\n",
            "Epoch: 01 [10844/20839 ( 52%)], Train Loss: 0.22119\n",
            "Epoch: 01 [10884/20839 ( 52%)], Train Loss: 0.22149\n",
            "Epoch: 01 [10924/20839 ( 52%)], Train Loss: 0.22188\n",
            "Epoch: 01 [10964/20839 ( 53%)], Train Loss: 0.22164\n",
            "Epoch: 01 [11004/20839 ( 53%)], Train Loss: 0.22130\n",
            "Epoch: 01 [11044/20839 ( 53%)], Train Loss: 0.22161\n",
            "Epoch: 01 [11084/20839 ( 53%)], Train Loss: 0.22110\n",
            "Epoch: 01 [11124/20839 ( 53%)], Train Loss: 0.22107\n",
            "Epoch: 01 [11164/20839 ( 54%)], Train Loss: 0.22087\n",
            "Epoch: 01 [11204/20839 ( 54%)], Train Loss: 0.22057\n",
            "Epoch: 01 [11244/20839 ( 54%)], Train Loss: 0.22046\n",
            "Epoch: 01 [11284/20839 ( 54%)], Train Loss: 0.22036\n",
            "Epoch: 01 [11324/20839 ( 54%)], Train Loss: 0.22021\n",
            "Epoch: 01 [11364/20839 ( 55%)], Train Loss: 0.21999\n",
            "Epoch: 01 [11404/20839 ( 55%)], Train Loss: 0.22051\n",
            "Epoch: 01 [11444/20839 ( 55%)], Train Loss: 0.22054\n",
            "Epoch: 01 [11484/20839 ( 55%)], Train Loss: 0.22039\n",
            "Epoch: 01 [11524/20839 ( 55%)], Train Loss: 0.22033\n",
            "Epoch: 01 [11564/20839 ( 55%)], Train Loss: 0.22019\n",
            "Epoch: 01 [11604/20839 ( 56%)], Train Loss: 0.21980\n",
            "Epoch: 01 [11644/20839 ( 56%)], Train Loss: 0.21942\n",
            "Epoch: 01 [11684/20839 ( 56%)], Train Loss: 0.21941\n",
            "Epoch: 01 [11724/20839 ( 56%)], Train Loss: 0.21930\n",
            "Epoch: 01 [11764/20839 ( 56%)], Train Loss: 0.21885\n",
            "Epoch: 01 [11804/20839 ( 57%)], Train Loss: 0.21827\n",
            "Epoch: 01 [11844/20839 ( 57%)], Train Loss: 0.21810\n",
            "Epoch: 01 [11884/20839 ( 57%)], Train Loss: 0.21775\n",
            "Epoch: 01 [11924/20839 ( 57%)], Train Loss: 0.21726\n",
            "Epoch: 01 [11964/20839 ( 57%)], Train Loss: 0.21704\n",
            "Epoch: 01 [12004/20839 ( 58%)], Train Loss: 0.21694\n",
            "Epoch: 01 [12044/20839 ( 58%)], Train Loss: 0.21696\n",
            "Epoch: 01 [12084/20839 ( 58%)], Train Loss: 0.21681\n",
            "Epoch: 01 [12124/20839 ( 58%)], Train Loss: 0.21648\n",
            "Epoch: 01 [12164/20839 ( 58%)], Train Loss: 0.21618\n",
            "Epoch: 01 [12204/20839 ( 59%)], Train Loss: 0.21621\n",
            "Epoch: 01 [12244/20839 ( 59%)], Train Loss: 0.21597\n",
            "Epoch: 01 [12284/20839 ( 59%)], Train Loss: 0.21593\n",
            "Epoch: 01 [12324/20839 ( 59%)], Train Loss: 0.21564\n",
            "Epoch: 01 [12364/20839 ( 59%)], Train Loss: 0.21555\n",
            "Epoch: 01 [12404/20839 ( 60%)], Train Loss: 0.21563\n",
            "Epoch: 01 [12444/20839 ( 60%)], Train Loss: 0.21585\n",
            "Epoch: 01 [12484/20839 ( 60%)], Train Loss: 0.21553\n",
            "Epoch: 01 [12524/20839 ( 60%)], Train Loss: 0.21554\n",
            "Epoch: 01 [12564/20839 ( 60%)], Train Loss: 0.21588\n",
            "Epoch: 01 [12604/20839 ( 60%)], Train Loss: 0.21608\n",
            "Epoch: 01 [12644/20839 ( 61%)], Train Loss: 0.21554\n",
            "Epoch: 01 [12684/20839 ( 61%)], Train Loss: 0.21552\n",
            "Epoch: 01 [12724/20839 ( 61%)], Train Loss: 0.21520\n",
            "Epoch: 01 [12764/20839 ( 61%)], Train Loss: 0.21488\n",
            "Epoch: 01 [12804/20839 ( 61%)], Train Loss: 0.21464\n",
            "Epoch: 01 [12844/20839 ( 62%)], Train Loss: 0.21415\n",
            "Epoch: 01 [12884/20839 ( 62%)], Train Loss: 0.21382\n",
            "Epoch: 01 [12924/20839 ( 62%)], Train Loss: 0.21360\n",
            "Epoch: 01 [12964/20839 ( 62%)], Train Loss: 0.21314\n",
            "Epoch: 01 [13004/20839 ( 62%)], Train Loss: 0.21288\n",
            "Epoch: 01 [13044/20839 ( 63%)], Train Loss: 0.21271\n",
            "Epoch: 01 [13084/20839 ( 63%)], Train Loss: 0.21228\n",
            "Epoch: 01 [13124/20839 ( 63%)], Train Loss: 0.21183\n",
            "Epoch: 01 [13164/20839 ( 63%)], Train Loss: 0.21188\n",
            "Epoch: 01 [13204/20839 ( 63%)], Train Loss: 0.21158\n",
            "Epoch: 01 [13244/20839 ( 64%)], Train Loss: 0.21197\n",
            "Epoch: 01 [13284/20839 ( 64%)], Train Loss: 0.21196\n",
            "Epoch: 01 [13324/20839 ( 64%)], Train Loss: 0.21153\n",
            "Epoch: 01 [13364/20839 ( 64%)], Train Loss: 0.21143\n",
            "Epoch: 01 [13404/20839 ( 64%)], Train Loss: 0.21113\n",
            "Epoch: 01 [13444/20839 ( 65%)], Train Loss: 0.21091\n",
            "Epoch: 01 [13484/20839 ( 65%)], Train Loss: 0.21047\n",
            "Epoch: 01 [13524/20839 ( 65%)], Train Loss: 0.21001\n",
            "Epoch: 01 [13564/20839 ( 65%)], Train Loss: 0.20987\n",
            "Epoch: 01 [13604/20839 ( 65%)], Train Loss: 0.20964\n",
            "Epoch: 01 [13644/20839 ( 65%)], Train Loss: 0.20931\n",
            "Epoch: 01 [13684/20839 ( 66%)], Train Loss: 0.20933\n",
            "Epoch: 01 [13724/20839 ( 66%)], Train Loss: 0.20924\n",
            "Epoch: 01 [13764/20839 ( 66%)], Train Loss: 0.20903\n",
            "Epoch: 01 [13804/20839 ( 66%)], Train Loss: 0.20933\n",
            "Epoch: 01 [13844/20839 ( 66%)], Train Loss: 0.20913\n",
            "Epoch: 01 [13884/20839 ( 67%)], Train Loss: 0.20956\n",
            "Epoch: 01 [13924/20839 ( 67%)], Train Loss: 0.20936\n",
            "Epoch: 01 [13964/20839 ( 67%)], Train Loss: 0.20918\n",
            "Epoch: 01 [14004/20839 ( 67%)], Train Loss: 0.20910\n",
            "Epoch: 01 [14044/20839 ( 67%)], Train Loss: 0.20922\n",
            "Epoch: 01 [14084/20839 ( 68%)], Train Loss: 0.20910\n",
            "Epoch: 01 [14124/20839 ( 68%)], Train Loss: 0.20886\n",
            "Epoch: 01 [14164/20839 ( 68%)], Train Loss: 0.20886\n",
            "Epoch: 01 [14204/20839 ( 68%)], Train Loss: 0.20873\n",
            "Epoch: 01 [14244/20839 ( 68%)], Train Loss: 0.20833\n",
            "Epoch: 01 [14284/20839 ( 69%)], Train Loss: 0.20823\n",
            "Epoch: 01 [14324/20839 ( 69%)], Train Loss: 0.20807\n",
            "Epoch: 01 [14364/20839 ( 69%)], Train Loss: 0.20829\n",
            "Epoch: 01 [14404/20839 ( 69%)], Train Loss: 0.20804\n",
            "Epoch: 01 [14444/20839 ( 69%)], Train Loss: 0.20852\n",
            "Epoch: 01 [14484/20839 ( 70%)], Train Loss: 0.20828\n",
            "Epoch: 01 [14524/20839 ( 70%)], Train Loss: 0.20815\n",
            "Epoch: 01 [14564/20839 ( 70%)], Train Loss: 0.20860\n",
            "Epoch: 01 [14604/20839 ( 70%)], Train Loss: 0.20836\n",
            "Epoch: 01 [14644/20839 ( 70%)], Train Loss: 0.20826\n",
            "Epoch: 01 [14684/20839 ( 70%)], Train Loss: 0.20809\n",
            "Epoch: 01 [14724/20839 ( 71%)], Train Loss: 0.20785\n",
            "Epoch: 01 [14764/20839 ( 71%)], Train Loss: 0.20765\n",
            "Epoch: 01 [14804/20839 ( 71%)], Train Loss: 0.20751\n",
            "Epoch: 01 [14844/20839 ( 71%)], Train Loss: 0.20729\n",
            "Epoch: 01 [14884/20839 ( 71%)], Train Loss: 0.20776\n",
            "Epoch: 01 [14924/20839 ( 72%)], Train Loss: 0.20767\n",
            "Epoch: 01 [14964/20839 ( 72%)], Train Loss: 0.20765\n",
            "Epoch: 01 [15004/20839 ( 72%)], Train Loss: 0.20770\n",
            "Epoch: 01 [15044/20839 ( 72%)], Train Loss: 0.20772\n",
            "Epoch: 01 [15084/20839 ( 72%)], Train Loss: 0.20762\n",
            "Epoch: 01 [15124/20839 ( 73%)], Train Loss: 0.20742\n",
            "Epoch: 01 [15164/20839 ( 73%)], Train Loss: 0.20730\n",
            "Epoch: 01 [15204/20839 ( 73%)], Train Loss: 0.20712\n",
            "Epoch: 01 [15244/20839 ( 73%)], Train Loss: 0.20677\n",
            "Epoch: 01 [15284/20839 ( 73%)], Train Loss: 0.20669\n",
            "Epoch: 01 [15324/20839 ( 74%)], Train Loss: 0.20649\n",
            "Epoch: 01 [15364/20839 ( 74%)], Train Loss: 0.20633\n",
            "Epoch: 01 [15404/20839 ( 74%)], Train Loss: 0.20634\n",
            "Epoch: 01 [15444/20839 ( 74%)], Train Loss: 0.20642\n",
            "Epoch: 01 [15484/20839 ( 74%)], Train Loss: 0.20623\n",
            "Epoch: 01 [15524/20839 ( 74%)], Train Loss: 0.20606\n",
            "Epoch: 01 [15564/20839 ( 75%)], Train Loss: 0.20604\n",
            "Epoch: 01 [15604/20839 ( 75%)], Train Loss: 0.20585\n",
            "Epoch: 01 [15644/20839 ( 75%)], Train Loss: 0.20586\n",
            "Epoch: 01 [15684/20839 ( 75%)], Train Loss: 0.20571\n",
            "Epoch: 01 [15724/20839 ( 75%)], Train Loss: 0.20572\n",
            "Epoch: 01 [15764/20839 ( 76%)], Train Loss: 0.20546\n",
            "Epoch: 01 [15804/20839 ( 76%)], Train Loss: 0.20536\n",
            "Epoch: 01 [15844/20839 ( 76%)], Train Loss: 0.20549\n",
            "Epoch: 01 [15884/20839 ( 76%)], Train Loss: 0.20534\n",
            "Epoch: 01 [15924/20839 ( 76%)], Train Loss: 0.20509\n",
            "Epoch: 01 [15964/20839 ( 77%)], Train Loss: 0.20529\n",
            "Epoch: 01 [16004/20839 ( 77%)], Train Loss: 0.20526\n",
            "Epoch: 01 [16044/20839 ( 77%)], Train Loss: 0.20500\n",
            "Epoch: 01 [16084/20839 ( 77%)], Train Loss: 0.20487\n",
            "Epoch: 01 [16124/20839 ( 77%)], Train Loss: 0.20485\n",
            "Epoch: 01 [16164/20839 ( 78%)], Train Loss: 0.20498\n",
            "Epoch: 01 [16204/20839 ( 78%)], Train Loss: 0.20513\n",
            "Epoch: 01 [16244/20839 ( 78%)], Train Loss: 0.20496\n",
            "Epoch: 01 [16284/20839 ( 78%)], Train Loss: 0.20459\n",
            "Epoch: 01 [16324/20839 ( 78%)], Train Loss: 0.20450\n",
            "Epoch: 01 [16364/20839 ( 79%)], Train Loss: 0.20420\n",
            "Epoch: 01 [16404/20839 ( 79%)], Train Loss: 0.20417\n",
            "Epoch: 01 [16444/20839 ( 79%)], Train Loss: 0.20397\n",
            "Epoch: 01 [16484/20839 ( 79%)], Train Loss: 0.20387\n",
            "Epoch: 01 [16524/20839 ( 79%)], Train Loss: 0.20353\n",
            "Epoch: 01 [16564/20839 ( 79%)], Train Loss: 0.20324\n",
            "Epoch: 01 [16604/20839 ( 80%)], Train Loss: 0.20315\n",
            "Epoch: 01 [16644/20839 ( 80%)], Train Loss: 0.20277\n",
            "Epoch: 01 [16684/20839 ( 80%)], Train Loss: 0.20271\n",
            "Epoch: 01 [16724/20839 ( 80%)], Train Loss: 0.20240\n",
            "Epoch: 01 [16764/20839 ( 80%)], Train Loss: 0.20243\n",
            "Epoch: 01 [16804/20839 ( 81%)], Train Loss: 0.20226\n",
            "Epoch: 01 [16844/20839 ( 81%)], Train Loss: 0.20231\n",
            "Epoch: 01 [16884/20839 ( 81%)], Train Loss: 0.20197\n",
            "Epoch: 01 [16924/20839 ( 81%)], Train Loss: 0.20215\n",
            "Epoch: 01 [16964/20839 ( 81%)], Train Loss: 0.20198\n",
            "Epoch: 01 [17004/20839 ( 82%)], Train Loss: 0.20254\n",
            "Epoch: 01 [17044/20839 ( 82%)], Train Loss: 0.20253\n",
            "Epoch: 01 [17084/20839 ( 82%)], Train Loss: 0.20302\n",
            "Epoch: 01 [17124/20839 ( 82%)], Train Loss: 0.20288\n",
            "Epoch: 01 [17164/20839 ( 82%)], Train Loss: 0.20312\n",
            "Epoch: 01 [17204/20839 ( 83%)], Train Loss: 0.20310\n",
            "Epoch: 01 [17244/20839 ( 83%)], Train Loss: 0.20318\n",
            "Epoch: 01 [17284/20839 ( 83%)], Train Loss: 0.20315\n",
            "Epoch: 01 [17324/20839 ( 83%)], Train Loss: 0.20311\n",
            "Epoch: 01 [17364/20839 ( 83%)], Train Loss: 0.20317\n",
            "Epoch: 01 [17404/20839 ( 84%)], Train Loss: 0.20311\n",
            "Epoch: 01 [17444/20839 ( 84%)], Train Loss: 0.20290\n",
            "Epoch: 01 [17484/20839 ( 84%)], Train Loss: 0.20274\n",
            "Epoch: 01 [17524/20839 ( 84%)], Train Loss: 0.20249\n",
            "Epoch: 01 [17564/20839 ( 84%)], Train Loss: 0.20239\n",
            "Epoch: 01 [17604/20839 ( 84%)], Train Loss: 0.20230\n",
            "Epoch: 01 [17644/20839 ( 85%)], Train Loss: 0.20230\n",
            "Epoch: 01 [17684/20839 ( 85%)], Train Loss: 0.20245\n",
            "Epoch: 01 [17724/20839 ( 85%)], Train Loss: 0.20219\n",
            "Epoch: 01 [17764/20839 ( 85%)], Train Loss: 0.20230\n",
            "Epoch: 01 [17804/20839 ( 85%)], Train Loss: 0.20202\n",
            "Epoch: 01 [17844/20839 ( 86%)], Train Loss: 0.20163\n",
            "Epoch: 01 [17884/20839 ( 86%)], Train Loss: 0.20133\n",
            "Epoch: 01 [17924/20839 ( 86%)], Train Loss: 0.20115\n",
            "Epoch: 01 [17964/20839 ( 86%)], Train Loss: 0.20090\n",
            "Epoch: 01 [18004/20839 ( 86%)], Train Loss: 0.20088\n",
            "Epoch: 01 [18044/20839 ( 87%)], Train Loss: 0.20059\n",
            "Epoch: 01 [18084/20839 ( 87%)], Train Loss: 0.20026\n",
            "Epoch: 01 [18124/20839 ( 87%)], Train Loss: 0.20012\n",
            "Epoch: 01 [18164/20839 ( 87%)], Train Loss: 0.19989\n",
            "Epoch: 01 [18204/20839 ( 87%)], Train Loss: 0.19966\n",
            "Epoch: 01 [18244/20839 ( 88%)], Train Loss: 0.19958\n",
            "Epoch: 01 [18284/20839 ( 88%)], Train Loss: 0.19961\n",
            "Epoch: 01 [18324/20839 ( 88%)], Train Loss: 0.19940\n",
            "Epoch: 01 [18364/20839 ( 88%)], Train Loss: 0.19907\n",
            "Epoch: 01 [18404/20839 ( 88%)], Train Loss: 0.19932\n",
            "Epoch: 01 [18444/20839 ( 89%)], Train Loss: 0.19922\n",
            "Epoch: 01 [18484/20839 ( 89%)], Train Loss: 0.19894\n",
            "Epoch: 01 [18524/20839 ( 89%)], Train Loss: 0.19867\n",
            "Epoch: 01 [18564/20839 ( 89%)], Train Loss: 0.19853\n",
            "Epoch: 01 [18604/20839 ( 89%)], Train Loss: 0.19829\n",
            "Epoch: 01 [18644/20839 ( 89%)], Train Loss: 0.19813\n",
            "Epoch: 01 [18684/20839 ( 90%)], Train Loss: 0.19786\n",
            "Epoch: 01 [18724/20839 ( 90%)], Train Loss: 0.19789\n",
            "Epoch: 01 [18764/20839 ( 90%)], Train Loss: 0.19758\n",
            "Epoch: 01 [18804/20839 ( 90%)], Train Loss: 0.19734\n",
            "Epoch: 01 [18844/20839 ( 90%)], Train Loss: 0.19728\n",
            "Epoch: 01 [18884/20839 ( 91%)], Train Loss: 0.19732\n",
            "Epoch: 01 [18924/20839 ( 91%)], Train Loss: 0.19702\n",
            "Epoch: 01 [18964/20839 ( 91%)], Train Loss: 0.19689\n",
            "Epoch: 01 [19004/20839 ( 91%)], Train Loss: 0.19705\n",
            "Epoch: 01 [19044/20839 ( 91%)], Train Loss: 0.19697\n",
            "Epoch: 01 [19084/20839 ( 92%)], Train Loss: 0.19690\n",
            "Epoch: 01 [19124/20839 ( 92%)], Train Loss: 0.19717\n",
            "Epoch: 01 [19164/20839 ( 92%)], Train Loss: 0.19710\n",
            "Epoch: 01 [19204/20839 ( 92%)], Train Loss: 0.19712\n",
            "Epoch: 01 [19244/20839 ( 92%)], Train Loss: 0.19719\n",
            "Epoch: 01 [19284/20839 ( 93%)], Train Loss: 0.19687\n",
            "Epoch: 01 [19324/20839 ( 93%)], Train Loss: 0.19678\n",
            "Epoch: 01 [19364/20839 ( 93%)], Train Loss: 0.19660\n",
            "Epoch: 01 [19404/20839 ( 93%)], Train Loss: 0.19669\n",
            "Epoch: 01 [19444/20839 ( 93%)], Train Loss: 0.19684\n",
            "Epoch: 01 [19484/20839 ( 93%)], Train Loss: 0.19700\n",
            "Epoch: 01 [19524/20839 ( 94%)], Train Loss: 0.19697\n",
            "Epoch: 01 [19564/20839 ( 94%)], Train Loss: 0.19706\n",
            "Epoch: 01 [19604/20839 ( 94%)], Train Loss: 0.19677\n",
            "Epoch: 01 [19644/20839 ( 94%)], Train Loss: 0.19648\n",
            "Epoch: 01 [19684/20839 ( 94%)], Train Loss: 0.19641\n",
            "Epoch: 01 [19724/20839 ( 95%)], Train Loss: 0.19614\n",
            "Epoch: 01 [19764/20839 ( 95%)], Train Loss: 0.19604\n",
            "Epoch: 01 [19804/20839 ( 95%)], Train Loss: 0.19590\n",
            "Epoch: 01 [19844/20839 ( 95%)], Train Loss: 0.19587\n",
            "Epoch: 01 [19884/20839 ( 95%)], Train Loss: 0.19578\n",
            "Epoch: 01 [19924/20839 ( 96%)], Train Loss: 0.19567\n",
            "Epoch: 01 [19964/20839 ( 96%)], Train Loss: 0.19581\n",
            "Epoch: 01 [20004/20839 ( 96%)], Train Loss: 0.19588\n",
            "Epoch: 01 [20044/20839 ( 96%)], Train Loss: 0.19577\n",
            "Epoch: 01 [20084/20839 ( 96%)], Train Loss: 0.19575\n",
            "Epoch: 01 [20124/20839 ( 97%)], Train Loss: 0.19569\n",
            "Epoch: 01 [20164/20839 ( 97%)], Train Loss: 0.19541\n",
            "Epoch: 01 [20204/20839 ( 97%)], Train Loss: 0.19522\n",
            "Epoch: 01 [20244/20839 ( 97%)], Train Loss: 0.19536\n",
            "Epoch: 01 [20284/20839 ( 97%)], Train Loss: 0.19529\n",
            "Epoch: 01 [20324/20839 ( 98%)], Train Loss: 0.19512\n",
            "Epoch: 01 [20364/20839 ( 98%)], Train Loss: 0.19482\n",
            "Epoch: 01 [20404/20839 ( 98%)], Train Loss: 0.19509\n",
            "Epoch: 01 [20444/20839 ( 98%)], Train Loss: 0.19511\n",
            "Epoch: 01 [20484/20839 ( 98%)], Train Loss: 0.19496\n",
            "Epoch: 01 [20524/20839 ( 98%)], Train Loss: 0.19523\n",
            "Epoch: 01 [20564/20839 ( 99%)], Train Loss: 0.19495\n",
            "Epoch: 01 [20604/20839 ( 99%)], Train Loss: 0.19520\n",
            "Epoch: 01 [20644/20839 ( 99%)], Train Loss: 0.19543\n",
            "Epoch: 01 [20684/20839 ( 99%)], Train Loss: 0.19550\n",
            "Epoch: 01 [20724/20839 ( 99%)], Train Loss: 0.19560\n",
            "Epoch: 01 [20764/20839 (100%)], Train Loss: 0.19553\n",
            "Epoch: 01 [20804/20839 (100%)], Train Loss: 0.19566\n",
            "Epoch: 01 [20839/20839 (100%)], Train Loss: 0.19560\n",
            "----Validation Results Summary----\n",
            "Epoch: [1] Valid Loss: 0.22903\n",
            "\n",
            "Total Training Time: 6115.466376781464secs, Average Training Time per Epoch: 3057.733188390732secs.\n",
            "Total Validation Time: 245.4601502418518secs, Average Validation Time per Epoch: 122.7300751209259secs.\n",
            "[0.2720036358349279, 0.32137186476052365, 0.22517818749821006, 0.27430064060954523, 0.2065113527196303]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c600bffd"
      },
      "source": [
        "## ここをチェック"
      ],
      "id": "c600bffd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lVx1gGZrn-q"
      },
      "source": [
        "# [0.2639688412511144, 0.23578763837464295, 0.2082770317994482] tamil & hindi xsquad  lr = 1.5e-5 LB:0.758\n",
        "# [0.2692653296424045, 0.23645380146391437, 0.21420694968205684] hindi xsquad only  lr = 1.5e-5 LB:0.761\n",
        "# [0.3102661463084304, 0.23890210809797524, 0.22127942127638148] tamil hindi xsquad + hindi mlpa lr = 1.5e-5 LB:0.775 \n",
        "# [0.2616716774835217, 0.2830840063208972,  0.22212529014537136] train data only  lr = 1.5e-5\n",
        "\n",
        "# lr = 1.5e-5\n",
        "# [0.2720036358349279, 0.32137186476052365, 0.22517818749821006, 0.27430064060954523, 0.2065113527196303]\n",
        "\n"
      ],
      "id": "2lVx1gGZrn-q",
      "execution_count": null,
      "outputs": []
    }
  ]
}