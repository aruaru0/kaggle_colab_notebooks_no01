{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "chaii-qa-5-fold-xlmroberta-torch-fit-multi-sample-dropout",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3134.482195,
      "end_time": "2021-09-26T00:57:33.514736",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-09-26T00:05:19.032541",
      "version": "2.3.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aruaru0/colab_notebook/blob/main/chaii_qa_5_fold_xlmroberta_torch_fit_multi_sample_dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf20wKLVQUYg"
      },
      "source": [
        "# Add Multi sample dropout"
      ],
      "id": "Gf20wKLVQUYg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shE3S33KxTch"
      },
      "source": [
        "# for colab"
      ],
      "id": "shE3S33KxTch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZJxbaQFyRMb",
        "outputId": "1da41265-604b-4b6e-c9fe-cb3ba416ef9c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "BZJxbaQFyRMb",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov  3 01:40:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnzNG_0jxW_6",
        "outputId": "92690f44-efaf-4796-c1bc-8ca60ebe1cdd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "KnzNG_0jxW_6",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTgjav_xxW7p"
      },
      "source": [
        "! pip install --upgrade --force-reinstall --no-deps  kaggle > /dev/null\n",
        "! mkdir /root/.kaggle\n",
        "! cp \"/content/drive/My Drive/Kaggle/kaggle.json\" /root/.kaggle/\n",
        "! chmod 600 /root/.kaggle/kaggle.json"
      ],
      "id": "CTgjav_xxW7p",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_GIjVFxW4G",
        "outputId": "ab280946-2675-467e-edfb-ded21438b527"
      },
      "source": [
        "!kaggle competitions download -c chaii-hindi-and-tamil-question-answering\n",
        "!kaggle datasets download -d rhtsingh/mlqa-hindi-processed\n",
        "!kaggle datasets download -d nbroad/xlm-roberta-squad2\n",
        "!kaggle datasets download -d msafi04/squad-qa-tamil-dataset\n",
        "!kaggle datasets download -d kishalmandal/cleaned-data-for-chaii\n",
        "!kaggle datasets download -d harshwalia/chaiiextended-and-cleaned-datasetready-to-train\n",
        "!kaggle datasets download -d tkm2261/google-translated-squad20-to-hindi-and-tamil"
      ],
      "id": "MO_GIjVFxW4G",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading chaii-hindi-and-tamil-question-answering.zip to /content\n",
            "\r  0% 0.00/6.81M [00:00<?, ?B/s]\n",
            "100% 6.81M/6.81M [00:00<00:00, 166MB/s]\n",
            "Downloading mlqa-hindi-processed.zip to /content\n",
            "  0% 0.00/2.49M [00:00<?, ?B/s]\n",
            "100% 2.49M/2.49M [00:00<00:00, 172MB/s]\n",
            "Downloading xlm-roberta-squad2.zip to /content\n",
            "100% 2.53G/2.53G [00:16<00:00, 90.6MB/s]\n",
            "100% 2.53G/2.53G [00:16<00:00, 168MB/s] \n",
            "Downloading squad-qa-tamil-dataset.zip to /content\n",
            "  0% 0.00/564k [00:00<?, ?B/s]\n",
            "100% 564k/564k [00:00<00:00, 73.3MB/s]\n",
            "Downloading cleaned-data-for-chaii.zip to /content\n",
            " 75% 5.00M/6.71M [00:00<00:00, 30.9MB/s]\n",
            "100% 6.71M/6.71M [00:00<00:00, 40.1MB/s]\n",
            "Downloading chaiiextended-and-cleaned-datasetready-to-train.zip to /content\n",
            " 54% 5.00M/9.29M [00:00<00:00, 51.8MB/s]\n",
            "100% 9.29M/9.29M [00:00<00:00, 83.7MB/s]\n",
            "Downloading google-translated-squad20-to-hindi-and-tamil.zip to /content\n",
            " 59% 17.0M/29.0M [00:00<00:00, 64.9MB/s]\n",
            "100% 29.0M/29.0M [00:00<00:00, 88.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PObJQR5XxWrD",
        "outputId": "79246555-34f7-44fe-98f9-4fa323f24a4f"
      },
      "source": [
        "!apt install unzip"
      ],
      "id": "PObJQR5XxWrD",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyQVYuBDxxBn"
      },
      "source": [
        "!mkdir -p input output"
      ],
      "id": "wyQVYuBDxxBn",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3uTTHsFxw2D",
        "outputId": "73e12aa9-d7d8-4ba9-9f99-4b7a69680cee"
      },
      "source": [
        "!mkdir input/chaii-hindi-and-tamil-question-answering\n",
        "!unzip /content/chaii-hindi-and-tamil-question-answering.zip -d input/chaii-hindi-and-tamil-question-answering\n",
        "!mkdir input/mlqa-hindi-processed\n",
        "!unzip /content/mlqa-hindi-processed.zip -d input/mlqa-hindi-processed\n",
        "!mkdir input/xlm-roberta-squad2\n",
        "!unzip /content/xlm-roberta-squad2.zip -d input/xlm-roberta-squad2/\n",
        "!mkdir input/squad-qa-tamil-dataset\n",
        "!unzip /content/squad-qa-tamil-dataset.zip -d input/squad-qa-tamil-dataset\n",
        "!mkdir input/cleaned-data-for-chaii\n",
        "!unzip /content/cleaned-data-for-chaii.zip -d input/cleaned-data-for-chaii\n",
        "!mkdir input/chaiiextended-and-cleaned-datasetready-to-train\n",
        "!unzip /content/chaiiextended-and-cleaned-datasetready-to-train.zip -d input/chaiiextended-and-cleaned-datasetready-to-train\n",
        "!mkdir input/google-translated-squad20-to-hindi-and-tamil\n",
        "!unzip /content/google-translated-squad20-to-hindi-and-tamil.zip -d input/google-translated-squad20-to-hindi-and-tamil"
      ],
      "id": "_3uTTHsFxw2D",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/chaii-hindi-and-tamil-question-answering.zip\n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/sample_submission.csv  \n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/test.csv  \n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/train.csv  \n",
            "Archive:  /content/mlqa-hindi-processed.zip\n",
            "  inflating: input/mlqa-hindi-processed/mlqa_hindi.csv  \n",
            "  inflating: input/mlqa-hindi-processed/xquad.csv  \n",
            "Archive:  /content/xlm-roberta-squad2.zip\n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/config.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/pytorch_model.bin  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/sentencepiece.bpe.model  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/special_tokens_map.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/tokenizer.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/tokenizer_config.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/config.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/pytorch_model.bin  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/sentencepiece.bpe.model  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/special_tokens_map.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/tokenizer.json  \n",
            "  inflating: input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2/tokenizer_config.json  \n",
            "Archive:  /content/squad-qa-tamil-dataset.zip\n",
            "  inflating: input/squad-qa-tamil-dataset/squad_tamilQA.csv  \n",
            "Archive:  /content/cleaned-data-for-chaii.zip\n",
            "  inflating: input/cleaned-data-for-chaii/cleaned_train.csv  \n",
            "Archive:  /content/chaiiextended-and-cleaned-datasetready-to-train.zip\n",
            "  inflating: input/chaiiextended-and-cleaned-datasetready-to-train/ChaiiExtended.csv  \n",
            "Archive:  /content/google-translated-squad20-to-hindi-and-tamil.zip\n",
            "  inflating: input/google-translated-squad20-to-hindi-and-tamil/squad_hi.csv  \n",
            "  inflating: input/google-translated-squad20-to-hindi-and-tamil/squad_ta.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOWu9n5ueeIE"
      },
      "source": [
        "# !mkdir input/chaii\n",
        "# !kaggle datasets download -d aruaru0/chaii-no006\n",
        "#!unzip /content/chaii-no006.zip -d input/chaii\n",
        "#!kaggle datasets download -d nguyenduongthanh/chaii-xlmr-5-fold\n",
        "#!unzip /content/chaii-xlmr-5-fold.zip -d input/chaii"
      ],
      "id": "mOWu9n5ueeIE",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCRudbllx9Be"
      },
      "source": [
        "# restart here.."
      ],
      "id": "SCRudbllx9Be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrhwRDxfx6fa",
        "outputId": "0332fd72-2292-443f-983a-3a139827dddf"
      },
      "source": [
        "%cd output"
      ],
      "id": "IrhwRDxfx6fa",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPXOqpvbyBky",
        "outputId": "57ee6246-6077-4f0e-9b55-503d19d5108a"
      },
      "source": [
        "!pip install transformers"
      ],
      "id": "wPXOqpvbyBky",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 34.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.1.0-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.0 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c47c9139"
      },
      "source": [
        "<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n",
        "    \n",
        "<h3><span \"style: color=#444\">Introduction</span></h3>\n",
        "\n",
        "The kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n",
        "\n",
        "This is a three part kernel,\n",
        "\n",
        "- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n",
        "\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n",
        "\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n",
        "\n",
        "<h3><span \"style: color=#444\">Techniques</span></h3>\n",
        "\n",
        "The kernel has implementation for below techniques, click on the links to learn more -\n",
        "\n",
        " - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n",
        " \n",
        " - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n",
        " \n",
        " - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n",
        " \n",
        " - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n",
        " \n",
        " - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n",
        " \n",
        " - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n",
        " \n",
        " - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n",
        " \n",
        " - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n",
        " \n",
        " - etc.\n",
        " \n",
        "<h3><span \"style: color=#444\">References</span></h3>\n",
        "I would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n",
        "\n",
        "- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n",
        "\n",
        "- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n",
        "\n",
        "- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n",
        "\n",
        "- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n",
        "\n",
        "- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read."
      ],
      "id": "c47c9139"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43955668"
      },
      "source": [
        "<h3><span style=\"color=#444\">Note</span></h3>\n",
        "\n",
        "The below points are worth noting,\n",
        "\n",
        " - I haven't used FP16 because due to some reason this fails and model never starts training.\n",
        " - These are the original hyperparamters and setting that I have used for training my models.\n",
        " - I tried few pooling layers but none of them performed better than simple one.\n",
        " - Gradient clipping reduces model performance.\n",
        " - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n",
        " - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>"
      ],
      "id": "43955668"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f09433b"
      },
      "source": [
        "### Install APEX"
      ],
      "id": "8f09433b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4bb901a"
      },
      "source": [
        "# %%writefile setup.sh\n",
        "# export CUDA_HOME=/usr/local/cuda-10.1\n",
        "# git clone https://github.com/NVIDIA/apex\n",
        "# cd apex\n",
        "# pip install -v --disable-pip-version-check --no-cache-dir ./"
      ],
      "id": "f4bb901a",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef5e2a11"
      },
      "source": [
        "# %%capture\n",
        "# !sh setup.sh"
      ],
      "id": "ef5e2a11",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55059f9a"
      },
      "source": [
        "### Import Dependencies"
      ],
      "id": "55059f9a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a92ad27",
        "outputId": "e8fa220b-4cf9-4b91-ff03-138a45d1f6cc"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "gc.enable()\n",
        "import math\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import multiprocessing\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn import model_selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import (\n",
        "    Dataset, DataLoader,\n",
        "    SequentialSampler, RandomSampler\n",
        ")\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "try:\n",
        "    from apex import amp\n",
        "    APEX_INSTALLED = True\n",
        "except ImportError:\n",
        "    APEX_INSTALLED = False\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    get_cosine_schedule_with_warmup,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    get_constant_schedule_with_warmup,\n",
        "    logging,\n",
        "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
        ")\n",
        "logging.set_verbosity_warning()\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "def fix_all_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def optimal_num_of_loader_workers():\n",
        "    num_cpus = multiprocessing.cpu_count()\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
        "    return optimal_value\n",
        "\n",
        "print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
      ],
      "id": "8a92ad27",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apex AMP Installed :: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab75148d"
      },
      "source": [
        "### Training Configuration"
      ],
      "id": "ab75148d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMU6cmG6Y7s0"
      },
      "source": [
        "tamil & hindi xsquad"
      ],
      "id": "FMU6cmG6Y7s0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5934fe6a",
        "outputId": "74206eb4-614f-40dd-bab7-b6a0a4bdcdcf"
      },
      "source": [
        "class Config:\n",
        "    # model\n",
        "    model_type = 'xlm_roberta'\n",
        "    #model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n",
        "    model_name_or_path = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n",
        "    #config_name = \"deepset/xlm-roberta-large-squad2\"\n",
        "    config_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n",
        "    fp16 = True if APEX_INSTALLED else False\n",
        "    fp16_opt_level = \"O1\"\n",
        "    \n",
        "    gradient_accumulation_steps =  2#@param {type:\"integer\"}\n",
        "\n",
        "    # tokenizer\n",
        "    tokenizer_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n",
        "#    max_seq_length = 384\n",
        "#    doc_stride = 128\n",
        "    max_seq_length =  400#@param {type:\"integer\"}\n",
        "    doc_stride =  135#@param {type:\"integer\"}\n",
        "\n",
        "    # train\n",
        "    epochs =  12#@param {type:\"integer\"}\n",
        "    train_batch_size = 4\n",
        "    eval_batch_size = 8\n",
        "\n",
        "    # optimizer\n",
        "    optimizer_type = 'AdamW'\n",
        "    learning_rate =  1e-7#@param {type:\"number\"}\n",
        "    weight_decay =   1e-3#@param {type:\"number\"}\n",
        "    epsilon = 1e-10 #@param {type:\"number\"}\n",
        "    max_grad_norm = 1.0 #@param {type:\"number\"}\n",
        "\n",
        "    # scheduler\n",
        "    decay_name = 'linear-warmup'\n",
        "    warmup_ratio = 0.1\n",
        "    scheduler_step = 12#@param {type:\"integer\"}\n",
        "\n",
        "    # logging\n",
        "    ####logging_steps = 10\n",
        "    logging_steps = 250\n",
        "\n",
        "    # evaluate\n",
        "    output_dir = 'output'\n",
        "    seed =  42#@param {type:\"integer\"}\n",
        "\n",
        "print(\"acc_step\", Config.gradient_accumulation_steps,\n",
        "      \"lr\", Config.learning_rate, \"wd\", Config.weight_decay,\n",
        "      \"{}-{}\".format(Config.max_seq_length, Config.doc_stride),\n",
        "      \"seed\", Config.seed)"
      ],
      "id": "5934fe6a",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc_step 2 lr 1e-07 wd 0.001 400-135 seed 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97366e5"
      },
      "source": [
        "### Data Factory"
      ],
      "id": "a97366e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7neglrAWwgj"
      },
      "source": [
        "#external_tamil_xquad = pd.read_csv('/content/input/squad-qa-tamil-dataset/squad_tamilQA.csv')\n",
        "#external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n"
      ],
      "id": "U7neglrAWwgj",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV7PZDwBXGkm"
      },
      "source": [
        "#external_tamil_xquad['language'] = \"tamil\"\n",
        "#external_tamil_xquad.head()"
      ],
      "id": "EV7PZDwBXGkm",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "E4GOW8CXdNFm",
        "outputId": "b270ce5e-7042-45ce-b8e2-c69bd9c5f588"
      },
      "source": [
        "import ast\n",
        "external_tamil_xquad2 = pd.read_csv('/content/input/google-translated-squad20-to-hindi-and-tamil/squad_ta.csv')\n",
        "\n",
        "text, start = [], []\n",
        "for i in range(len(external_hindi_xquad2)):\n",
        "  ans = external_hindi_xquad2['answers'][i]\n",
        "  dic = ast.literal_eval(ans)[0]\n",
        "  text.append(dic['text'])\n",
        "  start.append(dic['answer_start'])\n",
        "\n",
        "external_hindi_xquad2['answer_text'] = text\n",
        "external_hindi_xquad2['answer_start'] = start\n",
        "external_hindi_xquad2['language'] = \"hindi\"\n",
        "\n",
        "external_hindi_xquad2 = external_hindi_xquad2.drop(['answers','c_id','is_in'], axis=1)\n",
        "external_hindi_xquad2.head()"
      ],
      "id": "E4GOW8CXdNFm",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>बियॉन्से कब लोकप्रिय होने लगी?</td>\n",
              "      <td>बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...</td>\n",
              "      <td>1990 के दशक के अंत में</td>\n",
              "      <td>263</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>जब वह बड़ी हो रही थी तो बियॉन्से ने किन क्षेत्...</td>\n",
              "      <td>बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...</td>\n",
              "      <td>गायन और नृत्य</td>\n",
              "      <td>213</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>बियॉन्से ने डेस्टिनीज़ चाइल्ड को कब छोड़ा और ए...</td>\n",
              "      <td>बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...</td>\n",
              "      <td>2003</td>\n",
              "      <td>535</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>बेयोंसे किस शहर और राज्य में पली-बढ़ी?</td>\n",
              "      <td>बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...</td>\n",
              "      <td>ह्यूस्टन, टेक्सास</td>\n",
              "      <td>135</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>बियॉन्से किस दशक में प्रसिद्ध हुई?</td>\n",
              "      <td>बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...</td>\n",
              "      <td>1990 के दशक के अंत में</td>\n",
              "      <td>263</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         id  ... language\n",
              "0  56be85543aeaaa14008c9063  ...    hindi\n",
              "1  56be85543aeaaa14008c9065  ...    hindi\n",
              "2  56be85543aeaaa14008c9066  ...    hindi\n",
              "3  56bf6b0f3aeaaa14008c9601  ...    hindi\n",
              "4  56bf6b0f3aeaaa14008c9602  ...    hindi\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "mM4548dsf0DT",
        "outputId": "afbb05e0-c745-48b7-8411-fe1819d33dce"
      },
      "source": [
        "external_tamil_xquad2 = pd.read_csv('/content/input/google-translated-squad20-to-hindi-and-tamil/squad_ta.csv')\n",
        "\n",
        "text, start = [], []\n",
        "for i in range(len(external_tamil_xquad2)):\n",
        "  ans = external_tamil_xquad2['answers'][i]\n",
        "  dic = ast.literal_eval(ans)[0]\n",
        "  text.append(dic['text'])\n",
        "  start.append(dic['answer_start'])\n",
        "\n",
        "external_tamil_xquad2['answer_text'] = text\n",
        "external_tamil_xquad2['answer_start'] = start\n",
        "external_tamil_xquad['language'] = \"tamil\"\n",
        "\n",
        "external_tamil_xquad2 = external_tamil_xquad2.drop(['answers','c_id','is_in'], axis=1)\n",
        "external_tamil_xquad2.head()"
      ],
      "id": "mM4548dsf0DT",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>answer_start</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>பியோனஸ் எப்போது பிரபலமடையத் தொடங்கினார்?</td>\n",
              "      <td>பியான்ஸ் கிசெல்லே நோல்ஸ்-கார்ட்டர் (/ biːˈjɒns...</td>\n",
              "      <td>1990 களின் பிற்பகுதியில்</td>\n",
              "      <td>272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>பியோனஸ் வளரும் போது என்ன பகுதிகளில் போட்டியிட்...</td>\n",
              "      <td>பியான்ஸ் கிசெல்லே நோல்ஸ்-கார்ட்டர் (/ biːˈjɒns...</td>\n",
              "      <td>பாட்டு மற்றும் நடனம்</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>பியோனஸ் எப்போது டெஸ்டினியின் குழந்தையை விட்டு ...</td>\n",
              "      <td>பியான்ஸ் கிசெல்லே நோல்ஸ்-கார்ட்டர் (/ biːˈjɒns...</td>\n",
              "      <td>2003</td>\n",
              "      <td>562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>பியோனஸ் எந்த நகரம் மற்றும் மாநிலத்தில் வளர்ந்த...</td>\n",
              "      <td>பியான்ஸ் கிசெல்லே நோல்ஸ்-கார்ட்டர் (/ biːˈjɒns...</td>\n",
              "      <td>ஹூஸ்டன், டெக்சாஸ்</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>எந்த தசாப்தத்தில் பியோனஸ் பிரபலமானது?</td>\n",
              "      <td>பியான்ஸ் கிசெல்லே நோல்ஸ்-கார்ட்டர் (/ biːˈjɒns...</td>\n",
              "      <td>1990 களின் பிற்பகுதியில்</td>\n",
              "      <td>272</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         id  ... answer_start\n",
              "0  56be85543aeaaa14008c9063  ...          272\n",
              "1  56be85543aeaaa14008c9065  ...           -1\n",
              "2  56be85543aeaaa14008c9066  ...          562\n",
              "3  56bf6b0f3aeaaa14008c9601  ...           -1\n",
              "4  56bf6b0f3aeaaa14008c9602  ...          272\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clbXSiD3esR3"
      },
      "source": [
        "train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n",
        "external_train = pd.concat([external_hindi_xquad2, external_tamil_xquad2]).reset_index(drop=True)\n",
        "\n",
        "def create_folds(data, num_splits):\n",
        "    data[\"kfold\"] = -1\n",
        "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=6654) # 2021)\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    return data\n",
        "\n",
        "train = create_folds(train, num_splits=2)\n",
        "external_train[\"kfold\"] = -1\n",
        "external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
        "\n",
        "train = pd.concat([train, external_train]).reset_index(drop=True)\n",
        "\n",
        "def convert_answers(row):\n",
        "    return {'answer_start': [row[0]], 'text': [row[1]]}\n",
        "\n",
        "train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
      ],
      "id": "clbXSiD3esR3",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JyXbPmpiskD",
        "outputId": "c7378d25-8306-44fb-c8d4-80ef6889c613"
      },
      "source": [
        "print(len(train),len(train['id'].unique()))"
      ],
      "id": "6JyXbPmpiskD",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "186612 186612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DvLqQYPovst"
      },
      "source": [
        "# train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n",
        "# # train = pd.read_csv('../input/cleaned-data-for-chaii/cleaned_train.csv')\n",
        "# # train = train[train['language']=='tamil'].reset_index(drop=True)\n",
        "# #train = pd.read_csv('/content/input/chaiiextended-and-cleaned-datasetready-to-train/ChaiiExtended.csv')\n",
        "# #train['language'] = \"xxxx\"\n",
        "\n",
        "# test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n",
        "# external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n",
        "# external_hindi_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n",
        "# external_tamil_xquad = pd.read_csv('/content/input/squad-qa-tamil-dataset/squad_tamilQA.csv')\n",
        "# external_tamil_xquad['language'] = \"tamil\"\n",
        "# external_xquad = pd.concat([external_hindi_xquad, external_tamil_xquad])\n",
        "# external_train = external_hindi_xquad #pd.concat([external_mlqa, external_xquad]).reset_index(drop=True)\n",
        "\n",
        "# def create_folds(data, num_splits):\n",
        "#     data[\"kfold\"] = -1\n",
        "#     kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=6654) # 2021)\n",
        "#     for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n",
        "#         data.loc[v_, 'kfold'] = f\n",
        "#     return data\n",
        "\n",
        "# #####train = create_folds(train, num_splits=5)\n",
        "# train = create_folds(train, num_splits=2)\n",
        "# external_train[\"kfold\"] = -1\n",
        "# external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
        "# #external_train = create_folds(external_train, num_splits=3) # foldに分割（2021.10.09変更）\n",
        "# #external_train[\"kfold\"] = external_train[\"kfold\"] + 10\n",
        "\n",
        "# train = pd.concat([train, external_train]).reset_index(drop=True)\n",
        "\n",
        "# def convert_answers(row):\n",
        "#     return {'answer_start': [row[0]], 'text': [row[1]]}\n",
        "\n",
        "# train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
      ],
      "id": "8DvLqQYPovst",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1jBlfhBLqxc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "1d34470f-3ff4-49f3-924e-5369210b2dde"
      },
      "source": [
        "train.head()"
      ],
      "id": "g1jBlfhBLqxc",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>language</th>\n",
              "      <th>kfold</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>903deec17</td>\n",
              "      <td>ஒரு சாதாரண வளர்ந்த மனிதனுடைய எலும்புக்கூடு பின...</td>\n",
              "      <td>மனித உடலில் எத்தனை எலும்புகள் உள்ளன?</td>\n",
              "      <td>206</td>\n",
              "      <td>53</td>\n",
              "      <td>tamil</td>\n",
              "      <td>0</td>\n",
              "      <td>{'answer_start': [53], 'text': ['206']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d9841668c</td>\n",
              "      <td>காளிதாசன் (தேவநாகரி: कालिदास) சமஸ்கிருத இலக்கி...</td>\n",
              "      <td>காளிதாசன் எங்கு பிறந்தார்?</td>\n",
              "      <td>காசுமீரில்</td>\n",
              "      <td>2358</td>\n",
              "      <td>tamil</td>\n",
              "      <td>1</td>\n",
              "      <td>{'answer_start': [2358], 'text': ['காசுமீரில்']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29d154b56</td>\n",
              "      <td>சர் அலெக்ஸாண்டர் ஃபிளெமிங் (Sir Alexander Flem...</td>\n",
              "      <td>பென்சிலின் கண்டுபிடித்தவர் யார்?</td>\n",
              "      <td>சர் அலெக்ஸாண்டர் ஃபிளெமிங்</td>\n",
              "      <td>0</td>\n",
              "      <td>tamil</td>\n",
              "      <td>1</td>\n",
              "      <td>{'answer_start': [0], 'text': ['சர் அலெக்ஸாண்ட...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>41660850a</td>\n",
              "      <td>குழந்தையின் அழுகையை  நிறுத்தவும், தூங்க வைக்கவ...</td>\n",
              "      <td>தமிழ்நாட்டில் குழந்தைகளை தூங்க வைக்க பாடும் பா...</td>\n",
              "      <td>தாலாட்டு</td>\n",
              "      <td>68</td>\n",
              "      <td>tamil</td>\n",
              "      <td>1</td>\n",
              "      <td>{'answer_start': [68], 'text': ['தாலாட்டு']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b29c82c22</td>\n",
              "      <td>சூரியக் குடும்பம் \\nசூரியக் குடும்பம் (Solar S...</td>\n",
              "      <td>பூமியின் அருகில் உள்ள விண்மீன் எது?</td>\n",
              "      <td>சூரியனும்</td>\n",
              "      <td>585</td>\n",
              "      <td>tamil</td>\n",
              "      <td>0</td>\n",
              "      <td>{'answer_start': [585], 'text': ['சூரியனும்']}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                            answers\n",
              "0  903deec17  ...            {'answer_start': [53], 'text': ['206']}\n",
              "1  d9841668c  ...   {'answer_start': [2358], 'text': ['காசுமீரில்']}\n",
              "2  29d154b56  ...  {'answer_start': [0], 'text': ['சர் அலெக்ஸாண்ட...\n",
              "3  41660850a  ...       {'answer_start': [68], 'text': ['தாலாட்டு']}\n",
              "4  b29c82c22  ...     {'answer_start': [585], 'text': ['சூரியனும்']}\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb636dc6"
      },
      "source": [
        "if 0:\n",
        "    train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n",
        "    test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n",
        "    external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n",
        "    external_hindi_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n",
        "    external_tamil_xquad = pd.read_csv('/content/input/squad-qa-tamil-dataset/squad_tamilQA.csv')\n",
        "    external_tamil_xquad['language'] = \"tamil\"\n",
        "    external_xquad = pd.concat([external_hindi_xquad, external_tamil_xquad])\n",
        "    external_train = external_hindi_xquad # pd.concat([external_mlqa, external_xquad])\n",
        "\n",
        "    def create_folds(data, num_splits):\n",
        "        data[\"kfold\"] = -1\n",
        "        kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
        "        for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n",
        "            data.loc[v_, 'kfold'] = f\n",
        "        return data\n",
        "\n",
        "    train = create_folds(train, num_splits=5)\n",
        "    external_train[\"kfold\"] = -1\n",
        "    external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
        "    train = pd.concat([train, external_train]).reset_index(drop=True)\n",
        "\n",
        "    def convert_answers(row):\n",
        "        return {'answer_start': [row[0]], 'text': [row[1]]}\n",
        "\n",
        "    train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
      ],
      "id": "eb636dc6",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60781636"
      },
      "source": [
        "### Covert Examples to Features (Preprocess)"
      ],
      "id": "60781636"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63b399b9"
      },
      "source": [
        "def prepare_train_features(args, example, tokenizer):\n",
        "    example[\"question\"] = example[\"question\"].lstrip()\n",
        "    tokenized_example = tokenizer(\n",
        "        example[\"question\"],\n",
        "        example[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=args.max_seq_length,\n",
        "        stride=args.doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
        "\n",
        "    features = []\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        feature = {}\n",
        "\n",
        "        input_ids = tokenized_example[\"input_ids\"][i]\n",
        "        attention_mask = tokenized_example[\"attention_mask\"][i]\n",
        "\n",
        "        feature['input_ids'] = input_ids\n",
        "        feature['attention_mask'] = attention_mask\n",
        "        feature['offset_mapping'] = offsets\n",
        "\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        sequence_ids = tokenized_example.sequence_ids(i)\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = example[\"answers\"]\n",
        "\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            feature[\"start_position\"] = cls_index\n",
        "            feature[\"end_position\"] = cls_index\n",
        "        else:\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                feature[\"start_position\"] = cls_index\n",
        "                feature[\"end_position\"] = cls_index\n",
        "            else:\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                feature[\"start_position\"] = token_start_index - 1\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                feature[\"end_position\"] = token_end_index + 1\n",
        "\n",
        "        features.append(feature)\n",
        "    return features"
      ],
      "id": "63b399b9",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55553f9f"
      },
      "source": [
        "### Dataset Retriever"
      ],
      "id": "55553f9f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c78cee8c"
      },
      "source": [
        "class DatasetRetriever(Dataset):\n",
        "    def __init__(self, features, mode='train'):\n",
        "        super(DatasetRetriever, self).__init__()\n",
        "        self.features = features\n",
        "        self.mode = mode\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, item):   \n",
        "        feature = self.features[item]\n",
        "        if self.mode == 'train':\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
        "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
        "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                'offset_mapping':feature['offset_mapping'],\n",
        "                'sequence_ids':feature['sequence_ids'],\n",
        "                'id':feature['example_id'],\n",
        "                'context': feature['context'],\n",
        "                'question': feature['question']\n",
        "            }"
      ],
      "id": "c78cee8c",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7e0677b"
      },
      "source": [
        "### Model"
      ],
      "id": "b7e0677b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd46e25e"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, modelname_or_path, config):\n",
        "        super(Model, self).__init__()\n",
        "        self.n_msd = 5\n",
        "        self.config = config\n",
        "        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        # self.dropouts = nn.ModuleList([nn.Dropout(0.1) for _ in range(self.n_msd)])\n",
        "        self._init_weights(self.qa_outputs)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids, \n",
        "        attention_mask=None, \n",
        "        # token_type_ids=None\n",
        "    ):\n",
        "        outputs = self.xlm_roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]\n",
        "        \n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        qa_logits = self.qa_outputs(sequence_output)\n",
        "        # qa_logits = sum([self.qa_outputs(dropout(sequence_output)) for dropout in self.dropouts])/self.n_msd\n",
        "        #print(qa_logits.size())\n",
        "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "    \n",
        "        return start_logits, end_logits"
      ],
      "id": "cd46e25e",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b054f2e5"
      },
      "source": [
        "### Loss"
      ],
      "id": "b054f2e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSq0_x7xecl0"
      },
      "source": [
        "def compute_kl_loss(p, q, pad_mask=None):\n",
        "    start_preds1, end_preds1 = p\n",
        "    start_preds2, end_preds2 = q\n",
        "\n",
        "    p_loss1 = F.kl_div(F.log_softmax(start_preds1, dim=-1), F.softmax(start_preds2, dim=-1), reduction='none')\n",
        "    q_loss1 = F.kl_div(F.log_softmax(start_preds2, dim=-1), F.softmax(start_preds1, dim=-1), reduction='none')\n",
        "    p_loss2 = F.kl_div(F.log_softmax(end_preds1, dim=-1), F.softmax(end_preds2, dim=-1), reduction='none')\n",
        "    q_loss2 = F.kl_div(F.log_softmax(end_preds2, dim=-1), F.softmax(end_preds1, dim=-1), reduction='none')\n",
        "\n",
        "    # You can choose whether to use function \"sum\" and \"mean\" depending on your task\n",
        "    p_loss = p_loss1.sum() + p_loss2.sum() \n",
        "    q_loss = q_loss1.sum() + q_loss2.sum()\n",
        "\n",
        "    loss = (p_loss + q_loss) / 2\n",
        "    return loss"
      ],
      "id": "JSq0_x7xecl0",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCkwCpg0FtDa"
      },
      "source": [
        "def linear_combination(x, y, epsilon):\n",
        "    return (1 - epsilon) * x + epsilon * y\n",
        "\n",
        "def reduce_loss(loss, reduction='mean'):\n",
        "    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss"
      ],
      "id": "YCkwCpg0FtDa",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU1pWXuBF5TP"
      },
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, epsilon=0.1, ignore_index = -1, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.reduction = reduction\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, preds, target):\n",
        "        n = preds.size()[-1]\n",
        "        log_preds = F.log_softmax(preds, dim=-1)\n",
        "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
        "        nll = F.nll_loss(log_preds, target, reduction=self.reduction, ignore_index=self.ignore_index)\n",
        "        return linear_combination(nll, loss/n, self.epsilon)"
      ],
      "id": "vU1pWXuBF5TP",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f490e487"
      },
      "source": [
        "def loss_fn(preds, labels):\n",
        "    start_preds, end_preds = preds\n",
        "    start_labels, end_labels = labels\n",
        "    \n",
        "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
        "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
        "    #start_loss = LabelSmoothingCrossEntropy(ignore_index=-1)(start_preds, start_labels)\n",
        "    #end_loss = LabelSmoothingCrossEntropy(ignore_index=-1)(end_preds, end_labels)\n",
        "    total_loss = (start_loss + end_loss) / 2\n",
        "    return total_loss"
      ],
      "id": "f490e487",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVSFIJZ5eh-Z"
      },
      "source": [
        "def loss_fn2(pred1, pred2, label):\n",
        "    # RMSE loss\n",
        "    loss1 = loss_fn(pred1, label)\n",
        "    loss2 = loss_fn(pred2, label)\n",
        "\n",
        "    # R-Drop\n",
        "    kl_loss = compute_kl_loss(pred1, pred2)\n",
        "\n",
        "    RDROP_ALPHA = 0.1\n",
        "    total_loss = (loss1 + loss2) / 2 + RDROP_ALPHA * kl_loss\n",
        "    return total_loss\n"
      ],
      "id": "WVSFIJZ5eh-Z",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9138cc0e"
      },
      "source": [
        "### Grouped Layerwise Learning Rate Decay"
      ],
      "id": "9138cc0e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9NTXqa0Va7V"
      },
      "source": [
        "# def make_model(args):\n",
        "#     config = AutoConfig.from_pretrained(args.config_name)\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
        "#     model = Model(args.model_name_or_path, config=config)\n",
        "#     return config, tokenizer, model\n",
        "\n",
        "# cfg, tk, m = make_model(Config)"
      ],
      "id": "l9NTXqa0Va7V",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lKHzzPsV6Hl"
      },
      "source": [
        "# x  = get_optimizer_grouped_parameters(Config(), m)"
      ],
      "id": "5lKHzzPsV6Hl",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e4bcde0"
      },
      "source": [
        "def get_optimizer_grouped_parameters(args, model):\n",
        "    model_type = 'xlm_roberta'\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters()\n",
        "                       if 'qa_outputs' in n],\n",
        "            \"weight_decay\": 0.0,\n",
        "            \"lr\": 1e-4 ####### args.learning_rate*5\n",
        "        },\n",
        "    ]\n",
        "    # print(optimizer_grouped_parameters)\n",
        "    num_layers = model.config.num_hidden_layers\n",
        "    layers = [getattr(model, model_type).embeddings] + list(getattr(model, model_type).encoder.layer)\n",
        "    layers.reverse()\n",
        "    lr = args.learning_rate\n",
        "    LR_DECAY = 0.95\n",
        "    for layer in layers:\n",
        "        lr *= LR_DECAY\n",
        "        optimizer_grouped_parameters += [\n",
        "            {\n",
        "                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": args.weight_decay,\n",
        "                \"lr\": lr,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "                \"lr\": lr,\n",
        "            },\n",
        "        ]\n",
        "    return optimizer_grouped_parameters\n",
        "\n",
        "\n",
        "# def get_optimizer_grouped_parameters(args, model):\n",
        "#     no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "#     group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
        "#     group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
        "#     group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "#     group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "#     optimizer_grouped_parameters = [\n",
        "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
        "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/8},\n",
        "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/4},\n",
        "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
        "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
        "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/8},\n",
        "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate/4},\n",
        "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
        "#         {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n",
        "#     ]\n",
        "#     return optimizer_grouped_parameters"
      ],
      "id": "4e4bcde0",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkWXuuwoQJM-"
      },
      "source": [
        "if 0:\n",
        "  def get_optimizer_grouped_parameters(args, model):\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
        "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
        "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n",
        "        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    return optimizer_grouped_parameters"
      ],
      "id": "VkWXuuwoQJM-",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d0ac100"
      },
      "source": [
        "### Metric Logger"
      ],
      "id": "2d0ac100"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65b3cd59"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.max = 0\n",
        "        self.min = 1e5\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        if val > self.max:\n",
        "            self.max = val\n",
        "        if val < self.min:\n",
        "            self.min = val"
      ],
      "id": "65b3cd59",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03f659a2"
      },
      "source": [
        "### Utilities"
      ],
      "id": "03f659a2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a7d36df"
      },
      "source": [
        "def make_model(args):\n",
        "    config = AutoConfig.from_pretrained(args.config_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
        "    model = Model(args.model_name_or_path, config=config)\n",
        "    return config, tokenizer, model\n",
        "\n",
        "def make_optimizer(args, model):\n",
        "    #################\n",
        "    optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n",
        "    # no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    # optimizer_grouped_parameters = [\n",
        "    #     {\n",
        "    #         \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "    #         \"weight_decay\": args.weight_decay,\n",
        "    #     },\n",
        "    #     {\n",
        "    #         \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "    #         \"weight_decay\": 0.0,\n",
        "    #     },\n",
        "    # ]\n",
        "    if args.optimizer_type == \"AdamW\":\n",
        "        optimizer = AdamW(\n",
        "            optimizer_grouped_parameters,\n",
        "            lr=args.learning_rate,\n",
        "            eps=args.epsilon,\n",
        "            correct_bias=True\n",
        "        )\n",
        "        return optimizer\n",
        "\n",
        "def make_scheduler(\n",
        "    args, optimizer, \n",
        "    num_warmup_steps, \n",
        "    num_training_steps\n",
        "):\n",
        "    if args.decay_name == \"cosine-warmup\":\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "    elif args.decay_name == \"liner-warmup\":\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "    else:\n",
        "        scheduler = get_constant_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "        )\n",
        "    return scheduler    \n",
        "\n",
        "def make_loader(\n",
        "    args, data, \n",
        "    tokenizer, fold\n",
        "):\n",
        "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
        "    \n",
        "    train_features, valid_features = [[] for _ in range(2)]\n",
        "    for i, row in train_set.iterrows():\n",
        "        train_features += prepare_train_features(args, row, tokenizer)\n",
        "    for i, row in valid_set.iterrows():\n",
        "        valid_features += prepare_train_features(args, row, tokenizer)\n",
        "\n",
        "    train_dataset = DatasetRetriever(train_features)\n",
        "    valid_dataset = DatasetRetriever(valid_features)\n",
        "    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n",
        "    \n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    valid_sampler = SequentialSampler(valid_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.train_batch_size,\n",
        "        sampler=train_sampler,\n",
        "        num_workers=optimal_num_of_loader_workers(),\n",
        "        pin_memory=True,\n",
        "        drop_last=False \n",
        "    )\n",
        "\n",
        "    valid_dataloader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=args.eval_batch_size, \n",
        "        sampler=valid_sampler,\n",
        "        num_workers=optimal_num_of_loader_workers(),\n",
        "        pin_memory=True, \n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    return train_dataloader, valid_dataloader"
      ],
      "id": "0a7d36df",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47101630"
      },
      "source": [
        "### Trainer"
      ],
      "id": "47101630"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81b92c70"
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self, model, tokenizer, \n",
        "        optimizer, scheduler\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def train(\n",
        "        self, args, \n",
        "        train_dataloader, \n",
        "        epoch, result_dict\n",
        "    ):\n",
        "        count = 0\n",
        "        losses = AverageMeter()\n",
        "        \n",
        "        self.model.zero_grad()\n",
        "        self.model.train()\n",
        "        \n",
        "        fix_all_seeds(args.seed)\n",
        "        \n",
        "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
        "                    batch_data['start_position'], batch_data['end_position']\n",
        "            \n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
        "\n",
        "            outputs_start, outputs_end = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "            \n",
        "            ########\n",
        "            # outputs_start2, outputs_end2 = self.model(\n",
        "            #     input_ids=input_ids,\n",
        "            #     attention_mask=attention_mask,\n",
        "            # )\n",
        "\n",
        "            #print(outputs_start.size())\n",
        "            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
        "            # loss = loss_fn2((outputs_start, outputs_end), (outputs_start2, outputs_end2), (targets_start, targets_end))\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            count += input_ids.size(0)\n",
        "            losses.update(loss.item(), input_ids.size(0))\n",
        "\n",
        "            # if args.fp16:\n",
        "            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n",
        "            # else:\n",
        "            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n",
        "                _s = str(len(str(len(train_dataloader.sampler))))\n",
        "                ret = [\n",
        "                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n",
        "                    'Train Loss: {: >4.5f}'.format(losses.avg),\n",
        "                ]\n",
        "                print(', '.join(ret))\n",
        "\n",
        "        result_dict['train_loss'].append(losses.avg)\n",
        "        return result_dict"
      ],
      "id": "81b92c70",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "468c7fad"
      },
      "source": [
        "### Evaluator"
      ],
      "id": "468c7fad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c77328b3"
      },
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    \n",
        "    def save(self, result, output_dir):\n",
        "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
        "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
        "\n",
        "    def evaluate(self, valid_dataloader, epoch, result_dict):\n",
        "        losses = AverageMeter()\n",
        "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
        "            self.model = self.model.eval()\n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
        "                    batch_data['start_position'], batch_data['end_position']\n",
        "            \n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
        "            \n",
        "            with torch.no_grad():            \n",
        "                outputs_start, outputs_end = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                )\n",
        "                \n",
        "                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
        "                losses.update(loss.item(), input_ids.size(0))\n",
        "                \n",
        "        print('----Validation Results Summary----')\n",
        "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
        "        result_dict['val_loss'].append(losses.avg)        \n",
        "        return result_dict"
      ],
      "id": "c77328b3",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86c0ad8e"
      },
      "source": [
        "### Initialize Training"
      ],
      "id": "86c0ad8e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edd188d2"
      },
      "source": [
        "def init_training(args, data, fold):\n",
        "    fix_all_seeds(args.seed)\n",
        "    \n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "    \n",
        "    # model\n",
        "    model_config, tokenizer, model = make_model(args)\n",
        "    if torch.cuda.device_count() >= 1:\n",
        "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
        "            torch.cuda.device_count(), \n",
        "            torch.cuda.get_device_name(0))\n",
        "        )\n",
        "        model = model.cuda() \n",
        "    else:\n",
        "        raise ValueError('CPU training is not supported')\n",
        "    \n",
        "    # print(\"load dict...\")\n",
        "    # model.load_state_dict(\n",
        "    #     torch.load(\"/content/input/chaii/output/checkpoint-fold-{}/pytorch_model.bin\".format(fold))\n",
        "    #     #torch.load(\"/content/drive/MyDrive/datas/chaii/output/output/checkpoint-fold-{}/pytorch_model.bin\".format(fold))\n",
        "    # );\n",
        "\n",
        "    # data loaders\n",
        "    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = make_optimizer(args, model)\n",
        "\n",
        "    # scheduler\n",
        "    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n",
        "    if args.warmup_ratio > 0:\n",
        "        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
        "    else:\n",
        "        num_warmup_steps = 0\n",
        "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n",
        "    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n",
        "\n",
        "    # mixed precision training with NVIDIA Apex\n",
        "    if args.fp16:\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "    \n",
        "    result_dict = {\n",
        "        'epoch':[], \n",
        "        'train_loss': [], \n",
        "        'val_loss' : [], \n",
        "        'best_val_loss': np.inf\n",
        "    }\n",
        "\n",
        "    return (\n",
        "        model, model_config, tokenizer, optimizer, scheduler, \n",
        "        train_dataloader, valid_dataloader, result_dict\n",
        "    )"
      ],
      "id": "edd188d2",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N-PyfKI9mzm"
      },
      "source": [
        ""
      ],
      "id": "_N-PyfKI9mzm",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb42970d"
      },
      "source": [
        "### Run"
      ],
      "id": "cb42970d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "915ef19d"
      },
      "source": [
        "def run(data, fold):\n",
        "    args = Config()\n",
        "    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n",
        "        valid_dataloader, result_dict = init_training(args, data, fold)\n",
        "    \n",
        "    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n",
        "    evaluator = Evaluator(model)\n",
        "\n",
        "    train_time_list = []\n",
        "    valid_time_list = []\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        result_dict['epoch'].append(epoch)\n",
        "\n",
        "        # Train\n",
        "        torch.cuda.synchronize()\n",
        "        tic1 = time.time()\n",
        "        result_dict = trainer.train(\n",
        "            args, train_dataloader, \n",
        "            epoch, result_dict\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        tic2 = time.time() \n",
        "        train_time_list.append(tic2 - tic1)\n",
        "        \n",
        "        # Evaluate\n",
        "        torch.cuda.synchronize()\n",
        "        tic3 = time.time()\n",
        "        result_dict = evaluator.evaluate(\n",
        "            valid_dataloader, epoch, result_dict\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        tic4 = time.time() \n",
        "        valid_time_list.append(tic4 - tic3)\n",
        "            \n",
        "        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n",
        "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
        "            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
        "            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n",
        "            \n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
        "            model_config.save_pretrained(output_dir)\n",
        "            tokenizer.save_pretrained(output_dir)\n",
        "            print(f\"Saving model checkpoint to {output_dir}.\")\n",
        "            \n",
        "        print()\n",
        "\n",
        "    evaluator.save(result_dict, output_dir)\n",
        "    \n",
        "    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n",
        "    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n",
        "    \n",
        "    best = result_dict[\"best_val_loss\"]\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    del trainer, evaluator\n",
        "    del model, model_config, tokenizer\n",
        "    del optimizer, scheduler\n",
        "    del train_dataloader, valid_dataloader, result_dict\n",
        "    gc.collect()\n",
        "\n",
        "    return best"
      ],
      "id": "915ef19d",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7a66e0a",
        "outputId": "b4a53130-5dbe-4a95-e8a8-f8a4b6d3f2b3"
      },
      "source": [
        "best = []\n",
        "for fold in range(2):\n",
        "    # if fold >= 2 :\n",
        "    #   continue\n",
        "    print();print()\n",
        "    print('-'*50)\n",
        "    print(f'FOLD: {fold}')\n",
        "    print('-'*50)\n",
        "    ret = run(train, fold)\n",
        "    !cp -r ./output /content/drive/MyDrive/datas/chaii2/output\n",
        "    best.append(ret)\n",
        "\n",
        "print(best)"
      ],
      "id": "b7a66e0a",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "FOLD: 0\n",
            "--------------------------------------------------\n",
            "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
            "Num examples Train= 8126, Num examples Valid=7360\n",
            "Total Training Steps: 12192, Total Warmup Steps: 1219\n",
            "Epoch: 00 [   4/8126 (  0%)], Train Loss: 2.80998\n",
            "Epoch: 00 [1004/8126 ( 12%)], Train Loss: 2.77864\n",
            "Epoch: 00 [2004/8126 ( 25%)], Train Loss: 2.55638\n",
            "Epoch: 00 [3004/8126 ( 37%)], Train Loss: 2.20171\n",
            "Epoch: 00 [4004/8126 ( 49%)], Train Loss: 1.82611\n",
            "Epoch: 00 [5004/8126 ( 62%)], Train Loss: 1.55395\n",
            "Epoch: 00 [6004/8126 ( 74%)], Train Loss: 1.35868\n",
            "Epoch: 00 [7004/8126 ( 86%)], Train Loss: 1.21398\n",
            "Epoch: 00 [8004/8126 ( 98%)], Train Loss: 1.10679\n",
            "Epoch: 00 [8126/8126 (100%)], Train Loss: 1.09352\n",
            "----Validation Results Summary----\n",
            "Epoch: [0] Valid Loss: 0.33589\n",
            "0 Epoch, Best epoch was updated! Valid Loss: 0.33589\n",
            "Saving model checkpoint to output/checkpoint-fold-0.\n",
            "\n",
            "Epoch: 01 [   4/8126 (  0%)], Train Loss: 0.47059\n",
            "Epoch: 01 [1004/8126 ( 12%)], Train Loss: 0.29127\n",
            "Epoch: 01 [2004/8126 ( 25%)], Train Loss: 0.30533\n",
            "Epoch: 01 [3004/8126 ( 37%)], Train Loss: 0.30964\n",
            "Epoch: 01 [4004/8126 ( 49%)], Train Loss: 0.30143\n",
            "Epoch: 01 [5004/8126 ( 62%)], Train Loss: 0.29833\n",
            "Epoch: 01 [6004/8126 ( 74%)], Train Loss: 0.29323\n",
            "Epoch: 01 [7004/8126 ( 86%)], Train Loss: 0.28920\n",
            "Epoch: 01 [8004/8126 ( 98%)], Train Loss: 0.28914\n",
            "Epoch: 01 [8126/8126 (100%)], Train Loss: 0.28732\n",
            "----Validation Results Summary----\n",
            "Epoch: [1] Valid Loss: 0.32115\n",
            "1 Epoch, Best epoch was updated! Valid Loss: 0.32115\n",
            "Saving model checkpoint to output/checkpoint-fold-0.\n",
            "\n",
            "Epoch: 02 [   4/8126 (  0%)], Train Loss: 0.50389\n",
            "Epoch: 02 [1004/8126 ( 12%)], Train Loss: 0.23945\n",
            "Epoch: 02 [2004/8126 ( 25%)], Train Loss: 0.25642\n",
            "Epoch: 02 [3004/8126 ( 37%)], Train Loss: 0.26531\n",
            "Epoch: 02 [4004/8126 ( 49%)], Train Loss: 0.25930\n",
            "Epoch: 02 [5004/8126 ( 62%)], Train Loss: 0.25879\n",
            "Epoch: 02 [6004/8126 ( 74%)], Train Loss: 0.25557\n",
            "Epoch: 02 [7004/8126 ( 86%)], Train Loss: 0.25308\n",
            "Epoch: 02 [8004/8126 ( 98%)], Train Loss: 0.25389\n",
            "Epoch: 02 [8126/8126 (100%)], Train Loss: 0.25225\n",
            "----Validation Results Summary----\n",
            "Epoch: [2] Valid Loss: 0.30301\n",
            "2 Epoch, Best epoch was updated! Valid Loss: 0.30301\n",
            "Saving model checkpoint to output/checkpoint-fold-0.\n",
            "\n",
            "Epoch: 03 [   4/8126 (  0%)], Train Loss: 0.49383\n",
            "Epoch: 03 [1004/8126 ( 12%)], Train Loss: 0.21280\n",
            "Epoch: 03 [2004/8126 ( 25%)], Train Loss: 0.22958\n",
            "Epoch: 03 [3004/8126 ( 37%)], Train Loss: 0.24016\n",
            "Epoch: 03 [4004/8126 ( 49%)], Train Loss: 0.23466\n",
            "Epoch: 03 [5004/8126 ( 62%)], Train Loss: 0.23508\n",
            "Epoch: 03 [6004/8126 ( 74%)], Train Loss: 0.23251\n",
            "Epoch: 03 [7004/8126 ( 86%)], Train Loss: 0.23054\n",
            "Epoch: 03 [8004/8126 ( 98%)], Train Loss: 0.23143\n",
            "Epoch: 03 [8126/8126 (100%)], Train Loss: 0.22986\n",
            "----Validation Results Summary----\n",
            "Epoch: [3] Valid Loss: 0.29258\n",
            "3 Epoch, Best epoch was updated! Valid Loss: 0.29258\n",
            "Saving model checkpoint to output/checkpoint-fold-0.\n",
            "\n",
            "Epoch: 04 [   4/8126 (  0%)], Train Loss: 0.49463\n",
            "Epoch: 04 [1004/8126 ( 12%)], Train Loss: 0.19288\n",
            "Epoch: 04 [2004/8126 ( 25%)], Train Loss: 0.20945\n",
            "Epoch: 04 [3004/8126 ( 37%)], Train Loss: 0.22112\n",
            "Epoch: 04 [4004/8126 ( 49%)], Train Loss: 0.21598\n",
            "Epoch: 04 [5004/8126 ( 62%)], Train Loss: 0.21706\n",
            "Epoch: 04 [6004/8126 ( 74%)], Train Loss: 0.21485\n",
            "Epoch: 04 [7004/8126 ( 86%)], Train Loss: 0.21321\n",
            "Epoch: 04 [8004/8126 ( 98%)], Train Loss: 0.21405\n",
            "Epoch: 04 [8126/8126 (100%)], Train Loss: 0.21252\n",
            "----Validation Results Summary----\n",
            "Epoch: [4] Valid Loss: 0.28638\n",
            "4 Epoch, Best epoch was updated! Valid Loss: 0.28638\n",
            "Saving model checkpoint to output/checkpoint-fold-0.\n",
            "\n",
            "Epoch: 05 [   4/8126 (  0%)], Train Loss: 0.49758\n",
            "Epoch: 05 [1004/8126 ( 12%)], Train Loss: 0.17719\n",
            "Epoch: 05 [2004/8126 ( 25%)], Train Loss: 0.19336\n",
            "Epoch: 05 [3004/8126 ( 37%)], Train Loss: 0.20577\n",
            "Epoch: 05 [4004/8126 ( 49%)], Train Loss: 0.20091\n",
            "Epoch: 05 [5004/8126 ( 62%)], Train Loss: 0.20253\n",
            "Epoch: 05 [6004/8126 ( 74%)], Train Loss: 0.20053\n",
            "Epoch: 05 [7004/8126 ( 86%)], Train Loss: 0.19913\n",
            "Epoch: 05 [8004/8126 ( 98%)], Train Loss: 0.19989\n",
            "Epoch: 05 [8126/8126 (100%)], Train Loss: 0.19838\n",
            "----Validation Results Summary----\n",
            "Epoch: [5] Valid Loss: 0.28290\n",
            "5 Epoch, Best epoch was updated! Valid Loss: 0.28290\n",
            "Saving model checkpoint to output/checkpoint-fold-0.\n",
            "\n",
            "Epoch: 06 [   4/8126 (  0%)], Train Loss: 0.50039\n",
            "Epoch: 06 [1004/8126 ( 12%)], Train Loss: 0.16419\n",
            "Epoch: 06 [2004/8126 ( 25%)], Train Loss: 0.17982\n",
            "Epoch: 06 [3004/8126 ( 37%)], Train Loss: 0.19277\n",
            "Epoch: 06 [4004/8126 ( 49%)], Train Loss: 0.18818\n",
            "Epoch: 06 [5004/8126 ( 62%)], Train Loss: 0.19021\n",
            "Epoch: 06 [6004/8126 ( 74%)], Train Loss: 0.18835\n",
            "Epoch: 06 [7004/8126 ( 86%)], Train Loss: 0.18719\n",
            "Epoch: 06 [8004/8126 ( 98%)], Train Loss: 0.18785\n",
            "Epoch: 06 [8126/8126 (100%)], Train Loss: 0.18637\n",
            "----Validation Results Summary----\n",
            "Epoch: [6] Valid Loss: 0.28121\n",
            "6 Epoch, Best epoch was updated! Valid Loss: 0.28121\n",
            "Saving model checkpoint to output/checkpoint-fold-0.\n",
            "\n",
            "Epoch: 07 [   4/8126 (  0%)], Train Loss: 0.50097\n",
            "Epoch: 07 [1004/8126 ( 12%)], Train Loss: 0.15314\n",
            "Epoch: 07 [2004/8126 ( 25%)], Train Loss: 0.16816\n",
            "Epoch: 07 [3004/8126 ( 37%)], Train Loss: 0.18139\n",
            "Epoch: 07 [4004/8126 ( 49%)], Train Loss: 0.17705\n",
            "Epoch: 07 [5004/8126 ( 62%)], Train Loss: 0.17938\n",
            "Epoch: 07 [6004/8126 ( 74%)], Train Loss: 0.17760\n",
            "Epoch: 07 [7004/8126 ( 86%)], Train Loss: 0.17667\n",
            "Epoch: 07 [8004/8126 ( 98%)], Train Loss: 0.17726\n",
            "Epoch: 07 [8126/8126 (100%)], Train Loss: 0.17581\n",
            "----Validation Results Summary----\n",
            "Epoch: [7] Valid Loss: 0.28071\n",
            "7 Epoch, Best epoch was updated! Valid Loss: 0.28071\n",
            "Saving model checkpoint to output/checkpoint-fold-0.\n",
            "\n",
            "Epoch: 08 [   4/8126 (  0%)], Train Loss: 0.49891\n",
            "Epoch: 08 [1004/8126 ( 12%)], Train Loss: 0.14344\n",
            "Epoch: 08 [2004/8126 ( 25%)], Train Loss: 0.15788\n",
            "Epoch: 08 [3004/8126 ( 37%)], Train Loss: 0.17121\n",
            "Epoch: 08 [4004/8126 ( 49%)], Train Loss: 0.16703\n",
            "Epoch: 08 [5004/8126 ( 62%)], Train Loss: 0.16956\n",
            "Epoch: 08 [6004/8126 ( 74%)], Train Loss: 0.16781\n",
            "Epoch: 08 [7004/8126 ( 86%)], Train Loss: 0.16708\n",
            "Epoch: 08 [8004/8126 ( 98%)], Train Loss: 0.16760\n",
            "Epoch: 08 [8126/8126 (100%)], Train Loss: 0.16620\n",
            "----Validation Results Summary----\n",
            "Epoch: [8] Valid Loss: 0.28109\n",
            "\n",
            "Epoch: 09 [   4/8126 (  0%)], Train Loss: 0.49417\n",
            "Epoch: 09 [1004/8126 ( 12%)], Train Loss: 0.13472\n",
            "Epoch: 09 [2004/8126 ( 25%)], Train Loss: 0.14847\n",
            "Epoch: 09 [3004/8126 ( 37%)], Train Loss: 0.16172\n",
            "Epoch: 09 [4004/8126 ( 49%)], Train Loss: 0.15770\n",
            "Epoch: 09 [5004/8126 ( 62%)], Train Loss: 0.16037\n",
            "Epoch: 09 [6004/8126 ( 74%)], Train Loss: 0.15864\n",
            "Epoch: 09 [7004/8126 ( 86%)], Train Loss: 0.15811\n",
            "Epoch: 09 [8004/8126 ( 98%)], Train Loss: 0.15858\n",
            "Epoch: 09 [8126/8126 (100%)], Train Loss: 0.15722\n",
            "----Validation Results Summary----\n",
            "Epoch: [9] Valid Loss: 0.28222\n",
            "\n",
            "Epoch: 10 [   4/8126 (  0%)], Train Loss: 0.48660\n",
            "Epoch: 10 [1004/8126 ( 12%)], Train Loss: 0.12667\n",
            "Epoch: 10 [2004/8126 ( 25%)], Train Loss: 0.13969\n",
            "Epoch: 10 [3004/8126 ( 37%)], Train Loss: 0.15281\n",
            "Epoch: 10 [4004/8126 ( 49%)], Train Loss: 0.14897\n",
            "Epoch: 10 [5004/8126 ( 62%)], Train Loss: 0.15170\n",
            "Epoch: 10 [6004/8126 ( 74%)], Train Loss: 0.14996\n",
            "Epoch: 10 [7004/8126 ( 86%)], Train Loss: 0.14962\n",
            "Epoch: 10 [8004/8126 ( 98%)], Train Loss: 0.15003\n",
            "Epoch: 10 [8126/8126 (100%)], Train Loss: 0.14872\n",
            "----Validation Results Summary----\n",
            "Epoch: [10] Valid Loss: 0.28390\n",
            "\n",
            "Epoch: 11 [   4/8126 (  0%)], Train Loss: 0.47630\n",
            "Epoch: 11 [1004/8126 ( 12%)], Train Loss: 0.11917\n",
            "Epoch: 11 [2004/8126 ( 25%)], Train Loss: 0.13134\n",
            "Epoch: 11 [3004/8126 ( 37%)], Train Loss: 0.14429\n",
            "Epoch: 11 [4004/8126 ( 49%)], Train Loss: 0.14066\n",
            "Epoch: 11 [5004/8126 ( 62%)], Train Loss: 0.14338\n",
            "Epoch: 11 [6004/8126 ( 74%)], Train Loss: 0.14163\n",
            "Epoch: 11 [7004/8126 ( 86%)], Train Loss: 0.14145\n",
            "Epoch: 11 [8004/8126 ( 98%)], Train Loss: 0.14184\n",
            "Epoch: 11 [8126/8126 (100%)], Train Loss: 0.14058\n",
            "----Validation Results Summary----\n",
            "Epoch: [11] Valid Loss: 0.28616\n",
            "\n",
            "Total Training Time: 14225.399207353592secs, Average Training Time per Epoch: 1185.4499339461327secs.\n",
            "Total Validation Time: 3917.032299757004secs, Average Validation Time per Epoch: 326.41935831308365secs.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "FOLD: 1\n",
            "--------------------------------------------------\n",
            "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
            "Num examples Train= 8658, Num examples Valid=6828\n",
            "Total Training Steps: 12996, Total Warmup Steps: 1299\n",
            "Epoch: 00 [   4/8658 (  0%)], Train Loss: 2.91612\n",
            "Epoch: 00 [1004/8658 ( 12%)], Train Loss: 2.77143\n",
            "Epoch: 00 [2004/8658 ( 23%)], Train Loss: 2.55470\n",
            "Epoch: 00 [3004/8658 ( 35%)], Train Loss: 2.21518\n",
            "Epoch: 00 [4004/8658 ( 46%)], Train Loss: 1.83621\n",
            "Epoch: 00 [5004/8658 ( 58%)], Train Loss: 1.56738\n",
            "Epoch: 00 [6004/8658 ( 69%)], Train Loss: 1.36299\n",
            "Epoch: 00 [7004/8658 ( 81%)], Train Loss: 1.22187\n",
            "Epoch: 00 [8004/8658 ( 92%)], Train Loss: 1.11226\n",
            "Epoch: 00 [8658/8658 (100%)], Train Loss: 1.05249\n",
            "----Validation Results Summary----\n",
            "Epoch: [0] Valid Loss: 0.35854\n",
            "0 Epoch, Best epoch was updated! Valid Loss: 0.35854\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 01 [   4/8658 (  0%)], Train Loss: 1.10845\n",
            "Epoch: 01 [1004/8658 ( 12%)], Train Loss: 0.29294\n",
            "Epoch: 01 [2004/8658 ( 23%)], Train Loss: 0.27543\n",
            "Epoch: 01 [3004/8658 ( 35%)], Train Loss: 0.27983\n",
            "Epoch: 01 [4004/8658 ( 46%)], Train Loss: 0.27108\n",
            "Epoch: 01 [5004/8658 ( 58%)], Train Loss: 0.27729\n",
            "Epoch: 01 [6004/8658 ( 69%)], Train Loss: 0.26813\n",
            "Epoch: 01 [7004/8658 ( 81%)], Train Loss: 0.27100\n",
            "Epoch: 01 [8004/8658 ( 92%)], Train Loss: 0.27070\n",
            "Epoch: 01 [8658/8658 (100%)], Train Loss: 0.27042\n",
            "----Validation Results Summary----\n",
            "Epoch: [1] Valid Loss: 0.34089\n",
            "1 Epoch, Best epoch was updated! Valid Loss: 0.34089\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 02 [   4/8658 (  0%)], Train Loss: 1.14422\n",
            "Epoch: 02 [1004/8658 ( 12%)], Train Loss: 0.24935\n",
            "Epoch: 02 [2004/8658 ( 23%)], Train Loss: 0.23584\n",
            "Epoch: 02 [3004/8658 ( 35%)], Train Loss: 0.24057\n",
            "Epoch: 02 [4004/8658 ( 46%)], Train Loss: 0.23423\n",
            "Epoch: 02 [5004/8658 ( 58%)], Train Loss: 0.24156\n",
            "Epoch: 02 [6004/8658 ( 69%)], Train Loss: 0.23472\n",
            "Epoch: 02 [7004/8658 ( 81%)], Train Loss: 0.23885\n",
            "Epoch: 02 [8004/8658 ( 92%)], Train Loss: 0.23903\n",
            "Epoch: 02 [8658/8658 (100%)], Train Loss: 0.23921\n",
            "----Validation Results Summary----\n",
            "Epoch: [2] Valid Loss: 0.32000\n",
            "2 Epoch, Best epoch was updated! Valid Loss: 0.32000\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 03 [   4/8658 (  0%)], Train Loss: 1.12406\n",
            "Epoch: 03 [1004/8658 ( 12%)], Train Loss: 0.22791\n",
            "Epoch: 03 [2004/8658 ( 23%)], Train Loss: 0.21555\n",
            "Epoch: 03 [3004/8658 ( 35%)], Train Loss: 0.21901\n",
            "Epoch: 03 [4004/8658 ( 46%)], Train Loss: 0.21323\n",
            "Epoch: 03 [5004/8658 ( 58%)], Train Loss: 0.22052\n",
            "Epoch: 03 [6004/8658 ( 69%)], Train Loss: 0.21470\n",
            "Epoch: 03 [7004/8658 ( 81%)], Train Loss: 0.21915\n",
            "Epoch: 03 [8004/8658 ( 92%)], Train Loss: 0.21934\n",
            "Epoch: 03 [8658/8658 (100%)], Train Loss: 0.21959\n",
            "----Validation Results Summary----\n",
            "Epoch: [3] Valid Loss: 0.30655\n",
            "3 Epoch, Best epoch was updated! Valid Loss: 0.30655\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 04 [   4/8658 (  0%)], Train Loss: 1.08523\n",
            "Epoch: 04 [1004/8658 ( 12%)], Train Loss: 0.21226\n",
            "Epoch: 04 [2004/8658 ( 23%)], Train Loss: 0.20065\n",
            "Epoch: 04 [3004/8658 ( 35%)], Train Loss: 0.20287\n",
            "Epoch: 04 [4004/8658 ( 46%)], Train Loss: 0.19729\n",
            "Epoch: 04 [5004/8658 ( 58%)], Train Loss: 0.20441\n",
            "Epoch: 04 [6004/8658 ( 69%)], Train Loss: 0.19927\n",
            "Epoch: 04 [7004/8658 ( 81%)], Train Loss: 0.20386\n",
            "Epoch: 04 [8004/8658 ( 92%)], Train Loss: 0.20405\n",
            "Epoch: 04 [8658/8658 (100%)], Train Loss: 0.20435\n",
            "----Validation Results Summary----\n",
            "Epoch: [4] Valid Loss: 0.29712\n",
            "4 Epoch, Best epoch was updated! Valid Loss: 0.29712\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 05 [   4/8658 (  0%)], Train Loss: 1.03961\n",
            "Epoch: 05 [1004/8658 ( 12%)], Train Loss: 0.19955\n",
            "Epoch: 05 [2004/8658 ( 23%)], Train Loss: 0.18849\n",
            "Epoch: 05 [3004/8658 ( 35%)], Train Loss: 0.18968\n",
            "Epoch: 05 [4004/8658 ( 46%)], Train Loss: 0.18413\n",
            "Epoch: 05 [5004/8658 ( 58%)], Train Loss: 0.19105\n",
            "Epoch: 05 [6004/8658 ( 69%)], Train Loss: 0.18640\n",
            "Epoch: 05 [7004/8658 ( 81%)], Train Loss: 0.19103\n",
            "Epoch: 05 [8004/8658 ( 92%)], Train Loss: 0.19124\n",
            "Epoch: 05 [8658/8658 (100%)], Train Loss: 0.19161\n",
            "----Validation Results Summary----\n",
            "Epoch: [5] Valid Loss: 0.29048\n",
            "5 Epoch, Best epoch was updated! Valid Loss: 0.29048\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 06 [   4/8658 (  0%)], Train Loss: 0.99412\n",
            "Epoch: 06 [1004/8658 ( 12%)], Train Loss: 0.18888\n",
            "Epoch: 06 [2004/8658 ( 23%)], Train Loss: 0.17799\n",
            "Epoch: 06 [3004/8658 ( 35%)], Train Loss: 0.17837\n",
            "Epoch: 06 [4004/8658 ( 46%)], Train Loss: 0.17278\n",
            "Epoch: 06 [5004/8658 ( 58%)], Train Loss: 0.17948\n",
            "Epoch: 06 [6004/8658 ( 69%)], Train Loss: 0.17523\n",
            "Epoch: 06 [7004/8658 ( 81%)], Train Loss: 0.17991\n",
            "Epoch: 06 [8004/8658 ( 92%)], Train Loss: 0.18012\n",
            "Epoch: 06 [8658/8658 (100%)], Train Loss: 0.18057\n",
            "----Validation Results Summary----\n",
            "Epoch: [6] Valid Loss: 0.28591\n",
            "6 Epoch, Best epoch was updated! Valid Loss: 0.28591\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 07 [   4/8658 (  0%)], Train Loss: 0.95047\n",
            "Epoch: 07 [1004/8658 ( 12%)], Train Loss: 0.17921\n",
            "Epoch: 07 [2004/8658 ( 23%)], Train Loss: 0.16844\n",
            "Epoch: 07 [3004/8658 ( 35%)], Train Loss: 0.16828\n",
            "Epoch: 07 [4004/8658 ( 46%)], Train Loss: 0.16264\n",
            "Epoch: 07 [5004/8658 ( 58%)], Train Loss: 0.16912\n",
            "Epoch: 07 [6004/8658 ( 69%)], Train Loss: 0.16521\n",
            "Epoch: 07 [7004/8658 ( 81%)], Train Loss: 0.16995\n",
            "Epoch: 07 [8004/8658 ( 92%)], Train Loss: 0.17015\n",
            "Epoch: 07 [8658/8658 (100%)], Train Loss: 0.17068\n",
            "----Validation Results Summary----\n",
            "Epoch: [7] Valid Loss: 0.28289\n",
            "7 Epoch, Best epoch was updated! Valid Loss: 0.28289\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 08 [   4/8658 (  0%)], Train Loss: 0.90882\n",
            "Epoch: 08 [1004/8658 ( 12%)], Train Loss: 0.17041\n",
            "Epoch: 08 [2004/8658 ( 23%)], Train Loss: 0.15962\n",
            "Epoch: 08 [3004/8658 ( 35%)], Train Loss: 0.15911\n",
            "Epoch: 08 [4004/8658 ( 46%)], Train Loss: 0.15341\n",
            "Epoch: 08 [5004/8658 ( 58%)], Train Loss: 0.15966\n",
            "Epoch: 08 [6004/8658 ( 69%)], Train Loss: 0.15602\n",
            "Epoch: 08 [7004/8658 ( 81%)], Train Loss: 0.16080\n",
            "Epoch: 08 [8004/8658 ( 92%)], Train Loss: 0.16100\n",
            "Epoch: 08 [8658/8658 (100%)], Train Loss: 0.16159\n",
            "----Validation Results Summary----\n",
            "Epoch: [8] Valid Loss: 0.28119\n",
            "8 Epoch, Best epoch was updated! Valid Loss: 0.28119\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 09 [   4/8658 (  0%)], Train Loss: 0.86854\n",
            "Epoch: 09 [1004/8658 ( 12%)], Train Loss: 0.16233\n",
            "Epoch: 09 [2004/8658 ( 23%)], Train Loss: 0.15136\n",
            "Epoch: 09 [3004/8658 ( 35%)], Train Loss: 0.15063\n",
            "Epoch: 09 [4004/8658 ( 46%)], Train Loss: 0.14484\n",
            "Epoch: 09 [5004/8658 ( 58%)], Train Loss: 0.15087\n",
            "Epoch: 09 [6004/8658 ( 69%)], Train Loss: 0.14747\n",
            "Epoch: 09 [7004/8658 ( 81%)], Train Loss: 0.15224\n",
            "Epoch: 09 [8004/8658 ( 92%)], Train Loss: 0.15244\n",
            "Epoch: 09 [8658/8658 (100%)], Train Loss: 0.15309\n",
            "----Validation Results Summary----\n",
            "Epoch: [9] Valid Loss: 0.28054\n",
            "9 Epoch, Best epoch was updated! Valid Loss: 0.28054\n",
            "Saving model checkpoint to output/checkpoint-fold-1.\n",
            "\n",
            "Epoch: 10 [   4/8658 (  0%)], Train Loss: 0.82764\n",
            "Epoch: 10 [1004/8658 ( 12%)], Train Loss: 0.15478\n",
            "Epoch: 10 [2004/8658 ( 23%)], Train Loss: 0.14368\n",
            "Epoch: 10 [3004/8658 ( 35%)], Train Loss: 0.14288\n",
            "Epoch: 10 [4004/8658 ( 46%)], Train Loss: 0.13699\n",
            "Epoch: 10 [5004/8658 ( 58%)], Train Loss: 0.14277\n",
            "Epoch: 10 [6004/8658 ( 69%)], Train Loss: 0.13958\n",
            "Epoch: 10 [7004/8658 ( 81%)], Train Loss: 0.14430\n",
            "Epoch: 10 [8004/8658 ( 92%)], Train Loss: 0.14450\n",
            "Epoch: 10 [8658/8658 (100%)], Train Loss: 0.14520\n",
            "----Validation Results Summary----\n",
            "Epoch: [10] Valid Loss: 0.28078\n",
            "\n",
            "Epoch: 11 [   4/8658 (  0%)], Train Loss: 0.78591\n",
            "Epoch: 11 [1004/8658 ( 12%)], Train Loss: 0.14747\n",
            "Epoch: 11 [2004/8658 ( 23%)], Train Loss: 0.13624\n",
            "Epoch: 11 [3004/8658 ( 35%)], Train Loss: 0.13542\n",
            "Epoch: 11 [4004/8658 ( 46%)], Train Loss: 0.12937\n",
            "Epoch: 11 [5004/8658 ( 58%)], Train Loss: 0.13492\n",
            "Epoch: 11 [6004/8658 ( 69%)], Train Loss: 0.13194\n",
            "Epoch: 11 [7004/8658 ( 81%)], Train Loss: 0.13660\n",
            "Epoch: 11 [8004/8658 ( 92%)], Train Loss: 0.13678\n",
            "Epoch: 11 [8658/8658 (100%)], Train Loss: 0.13752\n",
            "----Validation Results Summary----\n",
            "Epoch: [11] Valid Loss: 0.28180\n",
            "\n",
            "Total Training Time: 15155.818754673004secs, Average Training Time per Epoch: 1262.9848962227504secs.\n",
            "Total Validation Time: 3634.891853570938secs, Average Validation Time per Epoch: 302.90765446424484secs.\n",
            "[0.28071214857697235, 0.2805420857765256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaDr_pFZGOQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa8b0b4-99f6-4284-c68b-51e1a65bdfd2"
      },
      "source": [
        "np.mean(np.array(best))"
      ],
      "id": "BaDr_pFZGOQY",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.280627117176749"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c600bffd"
      },
      "source": [
        "## ここをチェック"
      ],
      "id": "c600bffd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lVx1gGZrn-q"
      },
      "source": [
        "# trainデータなし学習 valideデータはtrain.csvではないので注意\n",
        "#[1.2976857284959564, 1.3881498964859058, 1.3087206599546626, 1.2901440256843426, 1.1679372001281267]\n",
        "#[0.24349995289113208, 0.2663524366251205, 0.19786230920665912, 0.2553937918101003, 0.18950824988041495]\n",
        "#[0.24366667471828746, 0.28473904721862214, 0.15692045553454106, 0.24553805843296678, 0.18429825624839946]\n",
        "#[0.24311262005966766, 0.284736886900765, 0.15840444342909, 0.24527732736873206, 0.18645570367727277]\n",
        "#[0.23844826735717128, 0.28548798921814356, 0.15855387426083375, 0.24254149177365683, 0.18242730208896807]\n",
        "#[0.23633872994702976, 0.2876898618870076]\n",
        "\n",
        "#[0.2399075371348521, 0.21504926022690454, 0.28804823179640204]\n",
        "\n",
        "\n",
        "#tamil\n",
        "#[0.24800505987951157, 0.28482119431719183, 0.15574497073223997, 0.24578075964718757, 0.17901662780208277]\n",
        "\n",
        "\n",
        "\n",
        "# train & hindi xquad\n",
        "#[0.24965187649773146, 0.23444136391208764, 0.22017900116515676] 0.23475741385832527\n",
        "#[0.23096489540942408, 0.28216897352464915, 0.2203173596892458, 0.28736751974058533, 0.208414323353315] 0.2458466143434439\n",
        "#[0.22519750105769248, 0.25939778196668173, 0.20381177481000717, 0.2465142556059104, 0.17124023813670544] 0.22123231031539947\n",
        "#[0.2697170414320415, 0.3059851324188871, 0.2551263245063695, 0.3056244693620248, 0.21533218981467775] 0.2703570315068001\n",
        "#[0.22526602439154889, 0.26299701667625947, 0.20623224054625405, 0.26411216159941286, 0.182503771575793] 0.22822224295785362\n",
        "#[0.23691583103636119, 0.2686926209007856, 0.2075012739026476, 0.25571625346175986, 0.19657193895548936]\n",
        "#[0.23888897450804394, 0.26591575717939314, 0.19279731265472355, 0.275491834834653, 0.1854117791257561]\n",
        "#[0.2519814316728135, 0.26924624716331114, 0.19539388123118323, 0.27420934634778854, 0.184092709692541]\n",
        "#[0.2530554625445571, 0.27373010504284945, 0.20456079005861058, 0.28659096361988684, 0.20325152608371191]\n",
        "\n",
        "# ★★ここをベースに検討★★\n",
        "# hindi xsquid acc_step 2 lr 1e-05 wd 0.001 400-135 seed 42 msd 5\n",
        "# [0.2302274905697632, 0.27069914932622796, 0.21486875825171511, 0.2573498566401873, 0.18481068504706707] 0.23159118796699213 LB:0.768\n",
        "#-----------\n",
        "# hindi xsquid acc_step 2 lr 1e-05 wd 0.001 400-135 seed 42 msd 5 scheduler_step 2 FOLDデータが若干違う　10/12 chaii-1012\n",
        "#[0.24841012325690787, 0.26584840404003096, 0.2032461542127646, 0.2737542687907607, 0.22716575367217018] LB:0.776\n",
        "#-----------\n",
        "# hindi xsquid acc_step 2 lr 1.5e-05 wd 0.001 400-135 seed 42 msd 5 scheduler_step 2 FOLDデータが若干違う　10/13 chaii-1013\n",
        "# [0.24829580425794023, 0.2651147856025376, 0.21895726531102902, 5.991464610776605, 0.228712301645232] LB:0.778\n",
        "#      FOLD3 単独でやり直し（学習ミスのため）　1e-5 s-step3に変更 fold3=0.25424\n",
        "#-----------\n",
        "# hindi xsquid acc_step 2 lr 1e-05 wd 0.001 400-135 seed 42 msd 5 scheduler_step 3 FOLDデータが若干違う　※10/13が収束しなかったので変更\n",
        "# [0.23505390209724317, 0.2598879741057279, 0.20111809600251632, 0.2542404424046021, 0.18050300175809506] chaii-1014 LB:0.776\n",
        "\n",
        "# hindi xsquid acc_step 2 lr 9e-06 wd 0.001 400-135 seed 42 msd 5 scheduler_step 3 FOLDデータが若干違う　10/14\n",
        "# [0.23501111353810378, 0.2556940006469886, 0.19985474928692182, 0.256803787264871, 0.18670242301902526] chaii-2014-2 LB:0.765\n",
        "\n",
        "# hindi xsquid acc_step 2 lr 1.5e-05 wd 0.001 400-135 seed 923 msd 5 scheduler_step 2 FOLDデータが若干違う 10/14-2\n",
        "#[0.22942558399286914, 0.27648799994582035, 0.20045122814375774, 0.2518382845039409, 0.1895024780733636]\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "2lVx1gGZrn-q",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlNTCqWhlCkv"
      },
      "source": [
        "# [0.2639688412511144, 0.23578763837464295, 0.2082770317994482] tamil & hindi xsquad  lr = 1.5e-5 LB:0.758\n",
        "# [0.2692653296424045, 0.23645380146391437, 0.21420694968205684] hindi xsquad only  lr = 1.5e-5 LB:0.761\n",
        "# [0.3102661463084304, 0.23890210809797524, 0.22127942127638148] tamil hindi xsquad + hindi mlpa lr = 1.5e-5 LB:0.775 \n",
        "# [0.2616716774835217, 0.2830840063208972,  0.22212529014537136] train data only  lr = 1.5e-5\n",
        "\n",
        "# lr = 1.5e-5\n",
        "# [0.2720036358349279, 0.32137186476052365, 0.22517818749821006, 0.27430064060954523, 0.2065113527196303] LB:764\n",
        "\n",
        "# all acc_step 4 lr 1e-05 wd 0.001 400-135 seed 42 msd 8\n",
        "#[0.2674649548349373, 0.30405822478353256, 0.22417820907238017, 0.27306132025548135, 0.19995228248329291] chaii3-1006\n",
        "# hindi xsquid acc_step 2 lr 1e-05 wd 0.001 400-135 seed 42 msd 8\n",
        "#[0.2717109649169748, 0.29633474438177754, 0.22632861237664936, 0.2791072048015778, 0.19286308680928477]  LB: 0.758 ?\n",
        "\n"
      ],
      "id": "zlNTCqWhlCkv",
      "execution_count": 33,
      "outputs": []
    }
  ]
}