{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "muril-large-mlqa-xquad-chaii-fit.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aruaru0/colab_notebook/blob/main/muril_large_mlqa_xquad_chaii_fit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqNdRpHwi4sQ"
      },
      "source": [
        "# for colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T62Oc33qi9ed",
        "outputId": "67f6c6f9-f61a-4bab-fc26-325e9fe31e94"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct 17 00:04:19 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb8KXmIZi9Rv",
        "outputId": "3bae6749-1bf2-4ba8-94a1-1c7e0813168c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4nOz2o-jEUm"
      },
      "source": [
        "! pip install --upgrade --force-reinstall --no-deps  kaggle > /dev/null\n",
        "! mkdir /root/.kaggle\n",
        "! cp \"/content/drive/My Drive/Kaggle/kaggle.json\" /root/.kaggle/\n",
        "! chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnr2Si5WjE9s",
        "outputId": "b1fa66e2-1b86-4da1-caf6-8df12f23330d"
      },
      "source": [
        "!kaggle competitions download -c chaii-hindi-and-tamil-question-answering\n",
        "!kaggle datasets download -d rhtsingh/mlqa-hindi-processed\n",
        "!kaggle datasets download -d msafi04/squad-qa-tamil-dataset\n",
        "!kaggle datasets download -d nbroad/muril-large-pt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading chaii-hindi-and-tamil-question-answering.zip to /content\n",
            "\r  0% 0.00/6.81M [00:00<?, ?B/s]\r 73% 5.00M/6.81M [00:00<00:00, 23.0MB/s]\n",
            "100% 6.81M/6.81M [00:00<00:00, 30.7MB/s]\n",
            "Downloading mlqa-hindi-processed.zip to /content\n",
            "  0% 0.00/2.49M [00:00<?, ?B/s]\n",
            "100% 2.49M/2.49M [00:00<00:00, 198MB/s]\n",
            "Downloading squad-qa-tamil-dataset.zip to /content\n",
            "  0% 0.00/564k [00:00<?, ?B/s]\n",
            "100% 564k/564k [00:00<00:00, 109MB/s]\n",
            "Downloading muril-large-pt.zip to /content\n",
            " 99% 1.74G/1.75G [00:24<00:00, 86.0MB/s]\n",
            "100% 1.75G/1.75G [00:24<00:00, 75.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf1mZhPmjlhh",
        "outputId": "b7463808-8e1f-4b02-ac30-8dc49f9ec7fd"
      },
      "source": [
        "!apt install unzip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HawpKEhQjlUT"
      },
      "source": [
        "!mkdir -p input output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYMc3NQJjoro",
        "outputId": "dc05fe5c-e443-44a7-a5a1-450a4a0ff626"
      },
      "source": [
        "!mkdir input/chaii-hindi-and-tamil-question-answering\n",
        "!unzip /content/chaii-hindi-and-tamil-question-answering.zip -d input/chaii-hindi-and-tamil-question-answering\n",
        "!mkdir input/mlqa-hindi-processed\n",
        "!unzip /content/mlqa-hindi-processed.zip -d input/mlqa-hindi-processed\n",
        "!mkdir input/muril-large-pt\n",
        "!unzip /content/muril-large-pt.zip -d input/muril-large-pt/\n",
        "!mkdir input/squad-qa-tamil-dataset\n",
        "!unzip /content/squad-qa-tamil-dataset.zip -d input/squad-qa-tamil-dataset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/chaii-hindi-and-tamil-question-answering.zip\n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/sample_submission.csv  \n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/test.csv  \n",
            "  inflating: input/chaii-hindi-and-tamil-question-answering/train.csv  \n",
            "Archive:  /content/mlqa-hindi-processed.zip\n",
            "  inflating: input/mlqa-hindi-processed/mlqa_hindi.csv  \n",
            "  inflating: input/mlqa-hindi-processed/xquad.csv  \n",
            "Archive:  /content/muril-large-pt.zip\n",
            "  inflating: input/muril-large-pt/muril-large-cased/config.json  \n",
            "  inflating: input/muril-large-pt/muril-large-cased/pytorch_model.bin  \n",
            "  inflating: input/muril-large-pt/muril-large-cased/special_tokens_map.json  \n",
            "  inflating: input/muril-large-pt/muril-large-cased/tokenizer_config.json  \n",
            "  inflating: input/muril-large-pt/muril-large-cased/vocab.txt  \n",
            "Archive:  /content/squad-qa-tamil-dataset.zip\n",
            "  inflating: input/squad-qa-tamil-dataset/squad_tamilQA.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI1TglF8j5D4"
      },
      "source": [
        "# restart here..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeM15m_Kj73o",
        "outputId": "b9da687c-9df7-4a0e-bf99-39bf2b0149c0"
      },
      "source": [
        "%cd output"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KUWqOMDj7p2",
        "outputId": "543ef1d1-dd25-4b13-ef51-03a48d5a8931"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.024863,
          "end_time": "2021-08-21T23:14:01.671388",
          "exception": false,
          "start_time": "2021-08-21T23:14:01.646525",
          "status": "completed"
        },
        "tags": [],
        "id": "JPzjz7FLi2oM"
      },
      "source": [
        "# Training with MuRIL-Large\n",
        "\n",
        "<strong>Link:</strong> [MuRIL-Large](https://huggingface.co/google/muril-large-cased) \n",
        "    \n",
        "    \n",
        "\n",
        "### References : \n",
        "\n",
        "<ul> \n",
        "    <li>For training: <a href=\"https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit\">chaii QA - 5 Fold XLMRoberta Torch | FIT</a> by <a href=\"https://www.kaggle.com/rhtsingh\">torch</a></li>\n",
        "    <li>For Inference: <a href=\"https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer\">chaii QA - 5 Fold XLMRoberta Torch | Infer</a> by <a href=\"https://www.kaggle.com/rhtsingh\">torch</a></li>\n",
        "    <li>For post processing outputs: <a href=\"https://www.kaggle.com/nbroad/chaii-qa-torch-5-fold-with-post-processing-765\">chaii QA-Torch 5 fold with post-processing (.765)</a> by <a href=\"https://www.kaggle.com/nbroad\">Nicholas Broad 🟢</a></li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "### My other Notebooks :\n",
        "\n",
        "<ul>  \n",
        "    <li>Using the datasets : chaii, mlqa, squad, tamil_xquad </li>\n",
        "    <li>Training for 2 epochs : <a href=\"https://www.kaggle.com/kishalmandal/chaii-fit-2-epochs-mlqa-xquad-chaii/\">chaii | FIT - 2 epochs | mlqa, xquad, chaii</a> </li>\n",
        "    <li>Training for 7 epochs with tamil_xquad: <a href=\"https://www.kaggle.com/kishalmandal/chaii-fit-7-epochs-extra-tamil-data/\">chaii | FIT - 7 epochs | Extra Tamil Data</a> </li>\n",
        "    <li>Inferencing from 5 folds | fold-0 and fold-1 (7-epochs) | fold-2, fold-3 and fold-4 (2-epochs)| based on cross validation scores and a little bit of experimentation 😜: <a href=\"https://www.kaggle.com/kishalmandal/5-epochs-infer-combined-model-0-792/\">5 epochs | INFER | combined model (0.792)</a></li>\n",
        "    \n",
        "    \n",
        "</ul>\n",
        "\n",
        "### Kaggle Dataset version of the MuRIL-Large : [MuRIL Large pt](https://www.kaggle.com/nbroad/muril-large-pt)\n",
        "by [Nicholas Broad 🐢](https://www.kaggle.com/nbroad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022395,
          "end_time": "2021-08-21T23:14:01.762033",
          "exception": false,
          "start_time": "2021-08-21T23:14:01.739638",
          "status": "completed"
        },
        "tags": [],
        "id": "zDNEkkOii2oc"
      },
      "source": [
        "### Install APEX (apparently not 😅)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "aCY6yvR6ET3s",
        "papermill": {
          "duration": 0.039946,
          "end_time": "2021-08-21T23:14:01.824443",
          "exception": false,
          "start_time": "2021-08-21T23:14:01.784497",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:26.342671Z",
          "iopub.execute_input": "2021-10-16T03:03:26.343662Z",
          "iopub.status.idle": "2021-10-16T03:03:26.365508Z",
          "shell.execute_reply.started": "2021-10-16T03:03:26.343513Z",
          "shell.execute_reply": "2021-10-16T03:03:26.364793Z"
        },
        "trusted": true
      },
      "source": [
        "# %%writefile setup.sh\n",
        "# export CUDA_HOME=/usr/local/cuda-10.1\n",
        "# git clone https://github.com/NVIDIA/apex\n",
        "# cd apex\n",
        "# pip install -v --cuda_ext --cpp_ext --disable-pip-version-check --no-cache-dir ./"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l2Jsav9ET3v",
        "papermill": {
          "duration": 3.723267,
          "end_time": "2021-08-21T23:14:05.571564",
          "exception": false,
          "start_time": "2021-08-21T23:14:01.848297",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:26.367156Z",
          "iopub.execute_input": "2021-10-16T03:03:26.367553Z",
          "iopub.status.idle": "2021-10-16T03:03:26.370575Z",
          "shell.execute_reply.started": "2021-10-16T03:03:26.367516Z",
          "shell.execute_reply": "2021-10-16T03:03:26.369837Z"
        },
        "trusted": true
      },
      "source": [
        "# %%capture\n",
        "# !sh setup.sh"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.024166,
          "end_time": "2021-08-21T23:14:05.618884",
          "exception": false,
          "start_time": "2021-08-21T23:14:05.594718",
          "status": "completed"
        },
        "tags": [],
        "id": "rX3ymjuDi2ol"
      },
      "source": [
        "### Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4l6PirHET3x",
        "papermill": {
          "duration": 6.78196,
          "end_time": "2021-08-21T23:14:12.42377",
          "exception": false,
          "start_time": "2021-08-21T23:14:05.64181",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:26.371888Z",
          "iopub.execute_input": "2021-10-16T03:03:26.372364Z",
          "iopub.status.idle": "2021-10-16T03:03:33.685124Z",
          "shell.execute_reply.started": "2021-10-16T03:03:26.372330Z",
          "shell.execute_reply": "2021-10-16T03:03:33.683632Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d342ca1-0ca1-46fc-d333-bfff8fdf0bbb"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "gc.enable()\n",
        "import math\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import multiprocessing\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn import model_selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import (\n",
        "    Dataset, DataLoader,\n",
        "    SequentialSampler, RandomSampler\n",
        ")\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "try:\n",
        "    from apex import amp\n",
        "    APEX_INSTALLED = True\n",
        "except ImportError:\n",
        "    APEX_INSTALLED = False\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    get_cosine_schedule_with_warmup,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    logging,\n",
        "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
        ")\n",
        "logging.set_verbosity_warning()\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "def fix_all_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def optimal_num_of_loader_workers():\n",
        "    num_cpus = multiprocessing.cpu_count()\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
        "    return optimal_value\n",
        "\n",
        "print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apex AMP Installed :: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022837,
          "end_time": "2021-08-21T23:14:12.470632",
          "exception": false,
          "start_time": "2021-08-21T23:14:12.447795",
          "status": "completed"
        },
        "tags": [],
        "id": "oj7wXn9Zi2os"
      },
      "source": [
        "### Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUx5XplNET3y",
        "papermill": {
          "duration": 0.03102,
          "end_time": "2021-08-21T23:14:12.524405",
          "exception": false,
          "start_time": "2021-08-21T23:14:12.493385",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:33.687334Z",
          "iopub.execute_input": "2021-10-16T03:03:33.687577Z",
          "iopub.status.idle": "2021-10-16T03:03:33.695559Z",
          "shell.execute_reply.started": "2021-10-16T03:03:33.687542Z",
          "shell.execute_reply": "2021-10-16T03:03:33.694864Z"
        },
        "trusted": true
      },
      "source": [
        "class Config:\n",
        "    # model\n",
        "    model_type = 'xlm_roberta'\n",
        "    model_name_or_path = \"../input/muril-large-pt/muril-large-cased\"\n",
        "    config_name = \"../input/muril-large-pt/muril-large-cased\"\n",
        "    fp16 = True if APEX_INSTALLED else False\n",
        "    fp16_opt_level = \"O1\"\n",
        "    gradient_accumulation_steps = 2#@param {type:\"integer\"}\n",
        "\n",
        "    # tokenizer\n",
        "    tokenizer_name = \"../input/muril-large-pt/muril-large-cased\"\n",
        "    max_seq_length = 384#@param {type:\"integer\"}\n",
        "    doc_stride = 128#@param {type:\"integer\"}\n",
        "\n",
        "    # train\n",
        "    epochs = 2#@param {type:\"integer\"}\n",
        "    train_batch_size = 4\n",
        "    eval_batch_size = 8\n",
        "\n",
        "    # optimizer\n",
        "    optimizer_type = 'AdamW'\n",
        "    learning_rate = 1.5e-5#@param {type:\"number\"}\n",
        "    weight_decay = 1e-2#@param {type:\"number\"}\n",
        "    epsilon = 1e-8\n",
        "    max_grad_norm = 1.0\n",
        "\n",
        "    # scheduler\n",
        "    decay_name = 'linear-warmup'\n",
        "    warmup_ratio = 0.1\n",
        "\n",
        "    # logging\n",
        "    logging_steps = 400\n",
        "\n",
        "    # evaluate\n",
        "    output_dir = 'output'\n",
        "    seed = 42#@param {type:\"integer\"}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022662,
          "end_time": "2021-08-21T23:14:12.569859",
          "exception": false,
          "start_time": "2021-08-21T23:14:12.547197",
          "status": "completed"
        },
        "tags": [],
        "id": "xhDmKRC0i2ox"
      },
      "source": [
        "### Data Factory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_eRZQrzET3z",
        "papermill": {
          "duration": 1.346671,
          "end_time": "2021-08-21T23:14:13.939262",
          "exception": false,
          "start_time": "2021-08-21T23:14:12.592591",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:33.696865Z",
          "iopub.execute_input": "2021-10-16T03:03:33.697144Z",
          "iopub.status.idle": "2021-10-16T03:03:35.018933Z",
          "shell.execute_reply.started": "2021-10-16T03:03:33.697090Z",
          "shell.execute_reply": "2021-10-16T03:03:35.018164Z"
        },
        "trusted": true
      },
      "source": [
        "train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n",
        "test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n",
        "external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n",
        "external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n",
        "external_train = pd.concat([external_mlqa, external_xquad])\n",
        "\n",
        "def create_folds(data, num_splits):\n",
        "    data[\"kfold\"] = -1\n",
        "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    return data\n",
        "\n",
        "train = create_folds(train, num_splits=5)\n",
        "external_train[\"kfold\"] = -1\n",
        "external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
        "train = pd.concat([train, external_train]).reset_index(drop=True)\n",
        "\n",
        "def convert_answers(row):\n",
        "    return {'answer_start': [row[0]], 'text': [row[1]]}\n",
        "\n",
        "train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.023059,
          "end_time": "2021-08-21T23:14:13.9856",
          "exception": false,
          "start_time": "2021-08-21T23:14:13.962541",
          "status": "completed"
        },
        "tags": [],
        "id": "8wVthOmMi2o2"
      },
      "source": [
        "### Covert Examples to Features (Preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxbZdct1ET3z",
        "papermill": {
          "duration": 0.037206,
          "end_time": "2021-08-21T23:14:14.045801",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.008595",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.020488Z",
          "iopub.execute_input": "2021-10-16T03:03:35.020821Z",
          "iopub.status.idle": "2021-10-16T03:03:35.034271Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.020781Z",
          "shell.execute_reply": "2021-10-16T03:03:35.033457Z"
        },
        "trusted": true
      },
      "source": [
        "def prepare_train_features(args, example, tokenizer):\n",
        "    example[\"question\"] = example[\"question\"].lstrip()\n",
        "    tokenized_example = tokenizer(\n",
        "        example[\"question\"],\n",
        "        example[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=args.max_seq_length,\n",
        "        stride=args.doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
        "\n",
        "    features = []\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        feature = {}\n",
        "\n",
        "        input_ids = tokenized_example[\"input_ids\"][i]\n",
        "        attention_mask = tokenized_example[\"attention_mask\"][i]\n",
        "\n",
        "        feature['input_ids'] = input_ids\n",
        "        feature['attention_mask'] = attention_mask\n",
        "        feature['offset_mapping'] = offsets\n",
        "\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        sequence_ids = tokenized_example.sequence_ids(i)\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = example[\"answers\"]\n",
        "\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            feature[\"start_position\"] = cls_index\n",
        "            feature[\"end_position\"] = cls_index\n",
        "        else:\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                feature[\"start_position\"] = cls_index\n",
        "                feature[\"end_position\"] = cls_index\n",
        "            else:\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                feature[\"start_position\"] = token_start_index - 1\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                feature[\"end_position\"] = token_end_index + 1\n",
        "\n",
        "        features.append(feature)\n",
        "    return features"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.023265,
          "end_time": "2021-08-21T23:14:14.092477",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.069212",
          "status": "completed"
        },
        "tags": [],
        "id": "xaCvG-nai2o5"
      },
      "source": [
        "### Dataset Retriever"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TuzHdjmET30",
        "papermill": {
          "duration": 0.035698,
          "end_time": "2021-08-21T23:14:14.151155",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.115457",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.035663Z",
          "iopub.execute_input": "2021-10-16T03:03:35.035944Z",
          "iopub.status.idle": "2021-10-16T03:03:35.048476Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.035905Z",
          "shell.execute_reply": "2021-10-16T03:03:35.047824Z"
        },
        "trusted": true
      },
      "source": [
        "class DatasetRetriever(Dataset):\n",
        "    def __init__(self, features, mode='train'):\n",
        "        super(DatasetRetriever, self).__init__()\n",
        "        self.features = features\n",
        "        self.mode = mode\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, item):   \n",
        "        feature = self.features[item]\n",
        "        if self.mode == 'train':\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
        "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
        "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                'offset_mapping':feature['offset_mapping'],\n",
        "                'sequence_ids':feature['sequence_ids'],\n",
        "                'id':feature['example_id'],\n",
        "                'context': feature['context'],\n",
        "                'question': feature['question']\n",
        "            }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022716,
          "end_time": "2021-08-21T23:14:14.196862",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.174146",
          "status": "completed"
        },
        "tags": [],
        "id": "3K8gdxb2i2o8"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OxhKqxcET31",
        "papermill": {
          "duration": 0.034229,
          "end_time": "2021-08-21T23:14:14.254275",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.220046",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.049882Z",
          "iopub.execute_input": "2021-10-16T03:03:35.050168Z",
          "iopub.status.idle": "2021-10-16T03:03:35.061298Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.050124Z",
          "shell.execute_reply": "2021-10-16T03:03:35.060392Z"
        },
        "trusted": true
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, modelname_or_path, config):\n",
        "        super(Model, self).__init__()\n",
        "        self.config = config\n",
        "        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self._init_weights(self.qa_outputs)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids, \n",
        "        attention_mask=None, \n",
        "        # token_type_ids=None\n",
        "    ):\n",
        "        outputs = self.xlm_roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]\n",
        "        \n",
        "        # sequence_output = self.dropout(sequence_output)\n",
        "        qa_logits = self.qa_outputs(sequence_output)\n",
        "        \n",
        "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "    \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022742,
          "end_time": "2021-08-21T23:14:14.299904",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.277162",
          "status": "completed"
        },
        "tags": [],
        "id": "-SUDPDV5i2o-"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxuNrJqqET32",
        "papermill": {
          "duration": 0.031016,
          "end_time": "2021-08-21T23:14:14.353877",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.322861",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.062705Z",
          "iopub.execute_input": "2021-10-16T03:03:35.062966Z",
          "iopub.status.idle": "2021-10-16T03:03:35.073810Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.062930Z",
          "shell.execute_reply": "2021-10-16T03:03:35.073059Z"
        },
        "trusted": true
      },
      "source": [
        "def loss_fn(preds, labels):\n",
        "    start_preds, end_preds = preds\n",
        "    start_labels, end_labels = labels\n",
        "    \n",
        "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
        "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
        "    total_loss = (start_loss + end_loss) / 2\n",
        "    return total_loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02271,
          "end_time": "2021-08-21T23:14:14.402448",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.379738",
          "status": "completed"
        },
        "tags": [],
        "id": "YH1EsAU-i2pB"
      },
      "source": [
        "### Grouped Layerwise Learning Rate Decay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf6HVcu2ET34",
        "papermill": {
          "duration": 0.040818,
          "end_time": "2021-08-21T23:14:14.466229",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.425411",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.076944Z",
          "iopub.execute_input": "2021-10-16T03:03:35.077362Z",
          "iopub.status.idle": "2021-10-16T03:03:35.092405Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.077326Z",
          "shell.execute_reply": "2021-10-16T03:03:35.091753Z"
        },
        "trusted": true
      },
      "source": [
        "def get_optimizer_grouped_parameters(args, model):\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
        "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
        "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
        "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n",
        "        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    return optimizer_grouped_parameters"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNONozQii2pE"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022889,
          "end_time": "2021-08-21T23:14:14.51193",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.489041",
          "status": "completed"
        },
        "tags": [],
        "id": "3W0R8o5Mi2pE"
      },
      "source": [
        "### Metric Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkFB-iMcET34",
        "papermill": {
          "duration": 0.032289,
          "end_time": "2021-08-21T23:14:14.567232",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.534943",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.093344Z",
          "iopub.execute_input": "2021-10-16T03:03:35.094129Z",
          "iopub.status.idle": "2021-10-16T03:03:35.107015Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.094074Z",
          "shell.execute_reply": "2021-10-16T03:03:35.106268Z"
        },
        "trusted": true
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.max = 0\n",
        "        self.min = 1e5\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        if val > self.max:\n",
        "            self.max = val\n",
        "        if val < self.min:\n",
        "            self.min = val"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02303,
          "end_time": "2021-08-21T23:14:14.613353",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.590323",
          "status": "completed"
        },
        "tags": [],
        "id": "RuWUWKtpi2pG"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spFRutV0ET34",
        "papermill": {
          "duration": 0.039373,
          "end_time": "2021-08-21T23:14:14.676617",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.637244",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.108359Z",
          "iopub.execute_input": "2021-10-16T03:03:35.108938Z",
          "iopub.status.idle": "2021-10-16T03:03:35.123561Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.108894Z",
          "shell.execute_reply": "2021-10-16T03:03:35.122868Z"
        },
        "trusted": true
      },
      "source": [
        "def make_model(args):\n",
        "    config = AutoConfig.from_pretrained(args.config_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
        "    model = Model(args.model_name_or_path, config=config)\n",
        "    return config, tokenizer, model\n",
        "\n",
        "def make_optimizer(args, model):\n",
        "    optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n",
        "#     no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "#     optimizer_grouped_parameters = [\n",
        "#         {\n",
        "#             \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "#             \"weight_decay\": args.weight_decay,\n",
        "#         },\n",
        "#         {\n",
        "#             \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "#             \"weight_decay\": 0.0,\n",
        "#         },\n",
        "#     ]\n",
        "    if args.optimizer_type == \"AdamW\":\n",
        "        optimizer = AdamW(\n",
        "            optimizer_grouped_parameters,\n",
        "            lr=args.learning_rate,\n",
        "            eps=args.epsilon,\n",
        "            correct_bias=True\n",
        "        )\n",
        "        return optimizer\n",
        "\n",
        "def make_scheduler(\n",
        "    args, optimizer, \n",
        "    num_warmup_steps, \n",
        "    num_training_steps\n",
        "):\n",
        "    if args.decay_name == \"cosine-warmup\":\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "    else:\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "    return scheduler    \n",
        "\n",
        "def make_loader(\n",
        "    args, data, \n",
        "    tokenizer, fold\n",
        "):\n",
        "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
        "    \n",
        "    train_features, valid_features = [[] for _ in range(2)]\n",
        "    for i, row in train_set.iterrows():\n",
        "        train_features += prepare_train_features(args, row, tokenizer)\n",
        "    for i, row in valid_set.iterrows():\n",
        "        valid_features += prepare_train_features(args, row, tokenizer)\n",
        "\n",
        "    train_dataset = DatasetRetriever(train_features)\n",
        "    valid_dataset = DatasetRetriever(valid_features)\n",
        "    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n",
        "    \n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    valid_sampler = SequentialSampler(valid_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.train_batch_size,\n",
        "        sampler=train_sampler,\n",
        "        num_workers=optimal_num_of_loader_workers(),\n",
        "        pin_memory=True,\n",
        "        drop_last=False \n",
        "    )\n",
        "\n",
        "    valid_dataloader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=args.eval_batch_size, \n",
        "        sampler=valid_sampler,\n",
        "        num_workers=optimal_num_of_loader_workers(),\n",
        "        pin_memory=True, \n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    return train_dataloader, valid_dataloader"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.0232,
          "end_time": "2021-08-21T23:14:14.723041",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.699841",
          "status": "completed"
        },
        "tags": [],
        "id": "9-I2e98Ai2pI"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFLvh1VQET35",
        "papermill": {
          "duration": 0.038196,
          "end_time": "2021-08-21T23:14:14.784485",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.746289",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.124949Z",
          "iopub.execute_input": "2021-10-16T03:03:35.125285Z",
          "iopub.status.idle": "2021-10-16T03:03:35.140432Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.125244Z",
          "shell.execute_reply": "2021-10-16T03:03:35.139684Z"
        },
        "trusted": true
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self, model, tokenizer, \n",
        "        optimizer, scheduler\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def train(\n",
        "        self, args, \n",
        "        train_dataloader, \n",
        "        epoch, result_dict\n",
        "    ):\n",
        "        count = 0\n",
        "        losses = AverageMeter()\n",
        "        \n",
        "        self.model.zero_grad()\n",
        "        self.model.train()\n",
        "        \n",
        "        fix_all_seeds(args.seed)\n",
        "        \n",
        "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
        "                    batch_data['start_position'], batch_data['end_position']\n",
        "            \n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
        "\n",
        "            outputs_start, outputs_end = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "            \n",
        "            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            count += input_ids.size(0)\n",
        "            losses.update(loss.item(), input_ids.size(0))\n",
        "\n",
        "            # if args.fp16:\n",
        "            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n",
        "            # else:\n",
        "            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n",
        "                _s = str(len(str(len(train_dataloader.sampler))))\n",
        "                ret = [\n",
        "                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n",
        "                    'Train Loss: {: >4.5f}'.format(losses.avg),\n",
        "                ]\n",
        "                print(', '.join(ret))\n",
        "\n",
        "        result_dict['train_loss'].append(losses.avg)\n",
        "        return result_dict"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02277,
          "end_time": "2021-08-21T23:14:14.83028",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.80751",
          "status": "completed"
        },
        "tags": [],
        "id": "t_27ukfTi2pK"
      },
      "source": [
        "### Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a8kG2UYET36",
        "papermill": {
          "duration": 0.033756,
          "end_time": "2021-08-21T23:14:14.886884",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.853128",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.142730Z",
          "iopub.execute_input": "2021-10-16T03:03:35.144115Z",
          "iopub.status.idle": "2021-10-16T03:03:35.155586Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.142893Z",
          "shell.execute_reply": "2021-10-16T03:03:35.154673Z"
        },
        "trusted": true
      },
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    \n",
        "    def save(self, result, output_dir):\n",
        "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
        "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
        "\n",
        "    def evaluate(self, valid_dataloader, epoch, result_dict):\n",
        "        losses = AverageMeter()\n",
        "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
        "            self.model = self.model.eval()\n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
        "                    batch_data['start_position'], batch_data['end_position']\n",
        "            \n",
        "            input_ids, attention_mask, targets_start, targets_end = \\\n",
        "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
        "            \n",
        "            with torch.no_grad():            \n",
        "                outputs_start, outputs_end = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                )\n",
        "                \n",
        "                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
        "                losses.update(loss.item(), input_ids.size(0))\n",
        "                \n",
        "        print('----Validation Results Summary----')\n",
        "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
        "        result_dict['val_loss'].append(losses.avg)        \n",
        "        return result_dict"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022577,
          "end_time": "2021-08-21T23:14:14.932307",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.90973",
          "status": "completed"
        },
        "tags": [],
        "id": "uLgqq7Zni2pN"
      },
      "source": [
        "### Initialize Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-gUDyq2ET37",
        "papermill": {
          "duration": 0.034598,
          "end_time": "2021-08-21T23:14:14.99012",
          "exception": false,
          "start_time": "2021-08-21T23:14:14.955522",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.157912Z",
          "iopub.execute_input": "2021-10-16T03:03:35.158527Z",
          "iopub.status.idle": "2021-10-16T03:03:35.169799Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.158488Z",
          "shell.execute_reply": "2021-10-16T03:03:35.168921Z"
        },
        "trusted": true
      },
      "source": [
        "def init_training(args, data, fold):\n",
        "    fix_all_seeds(args.seed)\n",
        "    \n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "    \n",
        "    # model\n",
        "    model_config, tokenizer, model = make_model(args)\n",
        "    if torch.cuda.device_count() >= 1:\n",
        "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
        "            torch.cuda.device_count(), \n",
        "            torch.cuda.get_device_name(0))\n",
        "        )\n",
        "        model = model.cuda() \n",
        "    else:\n",
        "        raise ValueError('CPU training is not supported')\n",
        "    \n",
        "    # data loaders\n",
        "    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = make_optimizer(args, model)\n",
        "\n",
        "    # scheduler\n",
        "    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n",
        "    if args.warmup_ratio > 0:\n",
        "        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
        "    else:\n",
        "        num_warmup_steps = 0\n",
        "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n",
        "    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n",
        "\n",
        "    # mixed precision training with NVIDIA Apex\n",
        "    if args.fp16:\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "    \n",
        "    result_dict = {\n",
        "        'epoch':[], \n",
        "        'train_loss': [], \n",
        "        'val_loss' : [], \n",
        "        'best_val_loss': np.inf\n",
        "    }\n",
        "\n",
        "    return (\n",
        "        model, model_config, tokenizer, optimizer, scheduler, \n",
        "        train_dataloader, valid_dataloader, result_dict\n",
        "    )"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022908,
          "end_time": "2021-08-21T23:14:15.036018",
          "exception": false,
          "start_time": "2021-08-21T23:14:15.01311",
          "status": "completed"
        },
        "tags": [],
        "id": "E4TXIYBAi2pP"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39ei5Bm5ET37",
        "papermill": {
          "duration": 0.037195,
          "end_time": "2021-08-21T23:14:15.095984",
          "exception": false,
          "start_time": "2021-08-21T23:14:15.058789",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.171595Z",
          "iopub.execute_input": "2021-10-16T03:03:35.171883Z",
          "iopub.status.idle": "2021-10-16T03:03:35.187526Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.171832Z",
          "shell.execute_reply": "2021-10-16T03:03:35.186774Z"
        },
        "trusted": true
      },
      "source": [
        "def run(data, fold):\n",
        "    args = Config()\n",
        "    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n",
        "        valid_dataloader, result_dict = init_training(args, data, fold)\n",
        "    \n",
        "    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n",
        "    evaluator = Evaluator(model)\n",
        "\n",
        "    train_time_list = []\n",
        "    valid_time_list = []\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        result_dict['epoch'].append(epoch)\n",
        "\n",
        "        # Train\n",
        "        torch.cuda.synchronize()\n",
        "        tic1 = time.time()\n",
        "        result_dict = trainer.train(\n",
        "            args, train_dataloader, \n",
        "            epoch, result_dict\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        tic2 = time.time() \n",
        "        train_time_list.append(tic2 - tic1)\n",
        "        \n",
        "        # Evaluate\n",
        "        torch.cuda.synchronize()\n",
        "        tic3 = time.time()\n",
        "        result_dict = evaluator.evaluate(\n",
        "            valid_dataloader, epoch, result_dict\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        tic4 = time.time() \n",
        "        valid_time_list.append(tic4 - tic3)\n",
        "            \n",
        "        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n",
        "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
        "            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
        "            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n",
        "            \n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
        "            model_config.save_pretrained(output_dir)\n",
        "            tokenizer.save_pretrained(output_dir)\n",
        "            print(f\"Saving model checkpoint to {output_dir}.\")\n",
        "            \n",
        "        print()\n",
        "\n",
        "    evaluator.save(result_dict, output_dir)\n",
        "    \n",
        "    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n",
        "    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    del trainer, evaluator\n",
        "    del model, model_config, tokenizer\n",
        "    del optimizer, scheduler\n",
        "    del train_dataloader, valid_dataloader, result_dict\n",
        "    gc.collect()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPaGnnCnbhWl",
        "papermill": {
          "duration": 6122.076456,
          "end_time": "2021-08-22T00:56:17.195248",
          "exception": false,
          "start_time": "2021-08-21T23:14:15.118792",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:03:35.188907Z",
          "iopub.execute_input": "2021-10-16T03:03:35.189425Z",
          "iopub.status.idle": "2021-10-16T03:44:50.434676Z",
          "shell.execute_reply.started": "2021-10-16T03:03:35.189387Z",
          "shell.execute_reply": "2021-10-16T03:44:50.433837Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b54ea9-bdad-4fa7-e32b-41c8b5d6a831"
      },
      "source": [
        "for fold in range(1):\n",
        "    print();print()\n",
        "    print('-'*50)\n",
        "    print(f'FOLD: {fold}')\n",
        "    print('-'*50)\n",
        "    run(train, fold)\n",
        "    !cp -r ./output /content/drive/MyDrive/datas/chaii3/output\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "FOLD: 0\n",
            "--------------------------------------------------\n",
            "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
            "Num examples Train= 16736, Num examples Valid=2362\n",
            "Total Training Steps: 4184, Total Warmup Steps: 418\n",
            "Epoch: 00 [    4/16736 (  0%)], Train Loss: 2.96086\n",
            "Epoch: 00 [ 1604/16736 ( 10%)], Train Loss: 2.25617\n",
            "Epoch: 00 [ 3204/16736 ( 19%)], Train Loss: 1.95621\n",
            "Epoch: 00 [ 4804/16736 ( 29%)], Train Loss: 1.61900\n",
            "Epoch: 00 [ 6404/16736 ( 38%)], Train Loss: 1.43473\n",
            "Epoch: 00 [ 8004/16736 ( 48%)], Train Loss: 1.44800\n",
            "Epoch: 00 [ 9604/16736 ( 57%)], Train Loss: 1.41639\n",
            "Epoch: 00 [11204/16736 ( 67%)], Train Loss: 1.38246\n",
            "Epoch: 00 [12804/16736 ( 77%)], Train Loss: 1.31404\n",
            "Epoch: 00 [14404/16736 ( 86%)], Train Loss: 1.31382\n",
            "Epoch: 00 [16004/16736 ( 96%)], Train Loss: 1.48065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj4h1aRsi2pS"
      },
      "source": [
        "## Inline Download Link generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-16T03:44:50.444691Z",
          "iopub.execute_input": "2021-10-16T03:44:50.444909Z",
          "iopub.status.idle": "2021-10-16T03:44:50.450339Z",
          "shell.execute_reply.started": "2021-10-16T03:44:50.444883Z",
          "shell.execute_reply": "2021-10-16T03:44:50.449680Z"
        },
        "trusted": true,
        "id": "48nKnpj1i2pT"
      },
      "source": [
        "# import os  \n",
        "# os.chdir(r'/kaggle/working')\n",
        "# from IPython.display import FileLink \n",
        "# FileLink(r'./output/checkpoint-fold-0/pytorch_model.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkjRIhdbwjHx",
        "papermill": {
          "duration": 6011.140057,
          "end_time": "2021-08-22T02:36:29.164758",
          "exception": false,
          "start_time": "2021-08-22T00:56:18.024701",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:44:50.451522Z",
          "iopub.execute_input": "2021-10-16T03:44:50.452024Z",
          "iopub.status.idle": "2021-10-16T03:46:42.568763Z",
          "shell.execute_reply.started": "2021-10-16T03:44:50.451986Z",
          "shell.execute_reply": "2021-10-16T03:46:42.566830Z"
        },
        "trusted": true
      },
      "source": [
        "#example for training second fold\n",
        "\n",
        "for fold in range(1, 2):\n",
        "    print();print()\n",
        "    print('-'*50)\n",
        "    print(f'FOLD: {fold}')\n",
        "    print('-'*50)\n",
        "    run(train, fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 6074.125746,
          "end_time": "2021-08-22T04:17:43.825241",
          "exception": false,
          "start_time": "2021-08-22T02:36:29.699495",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:46:42.570219Z",
          "iopub.status.idle": "2021-10-16T03:46:42.570610Z",
          "shell.execute_reply.started": "2021-10-16T03:46:42.570400Z",
          "shell.execute_reply": "2021-10-16T03:46:42.570420Z"
        },
        "trusted": true,
        "id": "QbMJ2y5Ji2pV"
      },
      "source": [
        "for fold in range(2, 3):\n",
        "    print();print()\n",
        "    print('-'*50)\n",
        "    print(f'FOLD: {fold}')\n",
        "    print('-'*50)\n",
        "    run(train, fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 6023.915341,
          "end_time": "2021-08-22T05:58:08.53458",
          "exception": false,
          "start_time": "2021-08-22T04:17:44.619239",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:46:42.571869Z",
          "iopub.status.idle": "2021-10-16T03:46:42.572303Z",
          "shell.execute_reply.started": "2021-10-16T03:46:42.572054Z",
          "shell.execute_reply": "2021-10-16T03:46:42.572081Z"
        },
        "trusted": true,
        "id": "kDovVXo_i2pW"
      },
      "source": [
        "for fold in range(3, 4):\n",
        "    print();print()\n",
        "    print('-'*50)\n",
        "    print(f'FOLD: {fold}')\n",
        "    print('-'*50)\n",
        "    run(train, fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 6043.161934,
          "end_time": "2021-08-22T07:38:52.967351",
          "exception": false,
          "start_time": "2021-08-22T05:58:09.805417",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-16T03:46:42.573304Z",
          "iopub.status.idle": "2021-10-16T03:46:42.573718Z",
          "shell.execute_reply.started": "2021-10-16T03:46:42.573487Z",
          "shell.execute_reply": "2021-10-16T03:46:42.573508Z"
        },
        "trusted": true,
        "id": "Q3MtCKfai2pX"
      },
      "source": [
        "for fold in range(4, 5):\n",
        "    print();print()\n",
        "    print('-'*50)\n",
        "    print(f'FOLD: {fold}')\n",
        "    print('-'*50)\n",
        "    run(train, fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 1.372811,
          "end_time": "2021-08-22T07:38:55.630426",
          "exception": false,
          "start_time": "2021-08-22T07:38:54.257615",
          "status": "completed"
        },
        "tags": [],
        "id": "enjLcNvKi2pY"
      },
      "source": [
        "### Thanks and please do Upvote!"
      ]
    }
  ]
}